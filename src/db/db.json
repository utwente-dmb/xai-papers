[
  {
    "Paper-ID": "ijcai/PanLLZ20",
    "Title": "Explainable Recommendation via Interpretable Feature Mapping and Evaluation of Explainability.",
    "url": "https://doi.org/10.24963/ijcai.2020/373",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "IJCAI"
    },
    "Authors": [
      "Deng Pan",
      "Xiangrui Li",
      "Xin Li",
      "Dongxiao Zhu"
    ],
    "Type of Data": [
      "User-item matrix"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Other"
    ],
    "Type of Task": [
      "Recommendation"
    ],
    "Type of Explanation": [
      "Disentanglement",
      "Feature Importance"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Latent factor collaborative filtering (CF) has been a widely used technique for recommender system by learning the semantic representations of users and items. Recently, explainable recommendation has attracted much attention from research community. However, trade-off exists between explainability and performance of the recommendation where metadata is often needed to alleviate the dilemma. We present a novel feature mapping approach that maps the uninterpretable general features onto the interpretable aspect features, achieving both satisfactory accuracy and explainability in the recommendations by simultaneous minimization of rating prediction loss and interpretation loss. To evaluate the explainability, we propose two new evaluation metrics specifically designed for aspect-level explanation using surrogate ground truth. Experimental results demonstrate a strong performance in both recommendation and explaining explanation, eliminating the need for metadata. Code is available from this https URL.",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/TuHW0HZ20",
    "Title": "Select, Answer and Explain - Interpretable Multi-Hop Reading Comprehension over Multiple Documents.",
    "url": "https://aaai.org/ojs/index.php/AAAI/article/view/6441",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Ming Tu",
      "Kevin Huang",
      "Guangtao Wang",
      "Jing Huang",
      "Xiaodong He",
      "Bowen Zhou"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Question Answering"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Localization"
    ],
    "Method used to explain": [
      "Supervised explanation training"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/EsserRO20",
    "Title": "A Disentangling Invertible Interpretation Network for Explaining Latent Representations.",
    "url": "https://doi.org/10.1109/CVPR42600.2020.00924",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "Patrick Esser",
      "Robin Rombach",
      "Bj\u00f6rn Ommer"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification",
      "Representation learning"
    ],
    "Type of Explanation": [
      "Disentanglement",
      "Representation Synthesis",
      "Representation Visualization"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Neural networks have greatly boosted performance in computer vision by learning powerful representations of input data. The drawback of end-to-end training for maximal overall performance are black-box models whose hidden representations are lacking interpretability: Since distributed coding is optimal for latent layers to improve their robustness, attributing meaning to parts of a hidden feature vector or to individual neurons is hindered. We formulate interpretation as a translation of hidden representations onto semantic concepts that are comprehensible to the user. The mapping between both domains has to be bijective so that semantic modifications in the target domain correctly alter the original representation. The proposed invertible interpretation network can be transparently applied on top of existing architectures with no need to modify or retrain them. Consequently, we translate an original representation to an equivalent yet interpretable one and backwards without affecting the expressiveness and performance of the original. The invertible interpretation network disentangles the hidden representation into separate, semantically meaningful concepts. Moreover, we present an efficient approach to define semantic concepts by only sketching two images and also an unsupervised strategy. Experimental evaluation demonstrates the wide applicability to interpretation of existing classification and image generation networks as well as to semantically guided image manipulation.",
    "IsOld": true
  },
  {
    "Paper-ID": "icdm/LiuMWH0G20",
    "Title": "LP-Explain - Local Pictorial Explanation for Outliers.",
    "url": "https://doi.org/10.1109/ICDM50108.2020.00046",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ICDM"
    },
    "Authors": [
      "Haoyu Liu",
      "Fenglong Ma",
      "Yaqing Wang",
      "Shibo He",
      "Jiming Chen",
      "Jing Gao"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Anomaly detection"
    ],
    "Type of Explanation": [
      "Feature plot"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Outlier detection is of vital importance for various fields and applications. Existing works mainly focus on identifying outliers from underlying datasets, while how to provide sense-making explanations is largely ignored. In this paper, we propose to visualize data points in a set of scatter plots on two-dimensional (2-D) feature spaces that can provide meaningful explanations about the outlying behavior of outliers. Data are typically multidimensional and the number of 2-D combinations could be huge. Also, outliers may have diverse characteristics, and thus the global scatter plots containing all of outliers may degrade the explanation effectiveness for those outliers having idiosyncratic abnormal 2-D spaces. To address this problem, we propose a new outlier explanation approach, called LP-Explain, which tries to identify the set of best Local Pictorial explanations (defined as the scatter plots in the 2-D space of the feature pairs) that can Explain the behavior for cluster of outliers. We first define an effective measure to quantify the similarity between outliers, and then cluster outliers into different groups based on their abnormal feature pairs. We then propose to weigh the importance of feature pairs within each cluster through a multi-task learning framework to select the set of top feature pairs that best explain various outlier clusters. By adjusting a user-defined parameter indicating the \u201clocalization level\u201d, the proposed method can attain both global and local results for the explanation of the outliers. 2-D visual explanations can be plotted for the top-weighted feature pairs of each cluster. We conduct experiments on various public datasets, which show that the proposed approach can provide more meaningful explanations about the outlying behavior in a dataset.",
    "IsOld": true
  },
  {
    "Paper-ID": "iclr/TsangCLFZL20",
    "Title": "Feature Interaction Interpretability - A Case for Explaining Ad-Recommendation Systems via Neural Interaction Detection.",
    "url": "https://openreview.net/forum?id=BkgnhTEtDS",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ICLR"
    },
    "Authors": [
      "Michael Tsang",
      "Dehua Cheng",
      "Hanpeng Liu",
      "Xue Feng",
      "Eric Zhou",
      "Yan Liu"
    ],
    "Type of Data": [
      "Images",
      "Text",
      "Graph data",
      "Any"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Classification",
      "Recommendation"
    ],
    "Type of Explanation": [
      "Localization"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "icml/RiegerSMY20",
    "Title": "Interpretations are Useful - Penalizing Explanations to Align Neural Networks with Prior Knowledge.",
    "url": "http://proceedings.mlr.press/v119/rieger20a.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ICML"
    },
    "Authors": [
      "Laura Rieger",
      "Chandan Singh",
      "W. James Murdoch",
      "Bin Yu"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Images",
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Heatmap"
    ],
    "Method used to explain": [
      "Supervised explanation training"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/PanHLZL20",
    "Title": "xGAIL - Explainable Generative Adversarial Imitation Learning for Explainable Human Decision Analysis.",
    "url": "https://doi.org/10.1145/3394486.3403186",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Menghai Pan",
      "Weixiao Huang",
      "Yanhua Li",
      "Xun Zhou",
      "Jun Luo"
    ],
    "Type of Data": [
      "Other"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Policy learning"
    ],
    "Type of Explanation": [
      "Heatmap"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "To make daily decisions, human agents devise their own \"strategies\" governing their mobility dynamics (e.g., taxi drivers have preferred working regions and times, and urban commuters have preferred routes and transit modes). Recent research such as generative adversarial imitation learning (GAIL) demonstrates successes in learning human decision-making strategies from their behavior data using deep neural networks (DNNs), which can accurately mimic how humans behave in various scenarios, e.g., playing video games, etc. However, such DNN-based models are \"black box\" models in nature, making it hard to explain what knowledge the models have learned from human, and how the models make such decisions, which was not addressed in the literature of imitation learning. This paper addresses this research gap by proposing xGAIL, the first explainable generative adversarial imitation learning framework. The proposed xGAIL framework consists of two novel components, including Spatial Activation Maximization (SpatialAM) and Spatial Randomized Input Sampling Explanation (SpatialRISE), to extract both global and local knowledge from a well-trained GAIL model that explains how a human agent makes decisions. Especially, we take taxi drivers' passenger-seeking strategy as an example to validate the effectiveness of the proposed xGAIL framework. Our analysis on a large-scale real-world taxi trajectory data shows promising results from two aspects: i) global explainable knowledge of what nearby traffic condition impels a taxi driver to choose a particular direction to find the next passenger, and ii) local explainable knowledge of what key (sometimes hidden) factors a taxi driver considers when making a particular decision.",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/PalejaSCG20",
    "Title": "Interpretable and Personalized Apprenticeship Scheduling - Learning Interpretable Scheduling Policies from Heterogeneous User Demonstrations.",
    "url": "https://proceedings.neurips.cc/paper/2020/hash/477bdb55b231264bb53a7942fd84254d-Abstract.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Rohan R. Paleja",
      "Andrew Silva",
      "Letian Chen",
      "Matthew C. Gombolay"
    ],
    "Type of Data": [
      "Graph data",
      "Other"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Policy learning"
    ],
    "Type of Explanation": [
      "Decision Tree"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/VuT20",
    "Title": "PGM-Explainer - Probabilistic Graphical Model Explanations for Graph Neural Networks.",
    "url": "https://proceedings.neurips.cc/paper/2020/hash/8fb134f258b1f7865a6ab2d935a897c9-Abstract.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Minh N. Vu",
      "My T. Thai"
    ],
    "Type of Data": [
      "Graph data"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Graph"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/ZhouHZLSXT20",
    "Title": "Towards Interpretable Natural Language Understanding with Explanations as Latent Variables.",
    "url": "https://proceedings.neurips.cc/paper/2020/hash/4be2c8f27b8a420492f2d44463933eb6-Abstract.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Wangchunshu Zhou",
      "Jinyi Hu",
      "Hanlin Zhang",
      "Xiaodan Liang",
      "Maosong Sun",
      "Chenyan Xiong",
      "Jian Tang"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Text"
    ],
    "Method used to explain": [
      "Supervised explanation training"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "sigir/SenGVJ20",
    "Title": "The Curious Case of IR Explainability - Explaining Document Scores within and across Ranking Models.",
    "url": "https://doi.org/10.1145/3397271.3401286",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "SIGIR"
    },
    "Authors": [
      "Procheta Sen",
      "Debasis Ganguly",
      "Manisha Verma",
      "Gareth J. F. Jones"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Other"
    ],
    "Type of Task": [
      "Retrieval"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "It is often useful for an IR practitioner to analyze the similarity function of an IR model, or for a non-technical search engine user to understand why a document was shown at a certain rank, in terms of the three fundamental aspects of a similarity function, namely the a) frequency of a term in a document, b) frequency of a term in a collection and c) the length of a document. We propose a general methodology of approximating an IR model as the coefficients of a linear function of these three fundamental aspects (and an additional aspect of semantic similarity between terms for neural models), which potentially can help IR practitioners to optimize the relative importance of each aspect on specific document collection and types of queries. Our analysis shows that the coefficients, which represent the relative importance of the three fundamental aspects, are useful to compare a model's different parametric instantiations or compare across different models.",
    "IsOld": true
  },
  {
    "Paper-ID": "acl/LiuYW19",
    "Title": "Towards Explainable NLP - A Generative Explanation Framework for Text Classification.",
    "url": "https://doi.org/10.18653/v1/p19-1560",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ACL"
    },
    "Authors": [
      "Hui Liu",
      "Qingyu Yin",
      "William Yang Wang"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Text",
      "Other"
    ],
    "Method used to explain": [
      "Supervised explanation training"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Building explainable systems is a critical problem in the field of Natural Language Processing (NLP), since most machine learning models provide no explanations for the predictions. Existing approaches for explainable machine learning systems tend to focus on interpreting the outputs or the connections between inputs and outputs. However, the fine-grained information (e.g. textual explanations for the labels) is often ignored, and the systems do not explicitly generate the human-readable explanations. To solve this problem, we propose a novel generative explanation framework that learns to make classification decisions and generate fine-grained explanations at the same time. More specifically, we introduce the explainable factor and the minimum risk training approach that learn to generate more reasonable explanations. We construct two new datasets that contain summaries, rating scores, and fine-grained reasons. We conduct experiments on both datasets, comparing with several strong neural network baseline systems. Experimental results show that our method surpasses all baselines on both datasets, and is able to generate concise explanations at the same time.",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/WagnerKGHWB19",
    "Title": "Interpretable and Fine-Grained Visual Explanations for Convolutional Neural Networks.",
    "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wagner_Interpretable_and_Fine-Grained_Visual_Explanations_for_Convolutional_Neural_Networks_CVPR_2019_paper.html",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "J\u00f6rg Wagner",
      "Jan Mathias K\u00f6hler",
      "Tobias Gindele",
      "Leon Hetzel",
      "Jakob Thadd\u00e4us Wiedemer",
      "Sven Behnke"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Localization",
      "Heatmap"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "To verify and validate networks, it is essential to gain insight into their decisions, limitations as well as possible shortcomings of training data. In this work, we propose a post-hoc, optimization based visual explanation method, which highlights the evidence in the input image for a specific prediction. Our approach is based on a novel technique to defend against adversarial evidence (i.e. faulty evidence due to artefacts) by filtering gradients during optimization. The defense does not depend on human-tuned parameters. It enables explanations which are both fine-grained and preserve the characteristics of images, such as edges and colors. The explanations are interpretable, suited for visualizing detailed evidence and can be tested as they are valid model inputs. We qualitatively and quantitatively evaluate our approach on a multitude of models and datasets.",
    "IsOld": true
  },
  {
    "Paper-ID": "iclr/MWT19",
    "Title": "Visual Explanation by Interpretation - Improving Visual Feedback Capabilities of Deep Neural Networks.",
    "url": "https://openreview.net/forum?id=H1ziPjC5Fm",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ICLR"
    },
    "Authors": [
      "Jos\u00e9 Oramas M.",
      "Kaili Wang",
      "Tinne Tuytelaars"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Heatmap",
      "Representation Synthesis"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "icml/Wang19",
    "Title": "Gaining Free or Low-Cost Interpretability with Interpretable Partial Substitute.",
    "url": "http://proceedings.mlr.press/v97/wang19a.html",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ICML"
    },
    "Authors": [
      "Tong Wang"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Text",
      "Time series"
    ],
    "Type of Problem": [
      "Model Explanation",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Tree Ensemble",
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Decision Rules"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/SchwabK19",
    "Title": "CXPlain - Causal Explanations for Model Interpretation under Uncertainty.",
    "url": "https://proceedings.neurips.cc/paper/2019/hash/3ab6be46e1d6b21d59a3c3a0b9d0f6ef-Abstract.html",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Patrick Schwab",
      "Walter Karlen"
    ],
    "Type of Data": [
      "Images",
      "Any"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Classification",
      "Regression"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "sigir/ChenCXZ0QZ19",
    "Title": "Personalized Fashion Recommendation with Visual Explanations based on Multimodal Attention Network - Towards Visually Explainable Recommendation.",
    "url": "https://doi.org/10.1145/3331184.3331254",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "SIGIR"
    },
    "Authors": [
      "Xu Chen",
      "Hanxiong Chen",
      "Hongteng Xu",
      "Yongfeng Zhang",
      "Yixin Cao",
      "Zheng Qin",
      "Hongyuan Zha"
    ],
    "Type of Data": [
      "Images",
      "Text",
      "User-item matrix"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Recommendation"
    ],
    "Type of Explanation": [
      "Heatmap"
    ],
    "Method used to explain": [
      "Supervised explanation training"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Fashion recommendation has attracted increasing attention from both industry and academic communities. This paper proposes a novel neural architecture for fashion recommendation based on both image region-level features and user review information. Our basic intuition is that: for a fashion image, not all the regions are equally important for the users, i.e., people usually care about a few parts of the fashion image. To model such human sense, we learn an attention model over many pre-segmented image regions, based on which we can understand where a user is really interested in on the image, and correspondingly, represent the image in a more accurate manner. In addition, by discovering such fine-grained visual preference, we can visually explain a recommendation by highlighting some regions of its image. For better learning the attention model, we also introduce user review information as a weak supervision signal to collect more comprehensive user preference. In our final framework, the visual and textual features are seamlessly coupled by a multimodal attention network. Based on this architecture, we can not only provide accurate recommendation, but also can accompany each recommended item with novel visual explanations. We conduct extensive experiments to demonstrate the superiority of our proposed model in terms of Top-N recommendation, and also we build a collectively labeled dataset for evaluating our provided visual explanations in a quantitative manner.",
    "IsOld": true
  },
  {
    "Paper-ID": "sigir/VermaG19",
    "Title": "LIRME - Locally Interpretable Ranking Model Explanation.",
    "url": "https://doi.org/10.1145/3331184.3331377",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "SIGIR"
    },
    "Authors": [
      "Manisha Verma",
      "Debasis Ganguly"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Any (for a specific task); model-agnostic",
      "Other"
    ],
    "Type of Task": [
      "Retrieval"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/ZhangCSWZ18",
    "Title": "Interpreting CNN Knowledge via an Explanatory Graph.",
    "url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17354",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Quanshi Zhang",
      "Ruiming Cao",
      "Feng Shi",
      "Ying Nian Wu",
      "Song-Chun Zhu"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Heatmap",
      "Graph",
      "Prototypes",
      "Localization"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "icdm/YangLWH18",
    "Title": "Towards Interpretation of Recommender Systems with Sorted Explanation Paths.",
    "url": "https://doi.org/10.1109/ICDM.2018.00082",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "ICDM"
    },
    "Authors": [
      "Fan Yang",
      "Ninghao Liu",
      "Suhang Wang",
      "Xia Hu"
    ],
    "Type of Data": [
      "Tabular / structured",
      "User-item matrix"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Recommendation"
    ],
    "Type of Explanation": [
      "Graph"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Despite the wide application in recent years, most recommender systems are not capable of providing interpretations together with recommendation results, which impedes both deployers and customers from understanding or trusting the results. Recent advances in recommendation models, such as deep learning models, usually involve extracting latent representations of users and items. However, the representation space is not directly comprehensible since each dimension usually does not have any specific meaning. In addition, recommender systems incorporate various sources of information, such as user behaviors, item information, and other side content information. Properly organizing different types of information, as well as effectively selecting important information for interpretation, is challenging and has not been fully tackled by conventional interpretation methods. In this paper, we propose a post-hoc method called Sorted Explanation Paths (SEP) to interpret recommendation results. Specifically, we first build a unified heterogeneous information network to incorporate multiple types of objects and relations based on representations from the recommender system and information from the dataset. Then, we search for explanation paths between given recommendation pairs, and use the set of simple paths to construct semantic explanations. Next, three heuristic metrics, i.e., credibility, readability and diversity, are designed to measure the validity of each explanation path, and to sort all the paths comprehensively. The top-ranked explanation paths are selected as the final interpretation. After that, practical issues on computation and efficiency of the proposed SEP method are also handled by corresponding approaches. Finally, we conduct experiments on three real-world benchmark datasets, and demonstrate the applicability and effectiveness of the proposed SEP method.",
    "IsOld": true
  },
  {
    "Paper-ID": "icml/ChenSWJ18",
    "Title": "Learning to Explain - An Information-Theoretic Perspective on Model Interpretation.",
    "url": "http://proceedings.mlr.press/v80/chen18j.html",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "ICML"
    },
    "Authors": [
      "Jianbo Chen",
      "Le Song",
      "Martin J. Wainwright",
      "Michael I. Jordan"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Images",
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Localization"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/PeakeW18",
    "Title": "Explanation Mining - Post Hoc Interpretability of Latent Factor Models for Recommendation Systems.",
    "url": "https://doi.org/10.1145/3219819.3220072",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Georgina Peake",
      "Jun Wang"
    ],
    "Type of Data": [
      "User-item matrix"
    ],
    "Type of Problem": [
      "Model Explanation"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Recommendation"
    ],
    "Type of Explanation": [
      "Decision Rules"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "The widescale use of machine learning algorithms to drive decision-making has highlighted the critical importance of ensuring the interpretability of such models in order to engender trust in their output. The state-of-the-art recommendation systems use black-box latent factor models that provide no explanation of why a recommendation has been made, as they abstract their decision processes to a high-dimensional latent space which is beyond the direct comprehension of humans. We propose a novel approach for extracting explanations from latent factor recommendation systems by training association rules on the output of a matrix factorisation black-box model. By taking advantage of the interpretable structure of association rules, we demonstrate that predictive accuracy of the recommendation model can be maintained whilst yielding explanations with high fidelity to the black-box model on a unique industry dataset. Our approach mitigates the accuracy-interpretability trade-off whilst avoiding the need to sacrifice flexibility or use external data sources. We also contribute to the ill-defined problem of evaluating interpretability.",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/Alvarez-MelisJ18",
    "Title": "Towards Robust Interpretability with Self-Explaining Neural Networks.",
    "url": "https://proceedings.neurips.cc/paper/2018/hash/3e9f0fc9b2f89e043bc6233994dfcf76-Abstract.html",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "David Alvarez-Melis",
      "Tommi S. Jaakkola"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/DhurandharCLTTS18",
    "Title": "Explanations based on the Missing - Towards Contrastive Explanations with Pertinent Negatives.",
    "url": "https://proceedings.neurips.cc/paper/2018/hash/c5ff2543b53f4cc0ad3819a36752467b-Abstract.html",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Amit Dhurandhar",
      "Pin-Yu Chen",
      "Ronny Luss",
      "Chun-Chen Tu",
      "Pai-Shun Ting",
      "Karthikeyan Shanmugam",
      "Payel Das"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Localization"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "iccv/FongV17",
    "Title": "Interpretable Explanations of Black Boxes by Meaningful Perturbation.",
    "url": "https://doi.org/10.1109/ICCV.2017.371",
    "Year": "2017",
    "Venue": {
      "isOld": true,
      "value": "ICCV"
    },
    "Authors": [
      "Ruth C. Fong",
      "Andrea Vedaldi"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Heatmap"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "As machine learning algorithms are increasingly applied to high impact yet high risk tasks, such as medical diagnosis or autonomous driving, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks \u201clook\u201d in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints. In this paper, we make two main contributions: First, we propose a general framework for learning different kinds of explanations for any black box algorithm. Second, we specialise the framework to find the part of an image most responsible for a classifier decision. Unlike previous works, our method is model-agnostic and testable because it is grounded in explicit and interpretable image perturbations.",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/AkulaWZ20",
    "Title": "CoCoX - Generating Conceptual and Counterfactual Explanations via Fault-Lines.",
    "url": "https://aaai.org/ojs/index.php/AAAI/article/view/5643",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Arjun R. Akula",
      "Shuai Wang",
      "Song-Chun Zhu"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Prototypes"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/ChenJ20",
    "Title": "LS-Tree - Model Interpretation When the Data Are Linguistic.",
    "url": "https://aaai.org/ojs/index.php/AAAI/article/view/5749",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Jianbo Chen",
      "Michael I. Jordan"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Any (for a specific task); model-agnostic",
      "Logistic Regression"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Graph"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/CiravegnaGMMG20",
    "Title": "A Constraint-Based Approach to Learning and Explanation.",
    "url": "https://aaai.org/ojs/index.php/AAAI/article/view/5774",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Gabriele Ciravegna",
      "Francesco Giannini",
      "Stefano Melacci",
      "Marco Maggini",
      "Marco Gori"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Decision Rules"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/DalleigerV20",
    "Title": "Explainable Data Decompositions.",
    "url": "https://aaai.org/ojs/index.php/AAAI/article/view/5780",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Sebastian Dalleiger",
      "Jilles Vreeken"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Clustering"
    ],
    "Type of Explanation": [
      "Other"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model",
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/DongNCCZSLCM20",
    "Title": "Asymmetrical Hierarchical Networks with Attentive Interactions for Interpretable Review-Based Recommendation.",
    "url": "https://aaai.org/ojs/index.php/AAAI/article/view/6268",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Xin Dong",
      "Jingchao Ni",
      "Wei Cheng",
      "Zhengzhang Chen",
      "Bo Zong",
      "Dongjin Song",
      "Yanchi Liu",
      "Haifeng Chen",
      "Gerard de Melo"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Recommendation"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/HarderBP20",
    "Title": "Interpretable and Differentially Private Predictions.",
    "url": "https://aaai.org/ojs/index.php/AAAI/article/view/5827",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Frederik Harder",
      "Matthias Bauer",
      "Mijung Park"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Images"
    ],
    "Type of Problem": [
      "Model Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Representation Synthesis"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/HuaiWMZ20",
    "Title": "Towards Interpretation of Pairwise Learning.",
    "url": "https://aaai.org/ojs/index.php/AAAI/article/view/5837",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Mengdi Huai",
      "Di Wang",
      "Chenglin Miao",
      "Aidong Zhang"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Images",
      "Other"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Classification",
      "Representation learning"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/ItoTSYI20",
    "Title": "Word-Level Contextual Sentiment Analysis with Interpretability.",
    "url": "https://aaai.org/ojs/index.php/AAAI/article/view/5845",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Tomoki Ito",
      "Kota Tsubouchi",
      "Hiroki Sakaji",
      "Tatsuo Yamashita",
      "Kiyoshi Izumi"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Localization"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/LiFANZ20",
    "Title": "MRI Reconstruction with Interpretable Pixel-Wise Operations Using Reinforcement Learning.",
    "url": "https://aaai.org/ojs/index.php/AAAI/article/view/5423",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Wentian Li",
      "Xidong Feng",
      "Haotian An",
      "Xiang Yao Ng",
      "Yu-Jin Zhang"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Policy learning"
    ],
    "Type of Explanation": [
      "Localization"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/Madumal0SV20",
    "Title": "Explainable Reinforcement Learning through a Causal Lens.",
    "url": "https://aaai.org/ojs/index.php/AAAI/article/view/5631",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Prashan Madumal",
      "Tim Miller",
      "Liz Sonenberg",
      "Frank Vetere"
    ],
    "Type of Data": [
      "Other"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Tree Ensemble",
      "Any (for a specific task); model-agnostic",
      "Other"
    ],
    "Type of Task": [
      "Policy learning"
    ],
    "Type of Explanation": [
      "Text"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/NamGCWL20",
    "Title": "Relative Attributing Propagation - Interpreting the Comparative Contributions of Individual Units in Deep Neural Networks.",
    "url": "https://aaai.org/ojs/index.php/AAAI/article/view/5632",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Woo-Jeoung Nam",
      "Shir Gur",
      "Jaesik Choi",
      "Lior Wolf",
      "Seong-Whan Lee"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Heatmap"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/PathakLMP20",
    "Title": "Chemically Interpretable Graph Interaction Network for Prediction of Pharmacokinetic Properties of Drug-Like Molecules.",
    "url": "https://aaai.org/ojs/index.php/AAAI/article/view/5433",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Yashaswi Pathak",
      "Siddhartha Laghuvarapu",
      "Sarvesh Mehta",
      "U. Deva Priyakumar"
    ],
    "Type of Data": [
      "Graph data"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Regression"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/PatroAN20",
    "Title": "Explanation vs Attention - A Two-Player Game to Obtain Attention for VQA.",
    "url": "https://aaai.org/ojs/index.php/AAAI/article/view/6858",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Badri N. Patro",
      "Anupriy",
      "Vinay Namboodiri"
    ],
    "Type of Data": [
      "Images",
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Question Answering"
    ],
    "Type of Explanation": [
      "Heatmap"
    ],
    "Method used to explain": [
      "Supervised explanation training"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/WangZHZS20",
    "Title": "Dynamic Network Pruning with Interpretable Layerwise Channel Selection.",
    "url": "https://aaai.org/ojs/index.php/AAAI/article/view/6098",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Yulong Wang",
      "Xiaolu Zhang",
      "Xiaolin Hu",
      "Bo Zhang",
      "Hang Su"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Representation Visualization",
      "Other"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/WuPHKCZ0D20",
    "Title": "Regional Tree Regularization for Interpretability in Deep Neural Networks.",
    "url": "https://aaai.org/ojs/index.php/AAAI/article/view/6112",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Mike Wu",
      "Sonali Parbhoo",
      "Michael C. Hughes",
      "Ryan Kindle",
      "Leo A. Celi",
      "Maurizio Zazzi",
      "Volker Roth",
      "Finale Doshi-Velez"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Model Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Decision Tree"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/ZhongWTZ0S20",
    "Title": "Iteratively Questioning and Answering for Interpretable Legal Judgment Prediction.",
    "url": "https://aaai.org/ojs/index.php/AAAI/article/view/5479",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Haoxi Zhong",
      "Yuzhong Wang",
      "Cunchao Tu",
      "Tianyang Zhang",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Question Answering",
      "Classification"
    ],
    "Type of Explanation": [
      "Localization"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "acl/AtanasovaSLA20",
    "Title": "Generating Fact Checking Explanations.",
    "url": "https://doi.org/10.18653/v1/2020.acl-main.656",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ACL"
    },
    "Authors": [
      "Pepa Atanasova",
      "Jakob Grue Simonsen",
      "Christina Lioma",
      "Isabelle Augenstein"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification",
      "Other"
    ],
    "Type of Explanation": [
      "Text"
    ],
    "Method used to explain": [
      "Supervised explanation training"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Most existing work on automated fact checking is concerned with predicting the veracity of claims based on metadata, social network spread, language used in claims, and, more recently, evidence supporting or denying claims. A crucial piece of the puzzle that is still missing is to understand how to automate the most elaborate part of the process \u2013 generating justifications for verdicts on claims. This paper provides the first study of how these explanations can be generated automatically based on available claim context, and how this task can be modelled jointly with veracity prediction. Our results indicate that optimising both objectives at the same time, rather than training them separately, improves the performance of a fact checking system. The results of a manual evaluation further suggest that the informativeness, coverage and overall quality of the generated explanations are also improved in the multi-task model.",
    "IsOld": true
  },
  {
    "Paper-ID": "acl/ChenZJ20",
    "Title": "Generating Hierarchical Explanations on Text Classification via Feature Interaction Detection.",
    "url": "https://doi.org/10.18653/v1/2020.acl-main.494",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ACL"
    },
    "Authors": [
      "Hanjie Chen",
      "Guangtao Zheng",
      "Yangfeng Ji"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Graph"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Generating explanations for neural networks has become crucial for their applications in real-world with respect to reliability and trustworthiness. In natural language processing, existing methods usually provide important features which are words or phrases selected from an input text as an explanation, but ignore the interactions between them. It poses challenges for humans to interpret an explanation and connect it to model prediction. In this work, we build hierarchical explanations by detecting feature interactions. Such explanations visualize how words and phrases are combined at different levels of the hierarchy, which can help users understand the decision-making of black-box models. The proposed method is evaluated with three neural text classifiers (LSTM, CNN, and BERT) on two benchmark datasets, via both automatic and human evaluations. Experiments show the effectiveness of the proposed method in providing explanations that are both faithful to models and interpretable to humans.",
    "IsOld": true
  },
  {
    "Paper-ID": "acl/GonenJSG20",
    "Title": "Simple, Interpretable and Stable Method for Detecting Words with Usage Change across Corpora.",
    "url": "https://doi.org/10.18653/v1/2020.acl-main.51",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ACL"
    },
    "Authors": [
      "Hila Gonen",
      "Ganesh Jawahar",
      "Djam\u00e9 Seddah",
      "Yoav Goldberg"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Other"
    ],
    "Type of Explanation": [
      "Localization",
      "Representation Visualization"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "The problem of comparing two bodies of text and searching for words that differ in their usage between them arises often in digital humanities and computational social science. This is commonly approached by training word embeddings on each corpus, aligning the vector spaces, and looking for words whose cosine distance in the aligned space is large. However, these methods often require extensive filtering of the vocabulary to perform well, and - as we show in this work - result in unstable, and hence less reliable, results. We propose an alternative approach that does not use vector space alignment, and instead considers the neighbors of each word. The method is simple, interpretable and stable. We demonstrate its effectiveness in 9 different setups, considering different corpus splitting criteria (age, gender and profession of tweet authors, time of tweet) and different languages (English, French and Hebrew).",
    "IsOld": true
  },
  {
    "Paper-ID": "acl/HanWT20",
    "Title": "Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions.",
    "url": "https://doi.org/10.18653/v1/2020.acl-main.492",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ACL"
    },
    "Authors": [
      "Xiaochuang Han",
      "Byron C. Wallace",
      "Yulia Tsvetkov"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Prototypes"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Modern deep learning models for NLP are notoriously opaque. This has motivated the development of methods for interpreting such models, e.g., via gradient-based saliency maps or the visualization of attention weights. Such approaches aim to provide explanations for a particular model prediction by highlighting important words in the corresponding input text. While this might be useful for tasks where decisions are explicitly influenced by individual tokens in the input, we suspect that such highlighting is not suitable for tasks where model decisions should be driven by more complex reasoning. In this work, we investigate the use of influence functions for NLP, providing an alternative approach to interpreting neural text classifiers. Influence functions explain the decisions of a model by identifying influential training examples. Despite the promise of this approach, influence functions have not yet been extensively evaluated in the context of NLP, a gap addressed by this work. We conduct a comparison between influence functions and common word-saliency methods on representative tasks. As suspected, we find that influence functions are particularly useful for natural language inference, a task in which \u2018saliency maps\u2019 may not have clear interpretation. Furthermore, we develop a new quantitative measure based on influence functions that can reveal artifacts in training data.",
    "IsOld": true
  },
  {
    "Paper-ID": "acl/KumarT20",
    "Title": "NILE - Natural Language Inference with Faithful Natural Language Explanations.",
    "url": "https://doi.org/10.18653/v1/2020.acl-main.771",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ACL"
    },
    "Authors": [
      "Sawan Kumar",
      "Partha P. Talukdar"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Text",
      "Localization"
    ],
    "Method used to explain": [
      "Supervised explanation training"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "The recent growth in the popularity and success of deep learning models on NLP classification tasks has accompanied the need for generating some form of natural language explanation of the predicted labels. Such generated natural language (NL) explanations are expected to be faithful, i.e., they should correlate well with the model\u2019s internal decision making. In this work, we focus on the task of natural language inference (NLI) and address the following question: can we build NLI systems which produce labels with high accuracy, while also generating faithful explanations of its decisions? We propose Natural-language Inference over Label-specific Explanations (NILE), a novel NLI method which utilizes auto-generated label-specific NL explanations to produce labels along with its faithful explanation. We demonstrate NILE\u2019s effectiveness over previously reported methods through automated and human evaluation of the produced labels and explanations. Our evaluation of NILE also supports the claim that accurate systems capable of providing testable explanations of their decisions can be designed. We discuss the faithfulness of NILE\u2019s explanations in terms of sensitivity of the decisions to the corresponding explanations. We argue that explicit evaluation of faithfulness, in addition to label and explanation accuracy, is an important step in evaluating model\u2019s explanations. Further, we demonstrate that task-specific probes are necessary to establish such sensitivity.",
    "IsOld": true
  },
  {
    "Paper-ID": "acl/LuL20",
    "Title": "GCAN - Graph-aware Co-Attention Networks for Explainable Fake News Detection on Social Media.",
    "url": "https://doi.org/10.18653/v1/2020.acl-main.48",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ACL"
    },
    "Authors": [
      "Yi-Ju Lu",
      "Cheng-Te Li"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Heatmap"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "This paper solves the fake news detection problem under a more realistic scenario on social media. Given the source short-text tweet and the corresponding sequence of retweet users without text comments, we aim at predicting whether the source tweet is fake or not, and generating explanation by highlighting the evidences on suspicious retweeters and the words they concern. We develop a novel neural network-based model, Graph-aware Co-Attention Networks (GCAN), to achieve the goal. Extensive experiments conducted on real tweet datasets exhibit that GCAN can significantly outperform state-of-the-art methods by 16% in accuracy on average. In addition, the case studies also show that GCAN can produce reasonable explanations.",
    "IsOld": true
  },
  {
    "Paper-ID": "acl/MohankumarNNKSR20",
    "Title": "Towards Transparent and Explainable Attention Models.",
    "url": "https://doi.org/10.18653/v1/2020.acl-main.387",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ACL"
    },
    "Authors": [
      "Akash Kumar Mohankumar",
      "Preksha Nema",
      "Sharan Narasimhan",
      "Mitesh M. Khapra",
      "Balaji Vasan Srinivasan",
      "Balaraman Ravindran"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification",
      "Question Answering"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Recent studies on interpretability of attention distributions have led to notions of faithful and plausible explanations for a model\u2019s predictions. Attention distributions can be considered a faithful explanation if a higher attention weight implies a greater impact on the model\u2019s prediction. They can be considered a plausible explanation if they provide a human-understandable justification for the model\u2019s predictions. In this work, we first explain why current attention mechanisms in LSTM based encoders can neither provide a faithful nor a plausible explanation of the model\u2019s predictions. We observe that in LSTM based encoders the hidden representations at different time-steps are very similar to each other (high conicity) and attention weights in these situations do not carry much meaning because even a random permutation of the attention weights does not affect the model\u2019s predictions. Based on experiments on a wide variety of tasks and datasets, we observe attention distributions often attribute the model\u2019s predictions to unimportant words such as punctuation and fail to offer a plausible explanation for the predictions. To make attention mechanisms more faithful and plausible, we propose a modified LSTM cell with a diversity-driven training objective that ensures that the hidden representations learned at different time steps are diverse. We show that the resulting attention distributions offer more transparency as they (i) provide a more precise importance ranking of the hidden states (ii) are better indicative of words important for the model\u2019s predictions (iii) correlate better with gradient-based attribution methods. Human evaluations indicate that the attention distributions learned by our model offer a plausible explanation of the model\u2019s predictions. Our code has been made publicly available at https://github.com/akashkm99/Interpretable-Attention",
    "IsOld": true
  },
  {
    "Paper-ID": "acl/ShahbaziFGT20",
    "Title": "Relation Extraction with Explanation.",
    "url": "https://doi.org/10.18653/v1/2020.acl-main.579",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ACL"
    },
    "Authors": [
      "Hamed Shahbazi",
      "Xiaoli Z. Fern",
      "Reza Ghaeini",
      "Prasad Tadepalli"
    ],
    "Type of Data": [
      "Text",
      "Other"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Other"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Recent neural models for relation extraction with distant supervision alleviate the impact of irrelevant sentences in a bag by learning importance weights for the sentences. Efforts thus far have focused on improving extraction accuracy but little is known about their explanability. In this work we annotate a test set with ground-truth sentence-level explanations to evaluate the quality of explanations afforded by the relation extraction models. We demonstrate that replacing the entity mentions in the sentences with their fine-grained entity types not only enhances extraction accuracy but also improves explanation. We also propose to automatically generate \u201cdistractor\u201d sentences to augment the bags and train the model to ignore the distractors. Evaluations on the widely used FB-NYT dataset show that our methods achieve new state-of-the-art accuracy while improving model explanability.",
    "IsOld": true
  },
  {
    "Paper-ID": "acl/SubramanianBGWS20",
    "Title": "Obtaining Faithful Interpretations from Compositional Neural Networks.",
    "url": "https://doi.org/10.18653/v1/2020.acl-main.495",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ACL"
    },
    "Authors": [
      "Sanjay Subramanian",
      "Ben Bogin",
      "Nitish Gupta",
      "Tomer Wolfson",
      "Sameer Singh",
      "Jonathan Berant",
      "Matt Gardner"
    ],
    "Type of Data": [
      "Images",
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification",
      "Question Answering"
    ],
    "Type of Explanation": [
      "Localization",
      "Other"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model",
      "Supervised explanation training"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Neural module networks (NMNs) are a popular approach for modeling compositionality: they achieve high accuracy when applied to problems in language and vision, while reflecting the compositional structure of the problem in the network architecture. However, prior work implicitly assumed that the structure of the network modules, describing the abstract reasoning process, provides a faithful explanation of the model\u2019s reasoning; that is, that all modules perform their intended behaviour. In this work, we propose and conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2 and DROP, two datasets which require composing multiple reasoning steps. We find that the intermediate outputs differ from the expected output, illustrating that the network structure does not provide a faithful explanation of model behaviour. To remedy that, we train the model with auxiliary supervision and propose particular choices for module architecture that yield much better faithfulness, at a minimal cost to accuracy.",
    "IsOld": true
  },
  {
    "Paper-ID": "acl/WuCKL20",
    "Title": "Perturbed Masking - Parameter-free Probing for Analyzing and Interpreting BERT.",
    "url": "https://doi.org/10.18653/v1/2020.acl-main.383",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ACL"
    },
    "Authors": [
      "Zhiyong Wu",
      "Yun Chen",
      "Ben Kao",
      "Qun Liu"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification",
      "Other"
    ],
    "Type of Explanation": [
      "Heatmap",
      "Other"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "By introducing a small set of additional parameters, a probe learns to solve specific linguistic tasks (e.g., dependency parsing) in a supervised manner using feature representations (e.g., contextualized embeddings). The effectiveness of such probing tasks is taken as evidence that the pre-trained model encodes linguistic knowledge. However, this approach of evaluating a language model is undermined by the uncertainty of the amount of knowledge that is learned by the probe itself. Complementary to those works, we propose a parameter-free probing technique for analyzing pre-trained language models (e.g., BERT). Our method does not require direct supervision from the probing tasks, nor do we introduce additional parameters to the probing process. Our experiments on BERT show that syntactic trees recovered from BERT using our method are significantly better than linguistically-uninformed baselines. We further feed the empirically induced dependency structures into a downstream sentiment classification task and find its improvement compatible with or even superior to a human-designed dependency schema.",
    "IsOld": true
  },
  {
    "Paper-ID": "acl/WuRZLN20",
    "Title": "DTCA - Decision Tree-based Co-Attention Networks for Explainable Claim Verification.",
    "url": "https://doi.org/10.18653/v1/2020.acl-main.97",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ACL"
    },
    "Authors": [
      "Lianwei Wu",
      "Yuan Rao",
      "Yongqiang Zhao",
      "Hao Liang",
      "Ambreen Nazir"
    ],
    "Type of Data": [
      "Text",
      "Graph data"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Localization"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Recently, many methods discover effective evidence from reliable sources by appropriate neural networks for explainable claim verification, which has been widely recognized. However, in these methods, the discovery process of evidence is nontransparent and unexplained. Simultaneously, the discovered evidence is aimed at the interpretability of the whole sequence of claims but insufficient to focus on the false parts of claims. In this paper, we propose a Decision Tree-based Co-Attention model (DTCA) to discover evidence for explainable claim verification. Specifically, we first construct Decision Tree-based Evidence model (DTE) to select comments with high credibility as evidence in a transparent and interpretable way. Then we design Co-attention Self-attention networks (CaSa) to make the selected evidence interact with claims, which is for 1) training DTE to determine the optimal decision thresholds and obtain more powerful evidence; and 2) utilizing the evidence to find the false parts in the claim. Experiments on two public datasets, RumourEval and PHEME, demonstrate that DTCA not only provides explanations for the results of claim verification but also achieves the state-of-the-art performance, boosting the F1-score by more than 3.11%, 2.41%, respectively.",
    "IsOld": true
  },
  {
    "Paper-ID": "acl/ZhangSFL20",
    "Title": "Learning Interpretable Relationships between Entities, Relations and Concepts via Bayesian Structure Learning on Open Domain Facts.",
    "url": "https://doi.org/10.18653/v1/2020.acl-main.717",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ACL"
    },
    "Authors": [
      "Jingyuan Zhang",
      "Mingming Sun",
      "Yue Feng",
      "Ping Li"
    ],
    "Type of Data": [
      "Graph data"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "Bayesian or Hierarchical Network"
    ],
    "Type of Task": [
      "Other"
    ],
    "Type of Explanation": [
      "Graph"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Concept graphs are created as universal taxonomies for text understanding in the open-domain knowledge. The nodes in concept graphs include both entities and concepts. The edges are from entities to concepts, showing that an entity is an instance of a concept. In this paper, we propose the task of learning interpretable relationships from open-domain facts to enrich and refine concept graphs. The Bayesian network structures are learned from open-domain facts as the interpretable relationships between relations of facts and concepts of entities. We conduct extensive experiments on public English and Chinese datasets. Compared to the state-of-the-art methods, the learned network structures help improving the identification of concepts for entities based on the relations of entities on both datasets.",
    "IsOld": true
  },
  {
    "Paper-ID": "acl/ZhouZY20",
    "Title": "Interpretable Operational Risk Classification with Semi-Supervised Variational Autoencoder.",
    "url": "https://doi.org/10.18653/v1/2020.acl-main.78",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ACL"
    },
    "Authors": [
      "Fan Zhou",
      "Shengming Zhang",
      "Yi Yang"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Heatmap"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Operational risk management is one of the biggest challenges nowadays faced by financial institutions. There are several major challenges of building a text classification system for automatic operational risk prediction, including imbalanced labeled/unlabeled data and lacking interpretability. To tackle these challenges, we present a semi-supervised text classification framework that integrates multi-head attention mechanism with Semi-supervised variational inference for Operational Risk Classification (SemiORC). We empirically evaluate the framework on a real-world dataset. The results demonstrate that our method can better utilize unlabeled data and learn visually interpretable document representations. SemiORC also outperforms other baseline methods on operational risk classification.",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/ChengRCZ20",
    "Title": "Explaining Knowledge Distillation by Quantifying the Knowledge.",
    "url": "https://doi.org/10.1109/CVPR42600.2020.01294",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "Xu Cheng",
      "Zhefan Rao",
      "Yilan Chen",
      "Quanshi Zhang"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Heatmap",
      "Localization"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "This paper presents a method to interpret the success of knowledge distillation by quantifying and analyzing task-relevant and task-irrelevant visual concepts that are encoded in intermediate layers of a deep neural network (DNN). More specifically, three hypotheses are proposed as follows. 1. Knowledge distillation makes the DNN learn more visual concepts than learning from raw data. 2. Knowledge distillation ensures that the DNN is prone to learning various visual concepts simultaneously. Whereas, in the scenario of learning from raw data, the DNN learns visual concepts sequentially. 3. Knowledge distillation yields more stable optimization directions than learning from raw data. Accordingly, we design three types of mathematical metrics to evaluate feature representations of the DNN. In experiments, we diagnosed various DNNs, and above hypotheses were verified.",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/HuangL20",
    "Title": "Interpretable and Accurate Fine-grained Recognition via Region Grouping.",
    "url": "https://doi.org/10.1109/CVPR42600.2020.00869",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "Zixuan Huang",
      "Yin Li"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Heatmap",
      "Localization"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "We present an interpretable deep model for fine-grained visual recognition. At the core of our method lies the integration of region-based part discovery and attribution within a deep neural network. Our model is trained using image-level object labels, and provides an interpretation of its results via the segmentation of object parts and the identification of their contributions towards classification. To facilitate the learning of object parts without direct supervision, we explore a simple prior of the occurrence of object parts. We demonstrate that this prior, when combined with our region-based part discovery and attribution, leads to an interpretable model that remains highly accurate. Our model is evaluated on major fine-grained recognition datasets, including CUB-200, CelebA and iNaturalist. Our results compares favourably to state-of-the-art methods on classification tasks, and outperforms previous approaches on the localization of object parts.",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/JakabGBV20",
    "Title": "Self-Supervised Learning of Interpretable Keypoints From Unlabelled Videos.",
    "url": "https://doi.org/10.1109/CVPR42600.2020.00881",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "Tomas Jakab",
      "Ankush Gupta",
      "Hakan Bilen",
      "Andrea Vedaldi"
    ],
    "Type of Data": [
      "Images",
      "Video"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Other"
    ],
    "Type of Explanation": [
      "Disentanglement",
      "Localization"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "We propose a new method for recognizing the pose of objects from a single image that for learning uses only unlabelled videos and a weak empirical prior on the object poses. Video frames differ primarily in the pose of the objects they contain, so our method distils the pose information by analyzing the differences between frames. The distillation uses a new dual representation of the geometry of objects as a set of 2D keypoints, and as a pictorial representation, i.e. a skeleton image. This has three benefits: (1) it provides a tight 'geometric bottleneck' which disentangles pose from appearance, (2) it can leverage powerful image-to-image translation networks to map between photometry and geometry, and (3) it allows to incorporate empirical pose priors in the learning process. The pose priors are obtained from unpaired data, such as from a different dataset or modality such as mocap, such that no annotated image is ever used in learning the pose recognition network. In standard benchmarks for pose recognition for humans and faces, our method achieves state-of-the-art performance among methods that do not require any labelled images for training. Project page: http://www.robots.ox.ac.uk/~vgg/research/unsupervised_pose/",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/JalwanaABM20",
    "Title": "Attack to Explain Deep Representation.",
    "url": "https://doi.org/10.1109/CVPR42600.2020.00956",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "Mohammad A. A. K. Jalwana",
      "Naveed Akhtar",
      "Mohammed Bennamoun",
      "Ajmal Mian"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Representation Synthesis"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Deep visual models are susceptible to extremely low magnitude perturbations to input images. Though carefully crafted, the perturbation patterns generally appear noisy, yet they are able to perform controlled manipulation of model predictions. This observation is used to argue that deep representation is misaligned with human perception. This paper counter-argues and proposes the first attack on deep learning that aims at explaining the learned representation instead of fooling it. By extending the input domain of the manipulative signal and employing a model faithful channelling, we iteratively accumulate adversarial perturbations for a deep model. The accumulated signal gradually manifests itself as a collection of visually salient features of the target label (in model fooling), casting adversarial perturbations as primitive features of the target label. Our attack provides the first demonstration of systematically computing perturbations for adversarially non-robust classifiers that comprise salient visual features of objects. We leverage the model explaining character of our algorithm to perform image generation, inpainting and interactive image manipulation by attacking adversarially robust classifiers. The visually appealing results across these applications demonstrate the utility of our attack (and perturbations in general) beyond model fooling.",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/KimGPS20",
    "Title": "A Programmatic and Semantic Approach to Explaining and Debugging Neural Network Based Object Detectors.",
    "url": "https://doi.org/10.1109/CVPR42600.2020.01114",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "Edward Kim",
      "Divya Gopinath",
      "Corina S. Pasareanu",
      "Sanjit A. Seshia"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Explanation",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Decision Rules"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Even as deep neural networks have become very effective for tasks in vision and perception, it remains difficult to explain and debug their behavior. In this paper, we present a programmatic and semantic approach to explaining, understanding, and debugging the correct and incorrect behaviors of a neural network based perception system. Our approach is semantic in that it employs a high-level representation of the distribution of environment scenarios that the detector is intended to work on. It is programmatic in that the representation is a program in a domain-specific probabilistic programming language using which synthetic data can be generated to train and test the neural network. We present a framework that assesses the performance of the neural network to identify correct and incorrect detections, extracts rules from those results that semantically characterizes the correct and incorrect scenarios, and then specializes the probabilistic program with those rules in order to more precisely characterize the scenarios in which the neural network operates correctly or not, without human intervention. We demonstrate our results using the Scenic probabilistic programming language and a neural network-based object detector. Our experiments show that it is possible to automatically generate compact rules that significantly increase the correct detection rate (or conversely the incorrect detection rate) of the network and can thus help with debugging and understanding its behavior.",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/LiuLZKWBRC20",
    "Title": "Towards Visually Explaining Variational Autoencoders.",
    "url": "https://doi.org/10.1109/CVPR42600.2020.00867",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "WenQian Liu",
      "Runze Li",
      "Meng Zheng",
      "Srikrishna Karanam",
      "Ziyan Wu",
      "Bir Bhanu",
      "Richard J. Radke",
      "Octavia I. Camps"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Generation"
    ],
    "Type of Explanation": [
      "Disentanglement",
      "Heatmap"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Recent advances in Convolutional Neural Network (CNN) model interpretability have led to impressive progress in visualizing and understanding model predictions. In particular, gradient-based visual attention methods have driven much recent effort in using visual attention maps as a means for visual explanations. A key problem, however, is these methods are designed for classification and categorization tasks, and their extension to explaining generative models, e.g., variational autoencoders (VAE) is not trivial. In this work, we take a step towards bridging this crucial gap, proposing the first technique to visually explain VAEs by means of gradient-based attention. We present methods to generate visual attention from the learned latent space, and also demonstrate such attention explanations serve more than just explaining VAE predictions. We show how these attention maps can be used to localize anomalies in images, demonstrating state-of-the-art performance on the MVTec-AD dataset. We also show how they can be infused into model training, helping bootstrap the VAE into learning improved latent space disentanglement, demonstrated on the Dsprites dataset.",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/ShenGTZ20",
    "Title": "Interpreting the Latent Space of GANs for Semantic Face Editing.",
    "url": "https://doi.org/10.1109/CVPR42600.2020.00926",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "Yujun Shen",
      "Jinjin Gu",
      "Xiaoou Tang",
      "Bolei Zhou"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Generation"
    ],
    "Type of Explanation": [
      "Disentanglement"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Despite the recent advance of Generative Adversarial Networks (GANs) in high-fidelity image synthesis, there lacks enough understanding of how GANs are able to map a latent code sampled from a random distribution to a photo-realistic image. Previous work assumes the latent space learned by GANs follows a distributed representation but observes the vector arithmetic phenomenon. In this work, we propose a novel framework, called InterFaceGAN, for semantic face editing by interpreting the latent semantics learned by GANs. In this framework, we conduct a detailed study on how different semantics are encoded in the latent space of GANs for face synthesis. We find that the latent code of well-trained generative models actually learns a disentangled representation after linear transformations. We explore the disentanglement between various semantics and manage to decouple some entangled semantics with subspace projection, leading to more precise control of facial attributes. Besides manipulating gender, age, expression, and the presence of eyeglasses, we can even vary the face pose as well as fix the artifacts accidentally generated by GAN models. The proposed method is further applied to achieve real image manipulation when combined with GAN inversion methods or some encoder-involved models. Extensive results suggest that learning to synthesize faces spontaneously brings a disentangled and controllable facial attribute representation.",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/WangV20",
    "Title": "SCOUT - Self-Aware Discriminant Counterfactual Explanations.",
    "url": "https://doi.org/10.1109/CVPR42600.2020.00900",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "Pei Wang",
      "Nuno Vasconcelos"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Localization"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "The problem of counterfactual visual explanations is considered. A new family of discriminant explanations is introduced. These produce heatmaps that attribute high scores to image regions informative of a classifier prediction but not of a counter class. They connect attributive explanations, which are based on a single heat map, to counterfactual explanations, which account for both predicted class and counter class. The latter are shown to be computable by combination of two discriminant explanations, with reversed class pairs. It is argued that self-awareness, namely the ability to produce classification confidence scores, is important for the computation of discriminant explanations, which seek to identify regions where it is easy to discriminate between prediction and counter class. This suggests the computation of discriminant explanations by the combination of three attribution maps. The resulting counterfactual explanations are optimization free and thus much faster than previous methods. To address the difficulty of their evaluation, a proxy task and set of quantitative metrics are also proposed. Experiments under this protocol show that the proposed counterfactual explanations outperform the state of the art while achieving speeds much faster, for popular networks. In a human-learning machine teaching experiment, they are also shown to improve mean student accuracy from chance level to 95%.",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/WuSCZKLT20a",
    "Title": "Towards Global Explanations of Convolutional Neural Networks With Concept Attribution.",
    "url": "https://doi.org/10.1109/CVPR42600.2020.00868",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "Weibin Wu",
      "Yuxin Su",
      "Xixian Chen",
      "Shenglin Zhao",
      "Irwin King",
      "Michael R. Lyu",
      "Yu-Wing Tai"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Representation Synthesis"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "With the growing prevalence of convolutional neural networks (CNNs), there is an urgent demand to explain their behaviors. Global explanations contribute to understanding model predictions on a whole category of samples, and thus have attracted increasing interest recently. However, existing methods overwhelmingly conduct separate input attribution or rely on local approximations of models, making them fail to offer faithful global explanations of CNNs. To overcome such drawbacks, we propose a novel two-stage framework, Attacking for Interpretability (AfI), which explains model decisions in terms of the importance of user-defined concepts. AfI first conducts a feature occlusion analysis, which resembles a process of attacking models to derive the category-wide importance of different features. We then map the feature importance to concept importance through ad-hoc semantic tasks. Experimental results confirm the effectiveness of AfI and its superiority in providing more accurate estimations of concept importance than existing proposals.",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/XuYGLWLV20",
    "Title": "Explainable Object-Induced Action Decision for Autonomous Vehicles.",
    "url": "https://doi.org/10.1109/CVPR42600.2020.00954",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "Yiran Xu",
      "Xiaoyin Yang",
      "Lihang Gong",
      "Hsuan-Chu Lin",
      "Tz-Ying Wu",
      "Yunsheng Li",
      "Nuno Vasconcelos"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Localization",
      "Text"
    ],
    "Method used to explain": [
      "Supervised explanation training"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "A new paradigm is proposed for autonomous driving. The new paradigm lies between the end-to-end and pipelined approaches, and is inspired by how humans solve the problem. While it relies on scene understanding, the latter only considers objects that could originate hazard. These are denoted as action inducing, since changes in their state should trigger vehicle actions. They also define a set of explanations for these actions, which should be produced jointly with the latter. An extension of the BDD100K dataset, annotated for a set of 4 actions and 21 explanations, is proposed. A new multi-task formulation of the problem, which optimizes the accuracy of both action commands and explanations, is then introduced. A CNN architecture is finally proposed to solve this problem, by combining reasoning about action inducing objects and global scene context. Experimental results show that the requirement of explanations improves the recognition of action-inducing objects, which in turn leads to better action predictions.",
    "IsOld": true
  },
  {
    "Paper-ID": "icdm/JiWJMZ20",
    "Title": "Interpretable Spatiotemporal Deep Learning Model for Traffic Flow Prediction based on Potential Energy Fields.",
    "url": "https://doi.org/10.1109/ICDM50108.2020.00128",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ICDM"
    },
    "Authors": [
      "Jiahao Ji",
      "Jingyuan Wang",
      "Zhe Jiang",
      "Jingtian Ma",
      "Hu Zhang"
    ],
    "Type of Data": [
      "Time series"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Heatmap"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Traffic flow prediction is of great importance in traffic management and public safety, but is challenging due to the complex spatial-temporal dependencies as well as temporal dynamics. Existing work either focuses on traditional statistical models, which have limited prediction accuracy, or relies on black-box deep learning models, which have superior prediction accuracy but are hard to interpret. In contrast, we propose a novel interpretable spatiotemporal deep learning model for traffic flow prediction. Our main idea is to model the physics of traffic flow through a number of latent Spatio-Temporal Potential Energy Fields (ST-PEFs), similar to water flow driven by the gravity field. We develop a Wind field Decomposition (WD) algorithm to decompose traffic flow into poly-tree components so that ST-PEFs can be established. We then design a spatiotemporal deep learning model for the ST-PEFs, which consists of a temporal component (modeling the temporal correlation) and a spatial component (modeling the spatial dependencies). To the best of our knowledge, this is the first work that make traffic flow prediction based on ST-PEFs. Experimental results on real-world traffic datasets show the effectiveness of our model compared to the existing methods. A case study confirms our model interpretability.",
    "IsOld": true
  },
  {
    "Paper-ID": "iclr/JinWDXR20",
    "Title": "Towards Hierarchical Importance Attribution - Explaining Compositional Semantics for Neural Sequence Models.",
    "url": "https://openreview.net/forum?id=BkxRRkSKwr",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ICLR"
    },
    "Authors": [
      "Xisen Jin",
      "Zhongyu Wei",
      "Junyi Du",
      "Xiangyang Xue",
      "Xiang Ren"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Graph"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "iclr/MohanKSF20",
    "Title": "Robust And Interpretable Blind Image Denoising Via Bias-Free Convolutional Neural Networks.",
    "url": "https://openreview.net/forum?id=HJlSmC4FPS",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ICLR"
    },
    "Authors": [
      "Sreyas Mohan",
      "Zahra Kadkhodaie",
      "Eero P. Simoncelli",
      "Carlos Fernandez-Granda"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Other"
    ],
    "Type of Explanation": [
      "Representation Synthesis",
      "Other"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "iclr/OreshkinCCB20",
    "Title": "N-BEATS - Neural basis expansion analysis for interpretable time series forecasting.",
    "url": "https://openreview.net/forum?id=r1ecqn4YwB",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ICLR"
    },
    "Authors": [
      "Boris N. Oreshkin",
      "Dmitri Carpov",
      "Nicolas Chapados",
      "Yoshua Bengio"
    ],
    "Type of Data": [
      "Time series"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Regression"
    ],
    "Type of Explanation": [
      "Feature plot"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "iclr/PuriVGKDK020",
    "Title": "Explain Your Move - Understanding Agent Actions Using Specific and Relevant Feature Attribution.",
    "url": "https://openreview.net/forum?id=SJgzLkBKPB",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ICLR"
    },
    "Authors": [
      "Nikaash Puri",
      "Sukriti Verma",
      "Piyush Gupta",
      "Dhruv Kayastha",
      "Shripad Deshmukh",
      "Balaji Krishnamurthy",
      "Sameer Singh"
    ],
    "Type of Data": [
      "Images",
      "Other"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Policy learning"
    ],
    "Type of Explanation": [
      "Heatmap"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "iclr/SinglaPCB20",
    "Title": "Explanation by Progressive Exaggeration.",
    "url": "https://openreview.net/forum?id=H1xFWgrFPS",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ICLR"
    },
    "Authors": [
      "Sumedha Singla",
      "Brian Pollack",
      "Junxiang Chen",
      "Kayhan Batmanghelich"
    ],
    "Type of Data": [
      "Images",
      "Any"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Representation Synthesis",
      "Representation Visualization",
      "Heatmap"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "iclr/YangS20",
    "Title": "Learn to Explain Efficiently via Neural Logic Inductive Learning.",
    "url": "https://openreview.net/forum?id=SJlh8CEYDB",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ICLR"
    },
    "Authors": [
      "Yuan Yang",
      "Le Song"
    ],
    "Type of Data": [
      "Images",
      "Graph data"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Decision Rules"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "icml/AndersPDMK20",
    "Title": "Fairwashing explanations with off-manifold detergent.",
    "url": "http://proceedings.mlr.press/v119/anders20a.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ICML"
    },
    "Authors": [
      "Christopher J. Anders",
      "Plamen Pasliev",
      "Ann-Kathrin Dombrowski",
      "Klaus-Robert M\u00fcller",
      "Pan Kessel"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Any (for a specific task); model-agnostic",
      "Logistic Regression"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Heatmap"
    ],
    "Method used to explain": [
      "Post-hoc explanation method",
      "Supervised explanation training"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "icml/ChalasaniC00J20",
    "Title": "Concise Explanations of Neural Networks using Adversarial Training.",
    "url": "http://proceedings.mlr.press/v119/chalasani20a.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ICML"
    },
    "Authors": [
      "Prasad Chalasani",
      "Jiefeng Chen",
      "Amrita Roy Chowdhury",
      "Xi Wu",
      "Somesh Jha"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Heatmap"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "icml/ChaudharySG20",
    "Title": "Explainable and Discourse Topic-aware Neural Language Understanding.",
    "url": "http://proceedings.mlr.press/v119/chaudhary20a.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ICML"
    },
    "Authors": [
      "Yatin Chaudhary",
      "Hinrich Sch\u00fctze",
      "Pankaj Gupta"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification",
      "Retrieval"
    ],
    "Type of Explanation": [
      "Other"
    ],
    "Method used to explain": [
      "Supervised explanation training"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "icml/JinBJ20a",
    "Title": "Multi-Objective Molecule Generation using Interpretable Substructures.",
    "url": "http://proceedings.mlr.press/v119/jin20b.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ICML"
    },
    "Authors": [
      "Wengong Jin",
      "Regina Barzilay",
      "Tommi S. Jaakkola"
    ],
    "Type of Data": [
      "Graph data"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Generation"
    ],
    "Type of Explanation": [
      "Prototypes"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "icml/LakkarajuAB20",
    "Title": "Robust and Stable Black Box Explanations.",
    "url": "http://proceedings.mlr.press/v119/lakkaraju20a.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ICML"
    },
    "Authors": [
      "Himabindu Lakkaraju",
      "Nino Arsov",
      "Osbert Bastani"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Model Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Tree Ensemble",
      "Support Vector Machine",
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Decision Rules",
      "White-box model"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "icml/MoshkovitzDRF20",
    "Title": "Explainable k-Means and k-Medians Clustering.",
    "url": "http://proceedings.mlr.press/v119/moshkovitz20a.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ICML"
    },
    "Authors": [
      "Michal Moshkovitz",
      "Sanjoy Dasgupta",
      "Cyrus Rashtchian",
      "Nave Frost"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Clustering"
    ],
    "Type of Explanation": [
      "Decision Tree"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "icml/ParkCZYY20",
    "Title": "Multiresolution Tensor Learning for Efficient and Interpretable Spatial Analysis.",
    "url": "http://proceedings.mlr.press/v119/park20a.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ICML"
    },
    "Authors": [
      "Jung Yeon Park",
      "Kenneth Theo Carr",
      "Stephan Zheng",
      "Yisong Yue",
      "Rose Yu"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification",
      "Regression"
    ],
    "Type of Explanation": [
      "Heatmap"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "icml/PlumbTST20",
    "Title": "Explaining Groups of Points in Low-Dimensional Representations.",
    "url": "http://proceedings.mlr.press/v119/plumb20a.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ICML"
    },
    "Authors": [
      "Gregory Plumb",
      "Jonathan Terhorst",
      "Sriram Sankararaman",
      "Ameet Talwalkar"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Representation learning"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Representation Visualization"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "icml/QuinnNR0V20",
    "Title": "DeepCoDA - personalized interpretability for compositional health data.",
    "url": "http://proceedings.mlr.press/v119/quinn20a.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ICML"
    },
    "Authors": [
      "Thomas P. Quinn",
      "Dang Nguyen",
      "Santu Rana",
      "Sunil Gupta",
      "Svetha Venkatesh"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "White-box model"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "icml/ShiZM020",
    "Title": "Dispersed Exponential Family Mixture VAEs for Interpretable Text Generation.",
    "url": "http://proceedings.mlr.press/v119/shi20f.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ICML"
    },
    "Authors": [
      "Wenxian Shi",
      "Hao Zhou",
      "Ning Miao",
      "Lei Li"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Generation"
    ],
    "Type of Explanation": [
      "Disentanglement"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "icml/VoynovB20",
    "Title": "Unsupervised Discovery of Interpretable Directions in the GAN Latent Space.",
    "url": "http://proceedings.mlr.press/v119/voynov20a.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ICML"
    },
    "Authors": [
      "Andrey Voynov",
      "Artem Babenko"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Generation"
    ],
    "Type of Explanation": [
      "Disentanglement"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "ijcai/AlbiniRBT20",
    "Title": "Relation-Based Counterfactual Explanations for Bayesian Network Classifiers.",
    "url": "https://doi.org/10.24963/ijcai.2020/63",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "IJCAI"
    },
    "Authors": [
      "Emanuele Albini",
      "Antonio Rago",
      "Pietro Baroni",
      "Francesca Toni"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Bayesian or Hierarchical Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Graph"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "We propose a general method for generating counterfactual explanations (CFXs) for a range of Bayesian Network Classifiers (BCs), e.g. singleor multi-label, binary or multidimensional. We focus on explanations built from relations of (critical and potential) influence between variables, indicating the reasons for classifications, rather than any probabilistic information. We show by means of a theoretical analysis of CFXs\u2019 properties that they serve the purpose of indicating (potentially) pivotal factors in the classification process, whose absence would give rise to different classifications. We then prove empirically for various BCs that CFXs provide useful information in real world settings, e.g. when race plays a part in parole violation prediction, and show that they have inherent advantages over existing explanation methods in the literature.",
    "IsOld": true
  },
  {
    "Paper-ID": "ijcai/BeyazitTYT020",
    "Title": "Learning Interpretable Representations with Informative Entanglements.",
    "url": "https://doi.org/10.24963/ijcai.2020/273",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "IJCAI"
    },
    "Authors": [
      "Ege Beyazit",
      "Doruk Tuncel",
      "Xu Yuan",
      "Nian-Feng Tzeng",
      "Xindong Wu"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Representation learning",
      "Generation"
    ],
    "Type of Explanation": [
      "Disentanglement"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Learning interpretable representations in an unsupervised setting is an important yet a challenging task. Existing unsupervised interpretable methods focus on extracting independent salient features from data. However they miss out the fact that the entanglement of salient features may also be informative. Acknowledging these entanglements can improve the interpretability, resulting in extraction of higher quality and a wider variety of salient features. In this paper, we propose a new method to enable Generative Adversarial Networks (GANs) to discover salient features that may be entangled in an informative manner, instead of extracting only disentangled features. Specifically, we propose a regularizer to punish the disagreement between the extracted feature interactions and a given dependency structure while training. We model these interactions using a Bayesian network, estimate the maximum likelihood parameters and calculate a negative likelihood score to measure the disagreement. Upon qualitatively and quantitatively evaluating the proposed method using both synthetic and real-world datasets, we show that our proposed regularizer guides GANs to learn representations with disentanglement scores competing with the state-of-the-art, while extracting a wider variety of salient features.",
    "IsOld": true
  },
  {
    "Paper-ID": "ijcai/ChenW0PSAC20",
    "Title": "Towards Explainable Conversational Recommendation.",
    "url": "https://doi.org/10.24963/ijcai.2020/414",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "IJCAI"
    },
    "Authors": [
      "Zhongxia Chen",
      "Xiting Wang",
      "Xing Xie",
      "Mehul Parsana",
      "Akshay Soni",
      "Xiang Ao",
      "Enhong Chen"
    ],
    "Type of Data": [
      "User-item matrix"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Other"
    ],
    "Type of Task": [
      "Recommendation"
    ],
    "Type of Explanation": [
      "Text"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Recent studies have shown that both accuracy and explainability are important for recommendation. In this paper, we introduce explainable conversational recommendation, which enables incremental improvement of both recommendation accuracy and explanation quality through multi-turn user model conversation. We show how the problem can be formulated, and design an incremental multitask learning framework that enables tight collaboration between recommendation prediction, explanation generation, and user feedback integration. We also propose a multi-view feedback integration method to enable effective incremental model update. Empirical results demonstrate that our model not only consistently improves the recommendation accuracy but also generates explanations that fit user interests reflected in the feedbacks.",
    "IsOld": true
  },
  {
    "Paper-ID": "ijcai/CiravegnaGGMM20",
    "Title": "Human-Driven FOL Explanations of Deep Learning.",
    "url": "https://doi.org/10.24963/ijcai.2020/309",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "IJCAI"
    },
    "Authors": [
      "Gabriele Ciravegna",
      "Francesco Giannini",
      "Marco Gori",
      "Marco Maggini",
      "Stefano Melacci"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Decision Rules"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Deep neural networks are usually considered blackboxes due to their complex internal architecture, that cannot straightforwardly provide humanunderstandable explanations on how they behave. Indeed, Deep Learning is still viewed with skepticism in those real-world domains in which incorrect predictions may produce critical effects. This is one of the reasons why in the last few years Explainable Artificial Intelligence (XAI) techniques have gained a lot of attention in the scientific community. In this paper, we focus on the case of multilabel classification, proposing a neural network that learns the relationships among the predictors associated to each class, yielding First-Order Logic (FOL)-based descriptions. Both the explanationrelated network and the classification-related network are jointly learned, thus implicitly introducing a latent dependency between the development of the explanation mechanism and the development of the classifiers. Our model can integrate human-driven preferences that guide the learningto-explain process, and it is presented in a unified framework. Different typologies of explanations are evaluated in distinct experiments, showing that the proposed approach discovers new knowledge and can improve the classifier performance.",
    "IsOld": true
  },
  {
    "Paper-ID": "ijcai/KanamoriTKA20",
    "Title": "DACE - Distribution-Aware Counterfactual Explanation by Mixed-Integer Linear Optimization.",
    "url": "https://doi.org/10.24963/ijcai.2020/395",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "IJCAI"
    },
    "Authors": [
      "Kentaro Kanamori",
      "Takuya Takagi",
      "Ken Kobayashi",
      "Hiroki Arimura"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Any (for a specific task); model-agnostic",
      "Support Vector Machine",
      "Tree Ensemble"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Prototypes"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Counterfactual Explanation (CE) is one of the posthoc explanation methods that provides a perturbation vector so as to alter the prediction result obtained from a classifier. Users can directly interpret the perturbation as an \u201daction\u201d for obtaining their desired decision results. However, an action extracted by existing methods often becomes unrealistic for users because they do not adequately care about the characteristics corresponding to the empirical data distribution such as feature-correlations and outlier risk. To suggest an executable action for users, we propose a new framework of CE for extracting an action by evaluating its reality on the empirical data distribution. The key idea of our proposed method is to define a new cost function based on the Mahalanobis\u2019 distance and the local outlier factor. Then, we propose a mixed-integer linear optimization approach to extracting an optimal action by minimizing our cost function. By experiments on real datasets, we confirm the effectiveness of our method in comparison with existing methods for CE.",
    "IsOld": true
  },
  {
    "Paper-ID": "ijcai/LeL20",
    "Title": "Synthesizing Aspect-Driven Recommendation Explanations from Reviews.",
    "url": "https://doi.org/10.24963/ijcai.2020/336",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "IJCAI"
    },
    "Authors": [
      "Trung-Hoang Le",
      "Hady W. Lauw"
    ],
    "Type of Data": [
      "User-item matrix"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Recommendation"
    ],
    "Type of Explanation": [
      "Text"
    ],
    "Method used to explain": [
      "Post-hoc explanation method",
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Explanations help to make sense of recommendations, increasing the likelihood of adoption. However, existing approaches to explainable recommendations tend to rely on rigid, standardized templates, customized only via fill-in-the-blank aspect sentiments. For more flexible, literate, and varied explanations covering various aspects of interest, we synthesize an explanation by selecting snippets from reviews, while optimizing for representativeness and coherence. To fit target users\u2019 aspect preferences, we contextualize the opinions based on a compatible explainable recommendation model. Experiments on datasets of several product categories showcase the efficacies of our method as compared to baselines based on templates, review summarization, selection, and text generation.",
    "IsOld": true
  },
  {
    "Paper-ID": "ijcai/LiFCLYS20",
    "Title": "Recurrent Dirichlet Belief Networks for interpretable Dynamic Relational Data Modelling.",
    "url": "https://doi.org/10.24963/ijcai.2020/342",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "IJCAI"
    },
    "Authors": [
      "Yaqiong Li",
      "Xuhui Fan",
      "Ling Chen",
      "Bin Li",
      "Zheng Yu",
      "Scott A. Sisson"
    ],
    "Type of Data": [
      "Graph data"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "Bayesian or Hierarchical Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "White-box model"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "The Dirichlet Belief Network~(DirBN) has been recently proposed as a promising approach in learning interpretable deep latent representations for objects. \n\nIn this work, we leverage its interpretable modelling architecture and propose a deep dynamic probabilistic framework -- the Recurrent Dirichlet Belief Network~(Recurrent-DBN) -- to study interpretable hidden structures from dynamic relational data. The proposed Recurrent-DBN has the following merits: (1) it infers interpretable and organised hierarchical latent structures for objects within and across time steps; (2) it enables recurrent long-term temporal dependence modelling, which outperforms the one-order Markov descriptions in most of the dynamic probabilistic frameworks; (3) the computational cost scales to the number of positive links only. In addition, we develop a new inference strategy, which first upward-and-backward propagates latent counts and then downward-and-forward samples variables, to enable efficient Gibbs sampling for the Recurrent-DBN. We apply the Recurrent-DBN to dynamic relational data problems. The extensive experiment results on real-world data validate the advantages of the Recurrent-DBN over the state-of-the-art models in interpretable latent structure discovery and improved link prediction performance.",
    "IsOld": true
  },
  {
    "Paper-ID": "ijcai/RosaCN20",
    "Title": "Explainable Inference on Sequential Data via Memory-Tracking.",
    "url": "https://doi.org/10.24963/ijcai.2020/278",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "IJCAI"
    },
    "Authors": [
      "Biagio La Rosa",
      "Roberto Capobianco",
      "Daniele Nardi"
    ],
    "Type of Data": [
      "Other"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Policy learning"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "In this paper we present a novel mechanism to\n\nget explanations that allow to better understand\n\nnetwork predictions when dealing with sequential\n\ndata. Specifically, we adopt memory-based networks\n\n\u2014 Differential Neural Computers \u2014 to exploit \n\ntheir capability of storing data in memory and\n\nreusing it for inference. By tracking both the memory \n\naccess at prediction time, and the information\n\nstored by the network at each step of the input\n\nsequence, we can retrieve the most relevant input\n\nsteps associated to each prediction. We validate\n\nour approach (1) on a modified T-maze, which is a\n\nnon-Markovian discrete control task evaluating an\n\nalgorithm\u2019s ability to correlate events far apart in\n\nhistory, and (2) on the Story Cloze Test, which is\n\na commonsense reasoning framework for evaluating \n\nstory understanding that requires a system to\n\nchoose the correct ending to a four-sentence story.\n\nOur results show that we are able to explain agent\u2019s\n\ndecisions in (1) and to reconstruct the most relevant\n\nsentences used by the network to select the story\n\nending in (2). Additionally, we show not only that\n\nby removing those sentences the network prediction\n\nchanges, but also that the same are sufficient to\n\nreproduce the inference.",
    "IsOld": true
  },
  {
    "Paper-ID": "ijcai/WangZ20a",
    "Title": "Interpretable Multimodal Learning for Intelligent Regulation in Online Payment Systems.",
    "url": "https://doi.org/10.24963/ijcai.2020/645",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "IJCAI"
    },
    "Authors": [
      "Shuoyao Wang",
      "Diwei Zhu"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Text"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Retrieval"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Feature plot"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "With the explosive growth of transaction activities in online payment systems, effective and real-time regulation becomes a critical problem for payment service providers. Thanks to the rapid development of artificial intelligence (AI), AI-enable regulation emerges as a promising solution. \n\nOne main challenge of the AI-enabled regulation is how to utilize multimedia information, i.e., multimodal signals, in Financial Technology (FinTech).\n\nInspired by the attention mechanism in nature language processing, we propose a novel cross-modal and intra-modal attention network (CIAN) to investigate the relation between the text and transaction.\n\nMore specifically, we integrate the text and transaction information to enhance the text-trade joint-embedding learning, which clusters positive pairs and push negative pairs away from each other.\n\nAnother challenge of intelligent regulation is the interpretability of complicated machine learning models. To sustain the requirements of financial regulation, we design a CIAN-Explainer to interpret how the attention mechanism interacts the original features, which is formulated as a low-rank matrix approximation problem.\n\nWith the real datasets from the largest online payment system, WeChat Pay of Tencent, we conduct experiments to validate the practical application value of CIAN, where our method outperforms the state-of-the-art methods.",
    "IsOld": true
  },
  {
    "Paper-ID": "ijcai/WuRYWN20",
    "Title": "Evidence-Aware Hierarchical Interactive Attention Networks for Explainable Claim Verification.",
    "url": "https://doi.org/10.24963/ijcai.2020/193",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "IJCAI"
    },
    "Authors": [
      "Lianwei Wu",
      "Yuan Rao",
      "Xiong Yang",
      "Wanzhen Wang",
      "Ambreen Nazir"
    ],
    "Type of Data": [
      "Text",
      "Graph data",
      "Other"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Exploring evidence from relevant articles to confirm the veracity of claims is a trend towards explainable claim verification. However, most strategies capture the top-k check-worthy articles or salient words as evidence, but this evidence is difficult to focus on the questionable parts of unverified claims. Besides, they utilize relevant articles indiscriminately, ignoring the source credibility of these articles, which may cause quiet a few unreliable articles to interfere with the assessment results. In this paper, we propose Evidence-aware Hierarchical Interactive Attention Networks (EHIAN) by considering the capture of evidence fragments and the fusion of source credibility to explore more credible evidence semantics discussing the questionable parts of claims for explainable claim verification. EHIAN first designs internal interaction layer (IIL) to strengthen deep interaction and matching between claims and relevant articles for obtaining key evidence fragments, and then proposes global inference layer (GIL) that fuses source features of articles and interacts globally with the average semantics of all articles and finally earns the more credible evidence semantics discussing the questionable parts of claims. Experiments on two datasets demonstrate that EHIAN not only achieves the state-of-the-art performance but also secures effective evidence to explain the results.",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/GuoZQWSY20",
    "Title": "Interpretable Deep Graph Generation with Node-edge Co-disentanglement.",
    "url": "https://doi.org/10.1145/3394486.3403221",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Xiaojie Guo",
      "Liang Zhao",
      "Zhao Qin",
      "Lingfei Wu",
      "Amarda Shehu",
      "Yanfang Ye"
    ],
    "Type of Data": [
      "Graph data"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Generation",
      "Representation learning"
    ],
    "Type of Explanation": [
      "Disentanglement"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Disentangled representation learning has recently attracted a significant amount of attention, particularly in the field of image representation learning. However, learning the disentangled representations behind a graph remains largely unexplored, especially for the attributed graph with both node and edge features. Disentanglement learning for graph generation has substantial new challenges including 1) the lack of graph deconvolution operations to jointly decode node and edge attributes; and 2) the difficulty in enforcing the disentanglement among latent factors that respectively influence: i) only nodes, ii) only edges, and iii) joint patterns between them. To address these challenges, we propose a new disentanglement enhancement framework for deep generative models for attributed graphs. In particular, a novel variational objective is proposed to disentangle the above three types of latent factors, with novel architecture for node and edge deconvolutions. Qualitative and quantitative experiments on both synthetic and real-world datasets demonstrate the effectiveness of the proposed model and its extensions.",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/LancianoBG20",
    "Title": "Explainable Classification of Brain Networks via Contrast Subgraphs.",
    "url": "https://doi.org/10.1145/3394486.3403383",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Tommaso Lanciano",
      "Francesco Bonchi",
      "Aristides Gionis"
    ],
    "Type of Data": [
      "Images",
      "Graph data"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "Bayesian or Hierarchical Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature plot",
      "Graph",
      "Localization"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Mining human-brain networks to discover patterns that can be used to discriminate between healthy individuals and patients affected by some neurological disorder, is a fundamental task in neuro-science. Learning simple and interpretable models is as important as mere classification accuracy. In this paper we introduce a novel approach for classifying brain networks based on extracting contrast subgraphs, i.e., a set of vertices whose induced subgraphs are dense in one class of graphs and sparse in the other. We formally define the problem and present an algorithmic solution for extracting contrast subgraphs. We then apply our method to a brain-network dataset consisting of children affected by Autism Spectrum Disorder and children Typically Developed. Our analysis confirms the interestingness of the discovered patterns, which match background knowledge in the neuro-science literature. Further analysis on other classification tasks confirm the simplicity, soundness, and high explainability of our proposal, which also exhibits superior classification accuracy, to more complex state-of-the-art methods.",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/LeW020",
    "Title": "GRACE - Generating Concise and Informative Contrastive Sample to Explain Neural Network Model's Prediction.",
    "url": "https://doi.org/10.1145/3394486.3403066",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Thai Le",
      "Suhang Wang",
      "Dongwon Lee"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Text"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Despite the recent development in the topic of explainable AI/ML for image and text data, the majority of current solutions are not suitable to explain the prediction of neural network models when the datasets are tabular and their features are in high-dimensional vectorized formats. To mitigate this limitation, therefore, we borrow two notable ideas (i.e., \"explanation by intervention\" from causality and \"explanation are contrastive\" from philosophy) and propose a novel solution, named as GRACE, that better explains neural network models' predictions for tabular datasets. In particular, given a model's prediction as label X, GRACE intervenes and generates a minimally-modified contrastive sample to be classified as Y, with an intuitive textual explanation, answering the question of \"Why X rather than Y?\" We carry out comprehensive experiments using eleven public datasets of different scales and domains (e.g., # of features ranges from 5 to 216) and compare GRACE with competing baselines on different measures: fidelity, conciseness, info-gain, and influence. The user-studies show that our generated explanation is not only more intuitive and easy-to-understand but also facilitates end-users to make as much as 60% more accurate post-explanation decisions than that of Lime.",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/LiangBCBW20",
    "Title": "Adversarial Infidelity Learning for Model Interpretation.",
    "url": "https://doi.org/10.1145/3394486.3403071",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Jian Liang",
      "Bing Bai",
      "Yuren Cao",
      "Kun Bai",
      "Fei Wang"
    ],
    "Type of Data": [
      "Images",
      "Text",
      "Time series",
      "Any"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Localization"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Model interpretation is essential in data mining and knowledge discovery. It can help understand the intrinsic model working mechanism and check if the model has undesired characteristics. A popular way of performing model interpretation is Instance-wise Feature Selection (IFS), which provides an importance score of each feature representing the data samples to explain how the model generates the specific output. In this paper, we propose a Model-agnostic Effective Efficient Direct (MEED) IFS framework for model interpretation, mitigating concerns about sanity, combinatorial shortcuts, model identifiability, and information transmission. Also, we focus on the following setting: using selected features to directly predict the output of the given model, which serves as a primary evaluation metric for model-interpretation methods. Apart from the features, we involve the output of the given model as an additional input to learn an explainer based on more accurate information. To learn the explainer, besides fidelity, we propose an Adversarial Infidelity Learning (AIL) mechanism to boost the explanation learning by screening relatively unimportant features. Through theoretical and experimental analysis, we show that our AIL mechanism can help learn the desired conditional distribution between selected features and targets. Moreover, we extend our framework by integrating efficient interpretation methods as proper priors to provide a warm start. Comprehensive empirical evaluation results are provided by quantitative metrics and human evaluation to demonstrate the effectiveness and superiority of our proposed method. Our code is publicly available online at https://github.com/langlrsw/MEED.",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/TangLSSMW20",
    "Title": "Knowing your FATE - Friendship, Action and Temporal Explanations for User Engagement Prediction on Social Apps.",
    "url": "https://doi.org/10.1145/3394486.3403276",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Xianfeng Tang",
      "Yozen Liu",
      "Neil Shah",
      "Xiaolin Shi",
      "Prasenjit Mitra",
      "Suhang Wang"
    ],
    "Type of Data": [
      "Graph data"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Heatmap"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "With the rapid growth and prevalence of social network applications (Apps) in recent years, understanding user engagement has become increasingly important, to provide useful insights for future App design and development. While several promising neural modeling approaches were recently pioneered for accurate user engagement prediction, their black-box designs are unfortunately limited in model explainability. In this paper, we study a novel problem of explainable user engagement prediction for social network Apps. First, we propose a flexible definition of user engagement for various business scenarios, based on future metric expectations. Next, we design an end-to-end neural framework, FATE, which incorporates three key factors that we identify to influence user engagement, namely friendships, user actions, and temporal dynamics to achieve explainable engagement predictions. FATE is based on a tensor-based graph neural network (GNN), LSTM and a mixture attention mechanism, which allows for (a) predictive explanations based on learned weights across different feature categories, (b) reduced network complexity, and (c) improved performance in both prediction accuracy and training/inference time. We conduct extensive experiments on two large-scale datasets from Snapchat, where FATE outperforms state-of-the-art approaches by 10% error and 20% runtime reduction. We also evaluate explanations from FATE, showing strong quantitative and qualitative performance.",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/YuanTHJ20",
    "Title": "XGNN - Towards Model-Level Explanations of Graph Neural Networks.",
    "url": "https://doi.org/10.1145/3394486.3403085",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Hao Yuan",
      "Jiliang Tang",
      "Xia Hu",
      "Shuiwang Ji"
    ],
    "Type of Data": [
      "Graph data"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Prototypes"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Graphs neural networks (GNNs) learn node features by aggregating and combining neighbor information, which have achieved promising performance on many graph tasks. However, GNNs are mostly treated as black-boxes and lack human intelligible explanations. Thus, they cannot be fully trusted and used in certain application domains if GNN models cannot be explained. In this work, we propose a novel approach, known as XGNN, to interpret GNNs at the model-level. Our approach can provide high-level insights and generic understanding of how GNNs work. In particular, we propose to explain GNNs by training a graph generator so that the generated graph patterns maximize a certain prediction of the model. We formulate the graph generation as a reinforcement learning task, where for each step, the graph generator predicts how to add an edge into the current graph. The graph generator is trained via a policy gradient method based on information from the trained GNNs. In addition, we incorporate several graph rules to encourage the generated graphs to be valid. Experimental results on both synthetic and real-world datasets show that our proposed methods help understand and verify the trained GNNs. Furthermore, our experimental results indicate that the generated graphs can provide guidance on how to improve the trained GNNs.",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/ZhangQ0LCZD20",
    "Title": "INPREM - An Interpretable and Trustworthy Predictive Model for Healthcare.",
    "url": "https://doi.org/10.1145/3394486.3403087",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Xianli Zhang",
      "Buyue Qian",
      "Shilei Cao",
      "Yang Li",
      "Hang Chen",
      "Yefeng Zheng",
      "Ian Davidson"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Time series"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature plot"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Building a predictive model based on historical Electronic Health Records (EHRs) for personalized healthcare has become an active research area. Benefiting from the powerful ability of feature extraction, deep learning (DL) approaches have achieved promising performance in many clinical prediction tasks. However, due to the lack of interpretability and trustworthiness, it is difficult to apply DL in real clinical cases of decision making. To address this, in this paper, we propose an interpretable and trustworthy predictive model~(INPREM) for healthcare. Firstly, INPREM is designed as a linear model for interpretability while encoding non-linear relationships into the learning weights for modeling the dependencies between and within each visit. This enables us to obtain the contribution matrix of the input variables, which is served as the evidence of the prediction result(s), and help physicians understand why the model gives such a prediction, thereby making the model more interpretable. Secondly, for trustworthiness, we place a random gate (which follows a Bernoulli distribution to turn on or off) over each weight of the model, as well as an additional branch to estimate data noises. With the help of the Monto Carlo sampling and an objective function accounting for data noises, the model can capture the uncertainty of each prediction. The captured uncertainty, in turn, allows physicians to know how confident the model is, thus making the model more trustworthy. We empirically demonstrate that the proposed INPREM outperforms existing approaches with a significant margin. A case study is also presented to show how the contribution matrix and the captured uncertainty are used to assist physicians in making robust decisions.",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/0001GCIN20",
    "Title": "Explaining Naive Bayes and Other Linear Classifiers with Polynomial Time and Delay.",
    "url": "https://proceedings.neurips.cc/paper/2020/hash/eccd2a86bae4728b38627162ba297828-Abstract.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Jo\u00e3o Marques-Silva",
      "Thomas Gerspacher",
      "Martin C. Cooper",
      "Alexey Ignatiev",
      "Nina Narodytska"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Bayesian or Hierarchical Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Localization"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/ArikLYSELM0ZNSN20",
    "Title": "Interpretable Sequence Learning for Covid-19 Forecasting.",
    "url": "https://proceedings.neurips.cc/paper/2020/hash/d9dbc51dc534921589adf460c85cd824-Abstract.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Sercan \u00d6mer Arik",
      "Chun-Liang Li",
      "Jinsung Yoon",
      "Rajarishi Sinha",
      "Arkady Epshteyn",
      "Long T. Le",
      "Vikas Menon",
      "Shashank Singh",
      "Leyou Zhang",
      "Martin Nikoltchev",
      "Yash Sonthalia",
      "Hootan Nakhost",
      "Elli Kanal",
      "Tomas Pfister"
    ],
    "Type of Data": [
      "Time series"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Regression"
    ],
    "Type of Explanation": [
      "White-box model"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/BassSSTSR20",
    "Title": "ICAM - Interpretable Classification via Disentangled Representations and Feature Attribution Mapping.",
    "url": "https://proceedings.neurips.cc/paper/2020/hash/56f9f88906aebf4ad985aaec7fa01313-Abstract.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Cher Bass",
      "Mariana da Silva",
      "Carole H. Sudre",
      "Petru-Daniel Tudosiu",
      "Stephen M. Smith",
      "Emma C. Robinson"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification",
      "Representation learning"
    ],
    "Type of Explanation": [
      "Heatmap"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/CrabbeZZS20",
    "Title": "Learning outside the Black-Box - The pursuit of interpretable models.",
    "url": "https://proceedings.neurips.cc/paper/2020/hash/ce758408f6ef98d7c7a7b786eca7b3a8-Abstract.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Jonathan Crabb\u00e9",
      "Yao Zhang",
      "William R. Zame",
      "Mihaela van der Schaar"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Model Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Support Vector Machine",
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/FryeRF20",
    "Title": "Asymmetric Shapley values - incorporating causal knowledge into model-agnostic explainability.",
    "url": "https://proceedings.neurips.cc/paper/2020/hash/0d770c496aa3da6d2c3f2bd19e7b9d6b-Abstract.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Christopher Frye",
      "Colin Rowat",
      "Ilya Feige"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Time series",
      "Any"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/HarkonenHLP20",
    "Title": "GANSpace - Discovering Interpretable GAN Controls.",
    "url": "https://proceedings.neurips.cc/paper/2020/hash/6fe43269967adbb64ec6149852b5cc3e-Abstract.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Erik H\u00e4rk\u00f6nen",
      "Aaron Hertzmann",
      "Jaakko Lehtinen",
      "Sylvain Paris"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Generation"
    ],
    "Type of Explanation": [
      "Disentanglement",
      "Representation Synthesis"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/HeskesSBC20",
    "Title": "Causal Shapley Values - Exploiting Causal Knowledge to Explain Individual Predictions of Complex Models.",
    "url": "https://proceedings.neurips.cc/paper/2020/hash/32e54441e6382a7fbacbbbaf3c450059-Abstract.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Tom Heskes",
      "Evi Sijben",
      "Ioan Gabriel Bucur",
      "Tom Claassen"
    ],
    "Type of Data": [
      "Time series",
      "Any"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Classification",
      "Regression"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/JainVMLTH20",
    "Title": "Interpretable multi-timescale models for predicting fMRI responses to continuous natural speech.",
    "url": "https://proceedings.neurips.cc/paper/2020/hash/9e9a30b74c49d07d8150c8c83b1ccf07-Abstract.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Shailee Jain",
      "Vy A. Vo",
      "Shivangi Mahto",
      "Amanda LeBel",
      "Javier S. Turek",
      "Alexander Huth"
    ],
    "Type of Data": [
      "Time series"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification",
      "Regression"
    ],
    "Type of Explanation": [
      "Heatmap"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/LuoCXYZC020",
    "Title": "Parameterized Explainer for Graph Neural Network.",
    "url": "https://proceedings.neurips.cc/paper/2020/hash/e37b08dd3015330dcbb5d6663667b8b8-Abstract.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Dongsheng Luo",
      "Wei Cheng",
      "Dongkuan Xu",
      "Wenchao Yu",
      "Bo Zong",
      "Haifeng Chen",
      "Xiang Zhang"
    ],
    "Type of Data": [
      "Graph data"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Graph",
      "Feature Importance"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/MuA20",
    "Title": "Compositional Explanations of Neurons.",
    "url": "https://proceedings.neurips.cc/paper/2020/hash/c74956ffb38ba48ed6ce977af6727275-Abstract.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Jesse Mu",
      "Jacob Andreas"
    ],
    "Type of Data": [
      "Images",
      "Text"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Decision Rules",
      "Localization"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/OShaughnessyCCR20",
    "Title": "Generative causal explanations of black-box classifiers.",
    "url": "https://proceedings.neurips.cc/paper/2020/hash/3a93a609b97ec0ab0ff5539eb79ef33a-Abstract.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "\"Matthew R. OShaughnessy\"",
      "Gregory Canal",
      "Marissa Connor",
      "Christopher Rozell",
      "Mark A. Davenport"
    ],
    "Type of Data": [
      "Images",
      "Any"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Disentanglement",
      "Graph",
      "Representation Synthesis"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/PedapatiBSD20",
    "Title": "Learning Global Transparent Models consistent with Local Contrastive Explanations.",
    "url": "https://proceedings.neurips.cc/paper/2020/hash/24aef8cb3281a2422a59b51659f1ad2e-Abstract.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Tejaswini Pedapati",
      "Avinash Balakrishnan",
      "Karthikeyan Shanmugam",
      "Amit Dhurandhar"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Model Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Decision Tree"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/RamamurthyVZD20",
    "Title": "Model Agnostic Multilevel Explanations.",
    "url": "https://proceedings.neurips.cc/paper/2020/hash/426f990b332ef8193a61cc90516c1245-Abstract.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Karthikeyan Natesan Ramamurthy",
      "Bhanukiran Vinzamuri",
      "Yunfeng Zhang",
      "Amit Dhurandhar"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Tree Ensemble",
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Classification",
      "Regression"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/RawalL20",
    "Title": "Beyond Individualized Recourse - Interpretable and Interactive Summaries of Actionable Recourses.",
    "url": "https://proceedings.neurips.cc/paper/2020/hash/8ee7730e97c67473a424ccfeff49ab20-Abstract.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Kaivalya Rawal",
      "Himabindu Lakkaraju"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Model Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Tree Ensemble",
      "Any (for a specific task); model-agnostic",
      "Logistic Regression"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Decision Rules"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/TsangR020",
    "Title": "How does This Interaction Affect Me? Interpretable Attribution for Feature Interactions.",
    "url": "https://proceedings.neurips.cc/paper/2020/hash/443dec3062d0286986e21dc0631734c9-Abstract.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Michael Tsang",
      "Sirisha Rambhatla",
      "Yan Liu"
    ],
    "Type of Data": [
      "Images",
      "Text",
      "Any"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/TsengSK20",
    "Title": "Fourier-transform-based attribution priors improve the interpretability and stability of deep learning models for genomics.",
    "url": "https://proceedings.neurips.cc/paper/2020/hash/1487987e862c44b91a0296cf3866387e-Abstract.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Alex Tseng",
      "Avanti Shrikumar",
      "Anshul Kundaje"
    ],
    "Type of Data": [
      "Any",
      "Other"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/Yau0H20",
    "Title": "What Did You Think Would Happen? Explaining Agent Behaviour through Intended Outcomes.",
    "url": "https://proceedings.neurips.cc/paper/2020/hash/d5ab8dc7ef67ca92e41d730982c5c602-Abstract.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Herman Yau",
      "Chris Russell",
      "Simon Hadfield"
    ],
    "Type of Data": [
      "Other"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Policy learning"
    ],
    "Type of Explanation": [
      "Heatmap"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/YehKALPR20",
    "Title": "On Completeness-aware Concept-Based Explanations in Deep Neural Networks.",
    "url": "https://proceedings.neurips.cc/paper/2020/hash/ecb287ff763c169694f682af52c1f309-Abstract.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Chih-Kuan Yeh",
      "Been Kim",
      "Sercan \u00d6mer Arik",
      "Chun-Liang Li",
      "Tomas Pfister",
      "Pradeep Ravikumar"
    ],
    "Type of Data": [
      "Images",
      "Text",
      "Any"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Prototypes"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/ZhouW20",
    "Title": "Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-VAE.",
    "url": "https://proceedings.neurips.cc/paper/2020/hash/510f2318f324cf07fce24c3a4b89c771-Abstract.html",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Ding Zhou",
      "Xue-Xin Wei"
    ],
    "Type of Data": [
      "Time series"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Other"
    ],
    "Type of Explanation": [
      "Representation Visualization",
      "Feature plot"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "sigir/ChenYYH0020",
    "Title": "Try This Instead - Personalized and Interpretable Substitute Recommendation.",
    "url": "https://doi.org/10.1145/3397271.3401042",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "SIGIR"
    },
    "Authors": [
      "Tong Chen",
      "Hongzhi Yin",
      "Guanhua Ye",
      "Zi Huang",
      "Yang Wang",
      "Meng Wang"
    ],
    "Type of Data": [
      "Text",
      "User-item matrix"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Recommendation"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Text"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "As a fundamental yet significant process in personalized recommendation, candidate generation and suggestion effectively help users spot the most suitable items for them. Consequently, identifying substitutable items that are interchangeable opens up new opportunities to refine the quality of generated candidates. When a user is browsing a specific type of product (e.g., a laptop) to buy, the accurate recommendation of substitutes (e.g., better equipped laptops) can offer the user more suitable options to choose from, thus substantially increasing the chance of a successful purchase. However, existing methods merely treat this problem as mining pairwise item relationships without the consideration of users' personal preferences. Moreover, the substitutable relationships are implicitly identified through the learned latent representations of items, leading to uninterpretable recommendation results. In this paper, we propose attribute-aware collaborative filtering (A2CF) to perform substitute recommendation by addressing issues from both personalization and interpretability perspectives. In A2CF, instead of directly modelling user-item interactions, we extract explicit and polarized item attributes from user reviews with sentiment analysis, whereafter the representations of attributes, users, and items are simultaneously learned. Then, by treating attributes as the bridge between users and items, we can thoroughly model the user-item preferences (i.e., personalization) and item-item relationships (i.e., substitution) for recommendation. In addition, A2CF is capable of generating intuitive interpretations by analyzing which attributes a user currently cares the most and comparing the recommended substitutes with her/his currently browsed items at an attribute level. The recommendation effectiveness and interpretation quality of A2CF are further demonstrated via extensive experiments on three real-life datasets.",
    "IsOld": true
  },
  {
    "Paper-ID": "sigir/QuAW20",
    "Title": "Towards Explainable Retrieval Models for Precision Medicine Literature Search.",
    "url": "https://doi.org/10.1145/3397271.3401277",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "SIGIR"
    },
    "Authors": [
      "Jiaming Qu",
      "Jaime Arguello",
      "Yue Wang"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Retrieval"
    ],
    "Type of Explanation": [
      "Decision Tree"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "In professional search tasks such as precision medicine literature search, queries often involve multiple aspects. To assess the relevance of a document, a searcher often painstakingly validates each aspect in the query and follows a task-specific logic to make a relevance decision. In such scenarios, we say the searcher makes a structured relevance judgment, as opposed to the traditional univariate (binary or graded) relevance judgment. Ideally, a search engine can support searcher's workflow and follow the same steps to predict document relevance. This approach may not only yield highly effective retrieval models, but also open up opportunities for the model to explain its decision in the same \"lingo\" as the searcher. Using structured relevance judgment data from the TREC Precision Medicine track, we propose novel retrieval models that emulate how medical experts make structured relevance judgments. Our experiments demonstrate that these simple, explainable models can outperform complex, black-box learning-to-rank models.",
    "IsOld": true
  },
  {
    "Paper-ID": "sigir/YangZWL020",
    "Title": "Neural Concept Map Generation for Effective Document Classification with Interpretable Structured Summarization.",
    "url": "https://doi.org/10.1145/3397271.3401312",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "SIGIR"
    },
    "Authors": [
      "Carl Yang",
      "Jieyu Zhang",
      "Haonan Wang",
      "Bangzheng Li",
      "Jiawei Han"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Graph",
      "Text"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Concept maps provide concise structured representations for documents regarding their important concepts and interaction links, which have been widely used for document summarization and downstream tasks. However, the construction of concept maps often relies heavily on heuristic design and auxiliary tools. Recent popular neural network models, on the other hand, are shown effective in tasks across various domains, but are short in interpretability and prone to overfitting. In this work, we bridge the gap between concept map construction and neural network models, by designing doc2graph, a novel weakly-supervised text-to-graph neural network, which generates concept maps in the middle and is trained towards document-level tasks like document classification. In our experiments, doc2graph outperforms both its traditional baselines and neural counterparts by significant margins in document classification, while producing high-quality interpretable concept maps as document structured summarization.",
    "IsOld": true
  },
  {
    "Paper-ID": "www/LiJC0WL20",
    "Title": "Directional and Explainable Serendipity Recommendation.",
    "url": "https://doi.org/10.1145/3366423.3380100",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "WWW"
    },
    "Authors": [
      "Xueqi Li",
      "Wenjun Jiang",
      "Weiguang Chen",
      "Jie Wu",
      "Guojun Wang",
      "Kenli Li"
    ],
    "Type of Data": [
      "User-item matrix"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Recommendation"
    ],
    "Type of Explanation": [
      "Localization",
      "Text"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Serendipity recommendation has attracted more and more attention in recent years; it is committed to providing recommendations which could not only cater to users\u2019 demands but also broaden their horizons. However, existing approaches usually measure user-item relevance with a scalar instead of a vector, ignoring user preference direction, which increases the risk of unrelated recommendations. In addition, reasonable explanations increase users\u2019 trust and acceptance, but there is no work to provide explanations for serendipitous recommendations. To address these limitations, we propose a Directional and Explainable Serendipity Recommendation method named DESR. Specifically, we extract users\u2019 long-term preferences with an unsupervised method based on GMM (Gaussian Mixture Model) and capture their short-term demands with the capsule network at first. Then, we propose the serendipity vector to combine long-term preferences with short-term demands and generate directionally serendipitous recommendations with it. Finally, a back-routing scheme is exploited to offer explanations. Extensive experiments on real-world datasets show that DESR could effectively improve the serendipity and explainability, and give impetus to the diversity, compared with existing serendipity-based methods.",
    "IsOld": true
  },
  {
    "Paper-ID": "www/MathewSLS20",
    "Title": "The POLAR Framework - Polar Opposites Enable Interpretability of Pre-Trained Word Embeddings.",
    "url": "https://doi.org/10.1145/3366423.3380227",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "WWW"
    },
    "Authors": [
      "Binny Mathew",
      "Sandipan Sikdar",
      "Florian Lemmerich",
      "Markus Strohmaier"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Tree Ensemble",
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Disentanglement"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "We introduce \u2018POLAR\u2019 \u2014 a framework that adds interpretability to pre-trained word embeddings via the adoption of semantic differentials. Semantic differentials are a psychometric construct for measuring the semantics of a word by analysing its position on a scale between two polar opposites (e.g., cold \u2013 hot, soft \u2013 hard). The core idea of our approach is to transform existing, pre-trained word embeddings via semantic differentials to a new \u201cpolar\u201d space with interpretable dimensions defined by such polar opposites. Our framework also allows for selecting the most discriminative dimensions from a set of polar dimensions provided by an oracle, i.e., an external source. We demonstrate the effectiveness of our framework by deploying it to various downstream tasks, in which our interpretable word embeddings achieve a performance that is comparable to the original word embeddings. We also show that the interpretable dimensions selected by our framework align with human judgement. Together, these results demonstrate that interpretability can be added to word embeddings without compromising performance. Our work is relevant for researchers and engineers interested in interpreting pre-trained word embeddings.",
    "IsOld": true
  },
  {
    "Paper-ID": "www/NaumzikZF20",
    "Title": "Mining Points-of-Interest for Explaining Urban Phenomena - A Scalable Variational Inference Approach.",
    "url": "https://doi.org/10.1145/3366423.3380298",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "WWW"
    },
    "Authors": [
      "Christof Naumzik",
      "Patrick Zoechbauer",
      "Stefan Feuerriegel"
    ],
    "Type of Data": [
      "Other"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Other"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Feature plot",
      "Heatmap"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Points-of-interest (POIs; i.e., restaurants, bars, landmarks, and other entities) are common in web-mined data: they greatly explain the spatial distributions of urban phenomena. The conventional modeling approach relies upon feature engineering, yet it ignores the spatial structure among POIs. In order to overcome this shortcoming, the present paper proposes a novel spatial model for explaining spatial distributions based on web-mined POIs. Our key contributions are: (1) We present a rigorous yet highly interpretable formalization in order to model the influence of POIs on a given outcome variable. Specifically, we accommodate the spatial distributions of both the outcome and POIs. In our case, this modeled by the sum of latent Gaussian processes. (2) In contrast to previous literature, our model infers the influence of POIs without feature engineering, instead we model the influence of POIs via distance-weighted kernel functions with fully learnable parameterizations. (3) We propose a scalable learning algorithm based on sparse variational approximation. For this purpose, we derive a tailored evidence lower bound (ELBO) and, for appropriate likelihoods, we even show that an analytical expression can be obtained. This allows fast and accurate computation of the ELBO. Finally, the value of our approach for web mining is demonstrated in two real-world case studies. Our findings provide substantial improvements over state-of-the-art baselines with regard to both predictive and, in particular, explanatory performance. Altogether, this yields a novel spatial model for leveraging web-mined POIs. Within the context of location-based social networks, it promises an extensive range of new insights and use cases.",
    "IsOld": true
  },
  {
    "Paper-ID": "www/PawelczykBK20",
    "Title": "Learning Model-Agnostic Counterfactual Explanations for Tabular Data.",
    "url": "https://doi.org/10.1145/3366423.3380087",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "WWW"
    },
    "Authors": [
      "Martin Pawelczyk",
      "Klaus Broelemann",
      "Gjergji Kasneci"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Any (for a specific task); model-agnostic",
      "Logistic Regression"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Representation Synthesis"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Counterfactual explanations can be obtained by identifying the smallest change made to an input vector to influence a prediction in a positive way from a user\u2019s viewpoint; for example, from \u2019loan rejected\u2019 to \u2019awarded\u2019 or from \u2019high risk of cardiovascular disease\u2019 to \u2019low risk\u2019. Previous approaches would not ensure that the produced counterfactuals be proximate (i.e., not local outliers) and connected to regions with substantial data density (i.e., close to correctly classified observations), two requirements known as counterfactual faithfulness. Our contribution is twofold. First, drawing ideas from the manifold learning literature, we develop a framework, called C-CHVAE, that generates faithful counterfactuals. Second, we suggest to complement the catalog of counterfactual quality measures using a criterion to quantify the degree of difficulty for a certain counterfactual suggestion. Our real world experiments suggest that faithful counterfactuals come at the cost of higher degrees of difficulty.",
    "IsOld": true
  },
  {
    "Paper-ID": "www/SunWZFHW20",
    "Title": "Dual Learning for Explainable Recommendation - Towards Unifying User Preference Prediction and Review Generation.",
    "url": "https://doi.org/10.1145/3366423.3380164",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "WWW"
    },
    "Authors": [
      "Peijie Sun",
      "Le Wu",
      "Kun Zhang",
      "Yanjie Fu",
      "Richang Hong",
      "Meng Wang"
    ],
    "Type of Data": [
      "Text",
      "User-item matrix"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Recommendation"
    ],
    "Type of Explanation": [
      "Text"
    ],
    "Method used to explain": [
      "Supervised explanation training"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "In many recommender systems, users express item opinions through two kinds of behaviors: giving preferences and writing detailed reviews. As both kinds of behaviors reflect users\u2019 assessment of items, review enhanced recommender systems leverage these two kinds of user behaviors to boost recommendation performance. On the one hand, researchers proposed to better model the user and item embeddings with additional review information for enhancing preference prediction accuracy. On the other hand, some recent works focused on automatically generating item reviews for recommendation explanations with related user and item embeddings. We argue that, while the task of preference prediction with the accuracy goal is well recognized in the community, the task of generating reviews for explainable recommendation is also important to gain user trust and increase conversion rate. Some preliminary attempts have considered jointly modeling these two tasks, with the user and item embeddings are shared. These studies empirically showed that these two tasks are correlated, and jointly modeling them would benefit the performance of both tasks. In this paper, we make a further study of unifying these two tasks for explainable recommendation. Instead of simply correlating these two tasks with shared user and item embeddings, we argue that these two tasks are presented in dual forms. In other words, the input of the primal preference prediction task is exactly the output of the dual review generation task , with and denote the preference value space and review space. Therefore, we could explicitly model the probabilistic correlation between these two dual tasks with . We design a unified dual framework of how to inject the probabilistic duality of the two tasks in the training stage. Furthermore, as the detailed preference and review information are not available for each user-item pair in the test stage, we propose a transfer learning based model for preference prediction and review generation. Finally, extensive experimental results on two real-world datasets clearly show the effectiveness of our proposed model for both user preference prediction and review generation.",
    "IsOld": true
  },
  {
    "Paper-ID": "www/WangZZSP20",
    "Title": "DyCRS - Dynamic Interpretable Postoperative Complication Risk Scoring.",
    "url": "https://doi.org/10.1145/3366423.3380253",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "WWW"
    },
    "Authors": [
      "Wen Wang",
      "Han Zhao",
      "Honglei Zhuang",
      "Nirav Shah",
      "Rema Padman"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Time series"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "White-box model"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Early identification of patients at risk for postoperative complications can facilitate timely workups and treatments and improve health outcomes. Currently, a widely-used surgical risk calculator online web system developed by the American College of Surgeons (ACS) uses patients\u2019 static features, e.g. gender, age, to assess the risk of postoperative complications. However, the most crucial signals that reflect the actual postoperative physical conditions of patients are usually real-time dynamic signals, including the vital signs of patients (e.g., heart rate, blood pressure) collected from postoperative monitoring. In this paper, we develop a dynamic postoperative complication risk scoring framework (DyCRS) to detect the \u201cat-risk\u201d patients in a real-time way based on postoperative sequential vital signs and static features. DyCRS is based on adaptations of the Hidden Markov Model (HMM) that captures hidden states as well as observable states to generate a real-time, probabilistic, complication risk score. Evaluating our model using electronic health record (EHR) on elective Colectomy surgery from a major health system, we show that DyCRS significantly outperforms the state-of-the-art ACS calculator and real-time predictors with 50.16% area under precision-recall curve (AUCPRC) gain on average in terms of detection effectiveness. In terms of earliness, our DyCRS can predict 15hrs55mins earlier on average than clinician\u2019s diagnosis with the recall of 60% and precision of 55%. Furthermore, Our DyCRS can extract interpretable patients\u2019 stages, which are consistent with previous medical postoperative complication studies. We believe that our contributions demonstrate significant promise for developing a more accurate, robust and interpretable postoperative complication risk scoring system, which can benefit more than 50 million annual surgeries in the US by substantially lowering adverse events and healthcare costs.",
    "IsOld": true
  },
  {
    "Paper-ID": "www/ZhuXSZCGH20",
    "Title": "Modeling Users' Behavior Sequences with Hierarchical Explainable Network for Cross-domain Fraud Detection.",
    "url": "https://doi.org/10.1145/3366423.3380172",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "WWW"
    },
    "Authors": [
      "Yongchun Zhu",
      "Dongbo Xi",
      "Bowen Song",
      "Fuzhen Zhuang",
      "Shuai Chen",
      "Xi Gu",
      "Qing He"
    ],
    "Type of Data": [
      "Other"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "With the explosive growth of the e-commerce industry, detecting online transaction fraud in real-world applications has become increasingly important to the development of e-commerce platforms. The sequential behavior history of users provides useful information in differentiating fraudulent payments from regular ones. Recently, some approaches have been proposed to solve this sequence-based fraud detection problem. However, these methods usually suffer from two problems: the prediction results are difficult to explain and the exploitation of the internal information of behaviors is insufficient. To tackle the above two problems, we propose a Hierarchical Explainable Network (HEN) to model users\u2019 behavior sequences, which could not only improve the performance of fraud detection but also make the inference process interpretable. Meanwhile, as e-commerce business expands to new domains, e.g., new countries or new markets, one major problem for modeling user behavior in fraud detection systems is the limitation of data collection, e.g., very few data/labels available. Thus, in this paper, we further propose a transfer framework to tackle the cross-domain fraud detection problem, which aims to transfer knowledge from existing domains (source domains) with enough and mature data to improve the performance in the new domain (target domain). Our proposed method is a general transfer framework that could not only be applied upon HEN but also various existing models in the Embedding & MLP paradigm. By utilizing data from a world-leading cross-border e-commerce platform, we conduct extensive experiments in detecting card-stolen transaction frauds in different countries to demonstrate the superior performance of HEN. Besides, based on 90 transfer task experiments, we also demonstrate that our transfer framework could not only contribute to the cross-domain fraud detection task with HEN, but also be universal and expandable for various existing models. Moreover, HEN and the transfer framework form three-level attention which greatly increases the explainability of the detection results.",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/AnnasamyS19",
    "Title": "Towards Better Interpretability in Deep Q-Networks.",
    "url": "https://doi.org/10.1609/aaai.v33i01.33014561",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Raghuram Mandyam Annasamy",
      "Katia P. Sycara"
    ],
    "Type of Data": [
      "Images",
      "Other"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Policy learning"
    ],
    "Type of Explanation": [
      "Disentanglement",
      "Heatmap",
      "Representation Visualization"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Deep reinforcement learning techniques have demonstrated superior performance in a wide variety of environments. As improvements in training algorithms continue at a brisk pace, theoretical or empirical studies on understanding what these networks seem to learn, are far behind. In this paper we propose an interpretable neural network architecture for Q-learning which provides a global explanation of the model\u2019s behavior using key-value memories, attention and reconstructible embeddings. With a directed exploration strategy, our model can reach training rewards comparable to the state-of-the-art deep Q-learning models. However, results suggest that the features extracted by the neural network are extremely shallow and subsequent testing using out-of-sample examples shows that the agent can easily overfit to trajectories seen during training.",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/ChenZQ19",
    "Title": "Dynamic Explainable Recommendation Based on Neural Attentive Models.",
    "url": "https://doi.org/10.1609/aaai.v33i01.330153",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Xu Chen",
      "Yongfeng Zhang",
      "Zheng Qin"
    ],
    "Type of Data": [
      "Text",
      "User-item matrix"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Recommendation"
    ],
    "Type of Explanation": [
      "Localization"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Providing explanations in a recommender system is getting more and more attention in both industry and research communities. Most existing explainable recommender models regard user preferences as invariant to generate static explanations. However, in real scenarios, a user\u2019s preference is always dynamic, and she may be interested in different product features at different states. The mismatching between the explanation and user preference may degrade costumers\u2019 satisfaction, confidence and trust for the recommender system. \nWith the desire to fill up this gap, in this paper, we build a novel Dynamic Explainable Recommender (called DER) for more accurate user modeling and explanations. In specific, we design a time-aware gated recurrent unit (GRU) to model user dynamic preferences, and profile an item by its review information based on sentence-level convolutional neural network (CNN). By attentively learning the important review information according to the user current state, we are not only able to improve the recommendation performance, but also can provide explanations tailored for the users\u2019 current preferences. We conduct extensive experiments to demonstrate the superiority of our model for improving recommendation performance. And to evaluate the explainability of our model, we first present examples to provide intuitive analysis on the highlighted review information, and then crowd-sourcing based evaluations are conducted to quantitatively verify our model\u2019s superiority.",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/GaoWW019",
    "Title": "Explainable Recommendation through Attentive Multi-View Learning.",
    "url": "https://doi.org/10.1609/aaai.v33i01.33013622",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Jingyue Gao",
      "Xiting Wang",
      "Yasha Wang",
      "Xing Xie"
    ],
    "Type of Data": [
      "Text",
      "User-item matrix"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Recommendation"
    ],
    "Type of Explanation": [
      "Localization",
      "Text"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Recommender systems have been playing an increasingly important role in our daily life due to the explosive growth of information. Accuracy and explainability are two core aspects when we evaluate a recommendation model and have become one of the fundamental trade-offs in machine learning. In this paper, we propose to alleviate the trade-off between accuracy and explainability by developing an explainable deep model that combines the advantages of deep learning-based models and existing explainable methods. The basic idea is to build an initial network based on an explainable deep hierarchy (e.g., Microsoft Concept Graph) and improve the model accuracy by optimizing key variables in the hierarchy (e.g., node importance and relevance). To ensure accurate rating prediction, we propose an attentive multi-view learning framework. The framework enables us to handle sparse and noisy data by co-regularizing among different feature levels and combining predictions attentively. To mine readable explanations from the hierarchy, we formulate personalized explanation generation as a constrained tree node selection problem and propose a dynamic programming algorithm to solve it. Experimental results show that our model outperforms state-of-the-art methods in terms of both accuracy and explainability.",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/HeLSB19",
    "Title": "Interpretable Predictive Modeling for Climate Variables with Weighted Lasso.",
    "url": "https://doi.org/10.1609/aaai.v33i01.33011385",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Sijie He",
      "Xinyan Li",
      "Vidyashankar Sivakumar",
      "Arindam Banerjee"
    ],
    "Type of Data": [
      "Time series"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Regression"
    ],
    "Type of Explanation": [
      "White-box model"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "An important family of problems in climate science focus on finding predictive relationships between various climate variables. In this paper, we consider the problem of predicting monthly deseasonalized land temperature at different locations worldwide based on sea surface temperature (SST). Contrary to popular belief on the trade-off between (a) simple interpretable but inaccurate models and (b) complex accurate but uninterpretable models, we introduce a weighted Lasso model for the problem which yields interpretable results while being highly accurate. Covariate weights in the regularization of weighted Lasso are pre-determined, and proportional to the spatial distance of the covariate (sea surface location) from the target (land location). We establish finite sample estimation error bounds for weighted Lasso, and illustrate its superior empirical performance and interpretability over complex models such as deep neural networks (Deep nets) and gradient boosted trees (GBT). We also present a detailed empirical analysis of what went wrong with Deep nets here, which may serve as a helpful guideline for application of Deep nets to small sample scientific problems.",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/IgnatievNM19",
    "Title": "Abduction-Based Explanations for Machine Learning Models.",
    "url": "https://doi.org/10.1609/aaai.v33i01.33011511",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Alexey Ignatiev",
      "Nina Narodytska",
      "Jo\u00e3o Marques-Silva"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Localization"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "The growing range of applications of Machine Learning (ML) in a multitude of settings motivates the ability of computing small explanations for predictions made. Small explanations are generally accepted as easier for human decision makers to understand. Most earlier work on computing explanations is based on heuristic approaches, providing no guarantees of quality, in terms of how close such solutions are from cardinality- or subset-minimal explanations. This paper develops a constraint-agnostic solution for computing explanations for any ML model. The proposed solution exploits abductive reasoning, and imposes the requirement that the ML model can be represented as sets of constraints using some target constraint reasoning system for which the decision problem can be answered with some oracle. The experimental results, obtained on well-known datasets, validate the scalability of the proposed approach as well as the quality of the computed solutions.",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/LanG19",
    "Title": "Accurate and Interpretable Factorization Machines.",
    "url": "https://doi.org/10.1609/aaai.v33i01.33014139",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Liang Lan",
      "Yu Geng"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Text"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Heatmap"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Factorization Machines (FMs), a general predictor that can efficiently model high-order feature interactions, have been widely used for regression, classification and ranking problems. However, despite many successful applications of FMs, there are two main limitations of FMs: (1) FMs consider feature interactions among input features by using only polynomial expansion which fail to capture complex nonlinear patterns in data. (2) Existing FMs do not provide interpretable prediction to users. In this paper, we present a novel method named Subspace Encoding Factorization Machines (SEFM) to overcome these two limitations by using non-parametric subspace feature mapping. Due to the high sparsity of new feature representation, our proposed method achieves the same time complexity as the standard FMs but can capture more complex nonlinear patterns. Moreover, since the prediction score of our proposed model for a sample is a sum of contribution scores of the bins and grid cells that this sample lies in low-dimensional subspaces, it works similar like a scoring system which only involves data binning and score addition. Therefore, our proposed method naturally provides interpretable prediction. Our experimental results demonstrate that our proposed method efficiently provides accurate and interpretable prediction.",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/LyuYLG19",
    "Title": "SDRL - Interpretable and Data-Efficient Deep Reinforcement Learning Leveraging Symbolic Planning.",
    "url": "https://doi.org/10.1609/aaai.v33i01.33012970",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Daoming Lyu",
      "Fangkai Yang",
      "Bo Liu",
      "Steven Gustafson"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Policy learning"
    ],
    "Type of Explanation": [
      "Other"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Deep reinforcement learning (DRL) has gained great success by learning directly from high-dimensional sensory inputs, yet is notorious for the lack of interpretability. Interpretability of the subtasks is critical in hierarchical decision-making as it increases the transparency of black-box-style DRL approach and helps the RL practitioners to understand the high-level behavior of the system better. In this paper, we introduce symbolic planning into DRL and propose a framework of Symbolic Deep Reinforcement Learning (SDRL) that can handle both high-dimensional sensory inputs and symbolic planning. The task-level interpretability is enabled by relating symbolic actions to options.This framework features a planner \u2013 controller \u2013 meta-controller architecture, which takes charge of subtask scheduling, data-driven subtask learning, and subtask evaluation, respectively. The three components cross-fertilize each other and eventually converge to an optimal symbolic plan along with the learned subtasks, bringing together the advantages of long-term planning capability with symbolic knowledge and end-to-end reinforcement learning directly from a high-dimensional sensory input. Experimental results validate the interpretability of subtasks, along with improved data efficiency compared with state-of-the-art approaches.",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/PolatoA19",
    "Title": "Interpretable Preference Learning - A Game Theoretic Framework for Large Margin On-Line Feature and Rule Learning.",
    "url": "https://doi.org/10.1609/aaai.v33i01.33014723",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Mirko Polato",
      "Fabio Aiolli"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Images"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Decision Rules",
      "Representation Visualization"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "A large body of research is currently investigating on the connection between machine learning and game theory. In this work, game theory notions are injected into a preference learning framework. Specifically, a preference learning problem is seen as a two-players zero-sum game. An algorithm is proposed to incrementally include new useful features into the hypothesis. This can be particularly important when dealing with a very large number of potential features like, for instance, in relational learning and rule extraction. A game theoretical analysis is used to demonstrate the convergence of the algorithm. Furthermore, leveraging on the natural analogy between features and rules, the resulting models can be easily interpreted by humans. An extensive set of experiments on classification tasks shows the effectiveness of the proposed method in terms of interpretability and feature selection quality, with accuracy at the state-of-the-art.",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/ShakerinG19",
    "Title": "Induction of Non-Monotonic Logic Programs to Explain Boosted Tree Models Using LIME.",
    "url": "https://doi.org/10.1609/aaai.v33i01.33013052",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Farhad Shakerin",
      "Gopal Gupta"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Any"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "Tree Ensemble"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Decision Rules"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "We present a heuristic based algorithm to induce nonmonotonic logic programs that will explain the behavior of XGBoost trained classifiers. We use the technique based on the LIME approach to locally select the most important features contributing to the classification decision. Then, in order to explain the model\u2019s global behavior, we propose the LIME-FOLD algorithm \u2014a heuristic-based inductive logic programming (ILP) algorithm capable of learning nonmonotonic logic programs\u2014that we apply to a transformed dataset produced by LIME. Our proposed approach is agnostic to the choice of the ILP algorithm. Our experiments with UCI standard benchmarks suggest a significant improvement in terms of classification evaluation metrics. Meanwhile, the number of induced rules dramatically decreases compared to ALEPH, a state-of-the-art ILP system.",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/SilvaFH19",
    "Title": "Exploring Knowledge Graphs in an Interpretable Composite Approach for Text Entailment.",
    "url": "https://doi.org/10.1609/aaai.v33i01.33017023",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Vivian Dos Santos Silva",
      "Andr\u00e9 Freitas",
      "Siegfried Handschuh"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Graph",
      "Text"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Recognizing textual entailment is a key task for many semantic applications, such as Question Answering, Text Summarization, and Information Extraction, among others. Entailment scenarios can range from a simple syntactic variation to more complex semantic relationships between pieces of text, but most approaches try a one-size-fits-all solution that usually favors some scenario to the detriment of another. We propose a composite approach for recognizing text entailment which analyzes the entailment pair to decide whether it must be resolved syntactically or semantically. We also make the answer interpretable: whenever an entailment is solved semantically, we explore a knowledge base composed of structured lexical definitions to generate natural language humanlike justifications, explaining the semantic relationship holding between the pieces of text. Besides outperforming wellestablished entailment algorithms, our composite approach gives an important step towards Explainable AI, using world knowledge to make the semantic reasoning process explicit and understandable.",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/TopinV19",
    "Title": "Generation of Policy-Level Explanations for Reinforcement Learning.",
    "url": "https://doi.org/10.1609/aaai.v33i01.33012514",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Nicholay Topin",
      "Manuela Veloso"
    ],
    "Type of Data": [
      "Other"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Policy learning"
    ],
    "Type of Explanation": [
      "Graph",
      "Feature Importance"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Though reinforcement learning has greatly benefited from the incorporation of neural networks, the inability to verify the correctness of such systems limits their use. Current work in explainable deep learning focuses on explaining only a single decision in terms of input features, making it unsuitable for explaining a sequence of decisions. To address this need, we introduce Abstracted Policy Graphs, which are Markov chains of abstract states. This representation concisely summarizes a policy so that individual decisions can be explained in the context of expected future transitions. Additionally, we propose a method to generate these Abstracted Policy Graphs for deterministic policies given a learned value function and a set of observed transitions, potentially off-policy transitions used during training. Since no restrictions are placed on how the value function is generated, our method is compatible with many existing reinforcement learning methods. We prove that the worst-case time complexity of our method is quadratic in the number of features and linear in the number of provided transitions, O(|F|2|tr samples|). By applying our method to a family of domains, we show that our method scales well in practice and produces Abstracted Policy Graphs which reliably capture relationships within these domains.",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/WangWX00C19",
    "Title": "Explainable Reasoning over Knowledge Graphs for Recommendation.",
    "url": "https://doi.org/10.1609/aaai.v33i01.33015329",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Xiang Wang",
      "Dingxian Wang",
      "Canran Xu",
      "Xiangnan He",
      "Yixin Cao",
      "Tat-Seng Chua"
    ],
    "Type of Data": [
      "Graph data",
      "User-item matrix"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Recommendation"
    ],
    "Type of Explanation": [
      "Graph",
      "Text"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Incorporating knowledge graph into recommender systems has attracted increasing attention in recent years. By exploring the interlinks within a knowledge graph, the connectivity between users and items can be discovered as paths, which provide rich and complementary information to user-item interactions. Such connectivity not only reveals the semantics of entities and relations, but also helps to comprehend a user\u2019s interest. However, existing efforts have not fully explored this connectivity to infer user preferences, especially in terms of modeling the sequential dependencies within and holistic semantics of a path.In this paper, we contribute a new model named Knowledgeaware Path Recurrent Network (KPRN) to exploit knowledge graph for recommendation. KPRN can generate path representations by composing the semantics of both entities and relations. By leveraging the sequential dependencies within a path, we allow effective reasoning on paths to infer the underlying rationale of a user-item interaction. Furthermore, we design a new weighted pooling operation to discriminate the strengths of different paths in connecting a user with an item, endowing our model with a certain level of explainability. We conduct extensive experiments on two datasets about movie and music, demonstrating significant improvements over state-of-the-art solutions Collaborative Knowledge Base Embedding and Neural Factorization Machine.",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/WickramanayakeH19",
    "Title": "FLEX - Faithful Linguistic Explanations for Neural Net Based Model Decisions.",
    "url": "https://doi.org/10.1609/aaai.v33i01.33012539",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Sandareka Wickramanayake",
      "Wynne Hsu",
      "Mong-Li Lee"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Heatmap",
      "Text"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Explaining the decisions of a Deep Learning Network is imperative to safeguard end-user trust. Such explanations must be intuitive, descriptive, and faithfully explain why a model makes its decisions. In this work, we propose a framework called FLEX (Faithful Linguistic EXplanations) that generates post-hoc linguistic justifications to rationalize the decision of a Convolutional Neural Network. FLEX explains a model\u2019s decision in terms of features that are responsible for the decision. We derive a novel way to associate such features to words, and introduce a new decision-relevance metric that measures the faithfulness of an explanation to a model\u2019s reasoning. Experiment results on two benchmark datasets demonstrate that the proposed framework can generate discriminative and faithful explanations compared to state-of-the-art explanation generators. We also show how FLEX can generate explanations for images of unseen classes as well as automatically annotate objects in images.",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/YuanCHJ19",
    "Title": "Interpreting Deep Models for Text Analysis via Optimization and Regularization Methods.",
    "url": "https://doi.org/10.1609/aaai.v33i01.33015717",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Hao Yuan",
      "Yongjun Chen",
      "Xia Hu",
      "Shuiwang Ji"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Prototypes",
      "Representation Visualization"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Interpreting deep neural networks is of great importance to understand and verify deep models for natural language processing (NLP) tasks. However, most existing approaches only focus on improving the performance of models but ignore their interpretability. In this work, we propose an approach to investigate the meaning of hidden neurons of the convolutional neural network (CNN) models. We first employ saliency map and optimization techniques to approximate the detected information of hidden neurons from input sentences. Then we develop regularization terms and explore words in vocabulary to interpret such detected information. Experimental results demonstrate that our approach can identify meaningful and reasonable interpretations for hidden spatial locations. Additionally, we show that our approach can describe the decision procedure of deep NLP models.",
    "IsOld": true
  },
  {
    "Paper-ID": "acl/BastingsAT19",
    "Title": "Interpretable Neural Predictions with Differentiable Binary Variables.",
    "url": "https://doi.org/10.18653/v1/p19-1284",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ACL"
    },
    "Authors": [
      "Jasmijn Bastings",
      "Wilker Aziz",
      "Ivan Titov"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation",
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification",
      "Regression"
    ],
    "Type of Explanation": [
      "Localization",
      "Text",
      "White-box model"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "The success of neural networks comes hand in hand with a desire for more interpretability. We focus on text classifiers and make them more interpretable by having them provide a justification\u2013a rationale\u2013for their predictions. We approach this problem by jointly training two neural network models: a latent model that selects a rationale (i.e. a short and informative part of the input text), and a classifier that learns from the words in the rationale alone. Previous work proposed to assign binary latent masks to input positions and to promote short selections via sparsity-inducing penalties such as L0 regularisation. We propose a latent model that mixes discrete and continuous behaviour allowing at the same time for binary selections and gradient-based training without REINFORCE. In our formulation, we can tractably compute the expected value of penalties such as L0, which allows us to directly optimise the model towards a pre-specified text selection rate. We show that our approach is competitive with previous work on rationale extraction, and explore further uses in attention mechanisms.",
    "IsOld": true
  },
  {
    "Paper-ID": "acl/JiangJCB19",
    "Title": "Explore, Propose, and Assemble - An Interpretable Model for Multi-Hop Reading Comprehension.",
    "url": "https://doi.org/10.18653/v1/p19-1261",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ACL"
    },
    "Authors": [
      "Yichen Jiang",
      "Nitish Joshi",
      "Yen-Chun Chen",
      "Mohit Bansal"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Question Answering"
    ],
    "Type of Explanation": [
      "Localization"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Multi-hop reading comprehension requires the model to explore and connect relevant information from multiple sentences/documents in order to answer the question about the context. To achieve this, we propose an interpretable 3-module system called Explore-Propose-Assemble reader (EPAr). First, the Document Explorer iteratively selects relevant documents and represents divergent reasoning chains in a tree structure so as to allow assimilating information from all chains. The Answer Proposer then proposes an answer from every root-to-leaf path in the reasoning tree. Finally, the Evidence Assembler extracts a key sentence containing the proposed answer from every path and combines them to predict the final answer. Intuitively, EPAr approximates the coarse-to-fine-grained comprehension behavior of human readers when facing multiple long documents. We jointly optimize our 3 modules by minimizing the sum of losses from each stage conditioned on the previous stage\u2019s output. On two multi-hop reading comprehension datasets WikiHop and MedHop, our EPAr model achieves significant improvements over the baseline and competitive results compared to the state-of-the-art model. We also present multiple reasoning-chain-recovery tests and ablation studies to demonstrate our system\u2019s ability to perform interpretable and accurate reasoning.",
    "IsOld": true
  },
  {
    "Paper-ID": "acl/LuZXLZX19",
    "Title": "Constructing Interpretive Spatio-Temporal Features for Multi-Turn Responses Selection.",
    "url": "https://doi.org/10.18653/v1/p19-1006",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ACL"
    },
    "Authors": [
      "Junyu Lu",
      "Chenbin Zhang",
      "Zeying Xie",
      "Guang Ling",
      "Tom Chao Zhou",
      "Zenglin Xu"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Heatmap"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Response selection plays an important role in fully automated dialogue systems. Given the dialogue context, the goal of response selection is to identify the best-matched next utterance (i.e., response) from multiple candidates. Despite the efforts of many previous useful models, this task remains challenging due to the huge semantic gap and also the large size of candidate set. To address these issues, we propose a Spatio-Temporal Matching network (STM) for response selection. In detail, soft alignment is first used to obtain the local relevance between the context and the response. And then, we construct spatio-temporal features by aggregating attention images in time dimension and make use of 3D convolution and pooling operations to extract matching information. Evaluation on two large-scale multi-turn response selection tasks has demonstrated that our proposed model significantly outperforms the state-of-the-art model. Particularly, visualization analysis shows that the spatio-temporal features enables matching information in segment pairs and time sequences, and have good interpretability for multi-turn text matching.",
    "IsOld": true
  },
  {
    "Paper-ID": "acl/MoonSKS19",
    "Title": "OpenDialKG - Explainable Conversational Reasoning with Attention-based Walks over Knowledge Graphs.",
    "url": "https://doi.org/10.18653/v1/p19-1081",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ACL"
    },
    "Authors": [
      "Seungwhan Moon",
      "Pararth Shah",
      "Anuj Kumar",
      "Rajen Subba"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Generation"
    ],
    "Type of Explanation": [
      "Graph"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "We study a conversational reasoning model that strategically traverses through a large-scale common fact knowledge graph (KG) to introduce engaging and contextually diverse entities and attributes. For this study, we collect a new Open-ended Dialog <-> KG parallel corpus called OpenDialKG, where each utterance from 15K human-to-human role-playing dialogs is manually annotated with ground-truth reference to corresponding entities and paths from a large-scale KG with 1M+ facts. We then propose the DialKG Walker model that learns the symbolic transitions of dialog contexts as structured traversals over KG, and predicts natural entities to introduce given previous dialog contexts via a novel domain-agnostic, attention-based graph path decoder. Automatic and human evaluations show that our model can retrieve more natural and human-like responses than the state-of-the-art baselines or rule-based models, in both in-domain and cross-domain tasks. The proposed model also generates a KG walk path for each entity retrieved, providing a natural way to explain conversational reasoning.",
    "IsOld": true
  },
  {
    "Paper-ID": "acl/PanigrahiSB19",
    "Title": "Word2Sense - Sparse Interpretable Word Embeddings.",
    "url": "https://doi.org/10.18653/v1/p19-1570",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ACL"
    },
    "Authors": [
      "Abhishek Panigrahi",
      "Harsha Vardhan Simhadri",
      "Chiranjib Bhattacharyya"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "Tree Ensemble",
      "Support Vector Machine",
      "Any (for a specific task); model-agnostic",
      "Logistic Regression"
    ],
    "Type of Task": [
      "Classification",
      "Representation learning",
      "Other"
    ],
    "Type of Explanation": [
      "Disentanglement"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "We present an unsupervised method to generate Word2Sense word embeddings that are interpretable \u2014 each dimension of the embedding space corresponds to a fine-grained sense, and the non-negative value of the embedding along the j-th dimension represents the relevance of the j-th sense to the word. The underlying LDA-based generative model can be extended to refine the representation of a polysemous word in a short context, allowing us to use the embedings in contextual tasks. On computational NLP tasks, Word2Sense embeddings compare well with other word embeddings generated by unsupervised methods. Across tasks such as word similarity, entailment, sense induction, and contextual interpretation, Word2Sense is competitive with the state-of-the-art method for that task. Word2Sense embeddings are at least as sparse and fast to compute as prior art.",
    "IsOld": true
  },
  {
    "Paper-ID": "acl/RajaniMXS19",
    "Title": "Explain Yourself! Leveraging Language Models for Commonsense Reasoning.",
    "url": "https://doi.org/10.18653/v1/p19-1487",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ACL"
    },
    "Authors": [
      "Nazneen Fatema Rajani",
      "Bryan McCann",
      "Caiming Xiong",
      "Richard Socher"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Question Answering"
    ],
    "Type of Explanation": [
      "Text"
    ],
    "Method used to explain": [
      "Post-hoc explanation method",
      "Supervised explanation training"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Deep learning models perform poorly on tasks that require commonsense reasoning, which often necessitates some form of world-knowledge or reasoning over information not immediately present in the input. We collect human explanations for commonsense reasoning in the form of natural language sequences and highlighted annotations in a new dataset called Common Sense Explanations (CoS-E). We use CoS-E to train language models to automatically generate explanations that can be used during training and inference in a novel Commonsense Auto-Generated Explanation (CAGE) framework. CAGE improves the state-of-the-art by 10% on the challenging CommonsenseQA task. We further study commonsense reasoning in DNNs using both human and auto-generated explanations including transfer to out-of-domain tasks. Empirical results indicate that we can effectively leverage language models for commonsense reasoning.",
    "IsOld": true
  },
  {
    "Paper-ID": "acl/SydorovaPR19",
    "Title": "Interpretable Question Answering on Knowledge Bases and Text.",
    "url": "https://doi.org/10.18653/v1/p19-1488",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ACL"
    },
    "Authors": [
      "Alona Sydorova",
      "Nina P\u00f6rner",
      "Benjamin Roth"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Question Answering"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Post-hoc explanation method",
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Interpretability of machine learning (ML) models becomes more relevant with their increasing adoption. In this work, we address the interpretability of ML based question answering (QA) models on a combination of knowledge bases (KB) and text documents. We adapt post hoc explanation methods such as LIME and input perturbation (IP) and compare them with the self-explanatory attention mechanism of the model. For this purpose, we propose an automatic evaluation paradigm for explanation methods in the context of QA. We also conduct a study with human annotators to evaluate whether explanations help them identify better QA models. Our results suggest that IP provides better explanations than LIME or attention, according to both automatic and human evaluation. We obtain the same ranking of methods in both experiments, which supports the validity of our automatic evaluation paradigm.",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/FukuiHYF19",
    "Title": "Attention Branch Network - Learning of Attention Mechanism for Visual Explanation.",
    "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Fukui_Attention_Branch_Network_Learning_of_Attention_Mechanism_for_Visual_Explanation_CVPR_2019_paper.html",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "Hiroshi Fukui",
      "Tsubasa Hirakawa",
      "Takayoshi Yamashita",
      "Hironobu Fujiyoshi"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Heatmap"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Visual explanation enables humans to understand the decision making of deep convolutional neural network (CNN), but it is insufficient to contribute to improving CNN performance. In this paper, we focus on the attention map for visual explanation, which represents a high response value as the attention location in image recognition. This attention region significantly improves the performance of CNN by introducing an attention mechanism that focuses on a specific region in an image. In this work, we propose Attention Branch Network (ABN), which extends a response-based visual explanation model by introducing a branch structure with an attention mechanism. ABN can be applicable to several image recognition tasks by introducing a branch for the attention mechanism and is trainable for visual explanation and image recognition in an end-to-end manner. We evaluate ABN on several image recognition tasks such as image classification, fine-grained recognition, and multiple facial attribute recognition. Experimental results indicate that ABN outperforms the baseline models on these image recognition tasks while generating an attention map for visual explanation. Our code is available.",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/KanehiraH19",
    "Title": "Learning to Explain With Complemental Examples.",
    "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kanehira_Learning_to_Explain_With_Complemental_Examples_CVPR_2019_paper.html",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "Atsushi Kanehira",
      "Tatsuya Harada"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Prototypes",
      "Text"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "This paper addresses the generation of explanations with visual examples. Given an input sample, we build a system that not only classifies it to a specific category, but also outputs linguistic explanations and a set of visual examples that render the decision interpretable. Focusing especially on the complementarity of the multimodal information, i.e., linguistic and visual examples, we attempt to achieve it by maximizing the interaction information, which provides a natural definition of complementarity from an information theoretical viewpoint. We propose a novel framework to generate complemental explanations, on which the joint distribution of the variables to explain, and those to be explained is parameterized by three different neural networks: predictor, linguistic explainer, and example selector. Explanation models are trained collaboratively to maximize the interaction information to ensure the generated explanation are complemental to each other for the target. The results of experiments conducted on several datasets demonstrate the effectiveness of the proposed method.",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/KanehiraTIH19",
    "Title": "Multimodal Explanations by Predicting Counterfactuality in Videos.",
    "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Kanehira_Multimodal_Explanations_by_Predicting_Counterfactuality_in_Videos_CVPR_2019_paper.html",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "Atsushi Kanehira",
      "Kentaro Takemoto",
      "Sho Inayoshi",
      "Tatsuya Harada"
    ],
    "Type of Data": [
      "Video"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Localization",
      "Text"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "This study addresses generating counterfactual explanations with multimodal information. Our goal is not only to classify a video into a specific category, but also to provide explanations on why it is not categorized to a specific class with combinations of visual-linguistic information. Requirements that the expected output should satisfy are referred to as counterfactuality in this paper: (1) Compatibility of visual-linguistic explanations, and (2) Positiveness/negativeness for the specific positive/negative class. Exploiting a spatio-temporal region (tube) and an attribute as visual and linguistic explanations respectively, the explanation model is trained to predict the counterfactuality for possible combinations of multimodal information in a post-hoc manner. The optimization problem, which appears during training/inference, can be efficiently solved by inserting a novel neural network layer, namely the maximum subpath layer. We demonstrated the effectiveness of this method by comparison with a baseline of the action recognition datasets extended for this task. Moreover, we provide information-theoretical insight into the proposed method.",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/PopeKRMH19",
    "Title": "Explainability Methods for Graph Convolutional Neural Networks.",
    "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Pope_Explainability_Methods_for_Graph_Convolutional_Neural_Networks_CVPR_2019_paper.html",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "Phillip E. Pope",
      "Soheil Kolouri",
      "Mohammad Rostami",
      "Charles E. Martin",
      "Heiko Hoffmann"
    ],
    "Type of Data": [
      "Graph data"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Heatmap"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "With the growing use of graph convolutional neural networks (GCNNs) comes the need for explainability. In this paper, we introduce explainability methods for GCNNs. We develop the graph analogues of three prominent explainability methods for convolutional neural networks: contrastive gradient-based (CG) saliency maps, Class Activation Mapping (CAM), and Excitation Back-Propagation (EB) and their variants, gradient-weighted CAM (Grad-CAM) and contrastive EB (c-EB). We show a proof-of-concept of these methods on classification problems in two application domains: visual scene graphs and molecular graphs. To compare the methods, we identify three desirable properties of explanations: (1) their importance to classification, as measured by the impact of occlusions, (2) their contrastivity with respect to different classes, and (3) their sparseness on a graph. We call the corresponding quantitative metrics fidelity, contrastivity, and sparsity and evaluate them for each method. Lastly, we analyze the salient subgraphs obtained from explanations and report frequently occurring patterns.",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/ShiZL19",
    "Title": "Explainable and Explicit Visual Reasoning Over Scene Graphs.",
    "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Shi_Explainable_and_Explicit_Visual_Reasoning_Over_Scene_Graphs_CVPR_2019_paper.html",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "Jiaxin Shi",
      "Hanwang Zhang",
      "Juanzi Li"
    ],
    "Type of Data": [
      "Images",
      "Graph data"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Question Answering"
    ],
    "Type of Explanation": [
      "Localization"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "We aim to dismantle the prevalent black-box neural architectures used in complex visual reasoning tasks, into the proposed eXplainable and eXplicit Neural Modules (XNMs), which advance beyond existing neural module networks towards using scene graphs --- objects as nodes and the pairwise relationships as edges --- for explainable and explicit reasoning with structured knowledge. XNMs allow us to pay more attention to teach machines how to ``think'', regardless of what they ``look''. As we will show in the paper, by using scene graphs as an inductive bias, 1) we can design XNMs in a concise and flexible fashion, i.e., XNMs merely consist of 4 meta-types, which significantly reduce the number of parameters by 10 to 100 times, and 2) we can explicitly trace the reasoning-flow in terms of graph attentions. XNMs are so generic that they support a wide range of scene graph implementations with various qualities. For example, when the graphs are detected perfectly, XNMs achieve 100% accuracy on both CLEVR and CLEVR CoGenT, establishing an empirical performance upper-bound for visual reasoning; when the graphs are noisily detected from real-world images, XNMs are still robust to achieve a competitive 67.5% accuracy on VQAv2.0, surpassing the popular bag-of-objects attention models without graph structures.",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/ZengLSSYCU19",
    "Title": "End-To-End Interpretable Neural Motion Planner.",
    "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zeng_End-To-End_Interpretable_Neural_Motion_Planner_CVPR_2019_paper.html",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "Wenyuan Zeng",
      "Wenjie Luo",
      "Simon Suo",
      "Abbas Sadat",
      "Bin Yang",
      "Sergio Casas",
      "Raquel Urtasun"
    ],
    "Type of Data": [
      "Time series",
      "Other"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Policy learning"
    ],
    "Type of Explanation": [
      "Localization"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "In this paper, we propose a neural motion planner for learning to drive autonomously in complex urban scenarios that include traffic-light handling, yielding, and interactions with multiple road-users. Towards this goal, we design a holistic model that takes as input raw LIDAR data and a HD map and produces interpretable intermediate representations in the form of 3D detections and their future trajectories, as well as a cost volume defining the goodness of each position that the self-driving car can take within the planning horizon. We then sample a set of diverse physically possible trajectories and choose the one with the minimum learned cost. Importantly, our cost volume is able to naturally capture multi-modality. We demonstrate the effectiveness of our approach in real-world driving data captured in several cities in North America. Our experiments show that the learned cost volume can generate safer planning than all the baselines.",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/ZhangYMW19",
    "Title": "Interpreting CNNs via Decision Trees.",
    "url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_Interpreting_CNNs_via_Decision_Trees_CVPR_2019_paper.html",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "Quanshi Zhang",
      "Yu Yang",
      "Haotian Ma",
      "Ying Nian Wu"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Decision Tree",
      "Heatmap",
      "Prototypes",
      "Feature Importance"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "This paper aims to quantitatively explain the rationales of each prediction that is made by a pre-trained convolutional neural network (CNN). We propose to learn a decision tree, which clarifies the specific reason for each prediction made by the CNN at the semantic level. I.e., the decision tree decomposes feature representations in high conv-layers of the CNN into elementary concepts of object parts. In this way, the decision tree tells people which object parts activate which filters for the prediction and how much each object part contributes to the prediction score. Such semantic and quantitative explanations for CNN predictions have specific values beyond the traditional pixel-level analysis of CNNs. More specifically, our method mines all potential decision modes of the CNN, where each mode represents a typical case of how the CNN uses object parts for prediction. The decision tree organizes all potential decision modes in a coarse-to-fine manner to explain CNN predictions at different fine-grained levels. Experiments have demonstrated the effectiveness of the proposed method.",
    "IsOld": true
  },
  {
    "Paper-ID": "iccv/ChenCHRZ19",
    "Title": "Explaining Neural Networks Semantically and Quantitatively.",
    "url": "https://doi.org/10.1109/ICCV.2019.00928",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ICCV"
    },
    "Authors": [
      "Runjin Chen",
      "Hao Chen",
      "Ge Huang",
      "Jie Ren",
      "Quanshi Zhang"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "This paper presents a method to pursue a semantic and quantitative explanation for the knowledge encoded in a convolutional neural network (CNN). The estimation of the specific rationale of each prediction made by the CNN presents a key issue of understanding neural networks, and it is of significant values in real applications. In this study, we propose to distill knowledge from the CNN into an explainable additive model, which explains the CNN prediction quantitatively. We discuss the problem of the biased interpretation of CNN predictions. To overcome the biased interpretation, we develop prior losses to guide the learning of the explainable additive model. Experimental results have demonstrated the effectiveness of our method.",
    "IsOld": true
  },
  {
    "Paper-ID": "iccv/ManhardtA0BBNT19",
    "Title": "Explaining the Ambiguity of Object Detection and 6D Pose From Visual Data.",
    "url": "https://doi.org/10.1109/ICCV.2019.00694",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ICCV"
    },
    "Authors": [
      "Fabian Manhardt",
      "Diego Mart\u00edn Arroyo",
      "Christian Rupprecht",
      "Benjamin Busam",
      "Tolga Birdal",
      "Nassir Navab",
      "Federico Tombari"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Other"
    ],
    "Type of Explanation": [
      "Other"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "3D object detection and pose estimation from a single image are two inherently ambiguous problems. Oftentimes, objects appear similar from different viewpoints due to shape symmetries, occlusion and repetitive textures. This ambiguity in both detection and pose estimation means that an object instance can be perfectly described by several different poses and even classes. In this work we propose to explicitly deal with these ambiguities. For each object instance we predict multiple 6D pose outcomes to estimate the specific pose distribution generated by symmetries and repetitive textures. The distribution collapses to a single outcome when the visual appearance uniquely identifies just one valid pose. We show the benefits of our approach which provides not only a better explanation for pose ambiguity, but also a higher accuracy in terms of pose estimation.",
    "IsOld": true
  },
  {
    "Paper-ID": "iccv/MicheliniLLJ19",
    "Title": "A Tour of Convolutional Networks Guided by Linear Interpreters.",
    "url": "https://doi.org/10.1109/ICCV.2019.00485",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ICCV"
    },
    "Authors": [
      "Pablo Navarrete Michelini",
      "Hanwen Liu",
      "Yunhua Lu",
      "Xingqun Jiang"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification",
      "Generation"
    ],
    "Type of Explanation": [
      "Heatmap",
      "Representation Synthesis"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Convolutional networks are large linear systems divided into layers and connected by non-linear units. These units are the \"articulations\" that allow the network to adapt to the input. To understand how a network manages to solve a problem we must look at the articulated decisions in entirety. If we could capture the actions of non-linear units for a particular input, we would be able to replay the whole system back and forth as if it was always linear. It would also reveal the actions of non-linearities because the resulting linear system, a Linear Interpreter, depends on the input image. We introduce a hooking layer, called a LinearScope, which allows us to run the network and the linear interpreter in parallel. Its implementation is simple, flexible and efficient. From here we can make many curious inquiries: how do these linear systems look like? When the rows and columns of the transformation matrix are images, how do they look like? What type of basis do these linear transformations rely on? The answers depend on the problems presented, through which we take a tour to some popular architectures used for classification, super-resolution (SR) and image-to-image translation (I2I). For classification we observe that popular networks use a pixel-wise vote per class strategy and heavily rely on bias parameters. For SR and I2I we find that CNNs use wavelet-type basis similar to the human visual system. For I2I we reveal copy-move and template-creation strategies to generate outputs.",
    "IsOld": true
  },
  {
    "Paper-ID": "iccv/PatroLPN19",
    "Title": "U-CAM - Visual Explanation Using Uncertainty Based Class Activation Maps.",
    "url": "https://doi.org/10.1109/ICCV.2019.00754",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ICCV"
    },
    "Authors": [
      "Badri N. Patro",
      "Mayank Lunayach",
      "Shivansh Patel",
      "Vinay Namboodiri"
    ],
    "Type of Data": [
      "Images",
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification",
      "Question Answering"
    ],
    "Type of Explanation": [
      "Heatmap"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Understanding and explaining deep learning models is an imperative task. Towards this, we propose a method that obtains gradient-based certainty estimates that also provide visual attention maps. Particularly, we solve for visual question answering task. We incorporate modern probabilistic deep learning methods that we further improve by using the gradients for these estimates. These have two-fold benefits: a) improvement in obtaining the certainty estimates that correlate better with misclassified samples and b) improved attention maps that provide state-of-the-art results in terms of correlation with human attention regions. The improved attention maps result in consistent improvement for various methods for visual question answering. Therefore, the proposed technique can be thought of as a recipe for obtaining improved certainty estimates and explanation for deep learning models. We provide detailed empirical analysis for the visual question answering task on all standard benchmarks and comparison with state of the art methods.",
    "IsOld": true
  },
  {
    "Paper-ID": "iccv/SelvarajuLSJGHB19",
    "Title": "Taking a HINT - Leveraging Explanations to Make Vision and Language Models More Grounded.",
    "url": "https://doi.org/10.1109/ICCV.2019.00268",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ICCV"
    },
    "Authors": [
      "Ramprasaath Ramasamy Selvaraju",
      "Stefan Lee",
      "Yilin Shen",
      "Hongxia Jin",
      "Shalini Ghosh",
      "Larry P. Heck",
      "Dhruv Batra",
      "Devi Parikh"
    ],
    "Type of Data": [
      "Images",
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Question Answering",
      "Generation"
    ],
    "Type of Explanation": [
      "Heatmap"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Many vision and language models suffer from poor visual grounding -- often falling back on easy-to-learn language priors rather than basing their decisions on visual concepts in the image. In this work, we propose a generic approach called Human Importance-aware Network Tuning (HINT) that effectively leverages human demonstrations to improve visual grounding. HINT encourages deep networks to be sensitive to the same input regions as humans. Our approach optimizes the alignment between human attention maps and gradient-based network importances -- ensuring that models learn not just to look at but rather rely on visual concepts that humans found relevant for a task when making predictions. We apply HINT to Visual Question Answering and Image Captioning tasks, outperforming top approaches on splits that penalize over-reliance on language priors (VQA-CP and robust captioning) using human attention demonstrations for just 6% of the training data.",
    "IsOld": true
  },
  {
    "Paper-ID": "iccv/SubramanyaPP19",
    "Title": "Fooling Network Interpretation in Image Classification.",
    "url": "https://doi.org/10.1109/ICCV.2019.00211",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ICCV"
    },
    "Authors": [
      "Akshayvarun Subramanya",
      "Vipin Pillai",
      "Hamed Pirsiavash"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Heatmap"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Deep neural networks have been shown to be fooled rather easily using adversarial attack algorithms. Practical methods such as adversarial patches have been shown to be extremely effective in causing misclassification. However, these patches are highlighted using standard network interpretation algorithms, thus revealing the identity of the adversary. We show that it is possible to create adversarial patches which not only fool the prediction, but also change what we interpret regarding the cause of the prediction. Moreover, we introduce our attack as a controlled setting to measure the accuracy of interpretation algorithms. We show this using extensive experiments for Grad-CAM interpretation that transfers to occluding patch interpretation as well. We believe our algorithms can facilitate developing more robust network interpretation tools that truly explain the network's underlying decision making process.",
    "IsOld": true
  },
  {
    "Paper-ID": "iccv/SunRS19",
    "Title": "Adaptive Activation Thresholding - Dynamic Routing Type Behavior for Interpretability in Convolutional Neural Networks.",
    "url": "https://doi.org/10.1109/ICCV.2019.00504",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ICCV"
    },
    "Authors": [
      "Yiyou Sun",
      "Sathya N. Ravi",
      "Vikas Singh"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Heatmap",
      "Prototypes"
    ],
    "Method used to explain": [
      "Post-hoc explanation method",
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "There is a growing interest in strategies that can help us understand or interpret neural networks -- that is, not merely provide a prediction, but also offer additional context explaining why and how. While many current methods offer tools to perform this analysis for a given (trained) network post-hoc, recent results (especially on capsule networks) suggest that when classes map to a few high level ``concepts'' in the preceding layers of the network, the behavior of the network is easier to interpret or explain. Such training may be accomplished via dynamic/EM routing where the network ``routes'' for individual classes (or subsets of images) are dynamic and involve few nodes even if the full network may not be sparse. In this paper, we show how a simple modification of the SGD scheme can help provide dynamic/EM routing type behavior in convolutional neural networks. Through extensive experiments, we evaluate the effect of this idea for interpretability where we obtain promising results, while also showing that no compromise in attainable accuracy is involved. Further, we show that the minor modification is seemingly ad-hoc, the new algorithm can be analyzed by an approximate method which provably matches known rates for SGD.",
    "IsOld": true
  },
  {
    "Paper-ID": "iccv/WuS19",
    "Title": "Towards Interpretable Object Detection by Unfolding Latent Structures.",
    "url": "https://doi.org/10.1109/ICCV.2019.00613",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ICCV"
    },
    "Authors": [
      "Tianfu Wu",
      "Xi Song"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Localization"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "This paper first proposes a method of formulating model interpretability in visual understanding tasks based on the idea of unfolding latent structures. It then presents a case study in object detection using popular two-stage region-based convolutional network (i.e., R-CNN) detection systems. The proposed method focuses on weakly-supervised extractive rationale generation, that is learning to unfold latent discriminative part configurations of object instances automatically and simultaneously in detection without using any supervision for part configurations. It utilizes a top-down hierarchical and compositional grammar model embedded in a directed acyclic AND-OR Graph (AOG) to explore and unfold the space of latent part configurations of regions of interest (RoIs). It presents an AOGParsing operator that seamlessly integrates with the RoIPooling/RoIAlign operator widely used in R-CNN and is trained end-to-end. In object detection, a bounding box is interpreted by the best parse tree derived from the AOG on-the-fly, which is treated as the qualitatively extractive rationale generated for interpreting detection. In experiments, Faster R-CNN is used to test the proposed method on the PASCAL VOC 2007 and the COCO 2017 object detection datasets. The experimental results show that the proposed method can compute promising latent structures without hurting the performance. The code and pretrained models are available at https://github.com/iVMCL/iRCNN.",
    "IsOld": true
  },
  {
    "Paper-ID": "iccv/YinTLS019",
    "Title": "Towards Interpretable Face Recognition.",
    "url": "https://doi.org/10.1109/ICCV.2019.00944",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ICCV"
    },
    "Authors": [
      "Bangjie Yin",
      "Luan Tran",
      "Haoxiang Li",
      "Xiaohui Shen",
      "Xiaoming Liu"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Heatmap",
      "Disentanglement"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Deep CNNs have been pushing the frontier of visual recognition over past years. Besides recognition accuracy, strong demands in understanding deep CNNs in the research community motivate developments of tools to dissect pre-trained models to visualize how they make predictions. Recent works further push the interpretability in the network learning stage to learn more meaningful representations. In this work, focusing on a specific area of visual recognition, we report our efforts towards interpretable face recognition. We propose a spatial activation diversity loss to learn more structured face representations. By leveraging the structure, we further design a feature activation diversity loss to push the interpretable representations to be discriminative and robust to occlusions. We demonstrate on three face recognition benchmarks that our proposed method is able to achieve the state-of-art face recognition accuracy with easily interpretable face representations.",
    "IsOld": true
  },
  {
    "Paper-ID": "icdm/AssafGBS19",
    "Title": "MTEX-CNN - Multivariate Time Series EXplanations for Predictions with Convolutional Neural Networks.",
    "url": "https://doi.org/10.1109/ICDM.2019.00106",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ICDM"
    },
    "Authors": [
      "Roy Assaf",
      "Ioana Giurgiu",
      "Frank Bagehorn",
      "Anika Schumann"
    ],
    "Type of Data": [
      "Time series"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Heatmap"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "In this work we present MTEX-CNN, a novel explainable convolutional neural network architecture which can not only be used for making predictions based on multivariate time series data, but also for explaining these predictions. The network architecture consists of two stages and utilizes particular kernel sizes. This allows us to apply gradient based methods for generating saliency maps for both the time dimension and the features. The first stage of the architecture explains which features are most significant to the predictions, while the second stage explains which time segments are the most significant. We validate our approach on two use cases, namely to predict rare server outages in the wild, as well as the average energy production of photovoltaic power plants based on a benchmark data set. We show that our explanations shed light over what the model has learned. We validate this by retraining the network using the most significant features extracted from the explanations and retaining similar performance to training with the full set of features.",
    "IsOld": true
  },
  {
    "Paper-ID": "icdm/ChenL0X19",
    "Title": "Scalable Explanation of Inferences on Large Graphs.",
    "url": "https://doi.org/10.1109/ICDM.2019.00111",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ICDM"
    },
    "Authors": [
      "Chao Chen",
      "Yifei Liu",
      "Xi Zhang",
      "Sihong Xie"
    ],
    "Type of Data": [
      "Graph data"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Decision Tree"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Probabilistic inferences distill knowledge from graphs to aid human make important decisions. Due to the inherent uncertainty in the model and the complexity of the knowledge, it is desirable to help the end-users understand the inference outcomes. Different from deep or high dimensional parametric models, the lack of interpretability in graphical models is due to the cyclic and long-range dependencies and the byzantine inference procedures. Prior works did not tackle cycles and make the inferences interpretable. We formulate the explanation of probabilistic inferences as a constrained cross-entropy minimization problem to find simple subgraphs that faithfully approximate the inferences. We prove that the optimization is NP-hard, while the objective is not monotonic and submodular to guarantee efficient greedy approximation. We propose a beam search algorithm to find trees to enhance the explanation interpretability and diversity. To allow efficient search on large and dense graphs without hurting faithfulness, we further propose parallelization and a pruning strategy. We demonstrate superior performance on four networks from distinct applications, comparing favorably to other explanation methods, including LIME.",
    "IsOld": true
  },
  {
    "Paper-ID": "icdm/HamdiA19",
    "Title": "Interpretable Feature Learning of Graphs using Tensor Decomposition.",
    "url": "https://doi.org/10.1109/ICDM.2019.00037",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ICDM"
    },
    "Authors": [
      "Shah Muhammad Hamdi",
      "Rafal A. Angryk"
    ],
    "Type of Data": [
      "Graph data"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Disentanglement",
      "Heatmap"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "In recent years, node embedding algorithms, which learn low dimensional vector representations for nodes in a graph, have been one of the key research interests of the graph mining community. The existing algorithms either rely on computationally expensive eigendecomposition of the large matrices, or require tuning of the word embedding-based hyperparameters as a result of representing the graph as a node sequence similar to the sentences in a document. Moreover, the latent features produced by these algorithms are hard to interpret. In this paper, we present two novel tensor decomposition-based node embedding algorithms, that can learn node features from arbitrary types of graphs: undirected, directed, and/or weighted, without relying on eigendecomposition or word embedding-based hyperparameters. Both algorithms preserve the local and global structural properties of the graph by using k-step transition probability matrices to construct third-order multidimensional arrays or tensors and perform CANDECOMP/PARAFAC (CP) decomposition in order to produce an interpretable and low dimensional vector space for the nodes. Our experiments encompass different types of graphs (undirected/directed, unweighted/weighted, sparse/dense) of different domains such as social networking and neuroscience. Our experimental evaluation proves our models to be interpretable with respect to the understandability of the feature space, precise with respect to the network reconstruction and link prediction, and accurate with respect to node classification and graph classification.",
    "IsOld": true
  },
  {
    "Paper-ID": "icdm/YooS19",
    "Title": "EDiT - Interpreting Ensemble Models via Compact Soft Decision Trees.",
    "url": "https://doi.org/10.1109/ICDM.2019.00187",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ICDM"
    },
    "Authors": [
      "Jaemin Yoo",
      "Lee Sael"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "Tree Ensemble"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Decision Tree"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Given feature-based data, how can we accurately classify individual input and interpret the result of it? Ensemble models are often the best choice in terms of accuracy when dealing with feature-based datasets. However, interpreting the decision made by the ensemble model for individual input seems intractable. On the other hand, decision trees, although being prone to overfit, are considered as the most interpretable in terms of being able to trace the decision process of individual input. In this work, we propose Ensemble to Distilled Tree (EDiT), a novel distilling method that generates compact soft decision trees from ensemble models. EDiT exploits the interpretability of a tree-based structure by removing redundant branches and learning sparse weights, while enhancing accuracy by distilling the knowledge of ensemble models such as random forests (RF). Our experiments on eight datasets show that EDiT reduces the number of parameters of an RF by 6.4 to 498.4 times with a minor loss of classification accuracy.",
    "IsOld": true
  },
  {
    "Paper-ID": "icdm/ZhangQLYWZ19",
    "Title": "KnowRisk - An Interpretable Knowledge-Guided Model for Disease Risk Prediction.",
    "url": "https://doi.org/10.1109/ICDM.2019.00196",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ICDM"
    },
    "Authors": [
      "Xianli Zhang",
      "Buyue Qian",
      "Yang Li",
      "Changchang Yin",
      "Xudong Wang",
      "Qinghua Zheng"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Graph data"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Thanks to the widespread adoption of Electronic Health Record (EHR) systems, a variety of data-driven clinical risk prediction approaches have been spawned in recent years. However, there remain three challenges, which if addressed would improve the performance and applicability of such models. (i) Due to the limited data sharing between different health care institutions, the EHR data collected by a single institution is often inadequate or missing some visits records. The limited number of data cannot meet the large sample required of recent approaches especially deep learning models. In addition, the missing records (due to visiting different institution) may contain important health condition of the patient, which if ignored would cause prediction bias. (ii) Few existing approaches take clinical knowledge into account. The auxiliary knowledge if included can greatly reduce the data dependency of many modern learning algorithms. (iii) Most existing deep learning based methods are unable to identify the contribution of each medical event to the final results, which prohibits such models from being widely accepted in practical clinical applications. In this paper, we propose an interpretable and knowledge-guided deep model to address these challenges. Specifically, we distill knowledge from a clinical knowledge graph both explicitly and implicitly, which can not only supplement inadequate patient records but also guide the predicting process of the model. Furthermore, skip-connections and attention mechanisms are adopted to improve the interpretability of our model. In the context of heart failure prediction task, our model outperforms several state-of-the-art methods. Finally, a series of case studies are presented to prove the interpretability of our model.",
    "IsOld": true
  },
  {
    "Paper-ID": "icdm/ZhouSC19",
    "Title": "A Model-Agnostic Approach for Explaining the Predictions on Clustered Data.",
    "url": "https://doi.org/10.1109/ICDM.2019.00202",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ICDM"
    },
    "Authors": [
      "Zihan Zhou",
      "Mingxuan Sun",
      "Jianhua Chen"
    ],
    "Type of Data": [
      "User-item matrix",
      "Other"
    ],
    "Type of Problem": [
      "Model Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Any (for a specific task); model-agnostic",
      "Other"
    ],
    "Type of Task": [
      "Classification",
      "Regression"
    ],
    "Type of Explanation": [
      "White-box model"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Machine learning models especially deep neural network models have shown great potential in making decisions when analyzing clustered or longitudinal data. However, lack of model transparency is a major concern in risk sensitive domains such as social science and medical diagnosis. Despite the early success of explaining machine learning models, there is a lack of explanation methods that can be applied to any predictors on clustered data since most of the existing models assume that all observations are independent of each other. In this paper, we address this deficiency and propose to use a linear mixed model to mimic the local behavior of any complex model on clustered data, which can also improve the fidelity of the explanation method to the complex models. We apply our method to explain several models including a deep neural network model on two tasks including movie recommendation and medical record diagnosis. Experiment results show that our model outperforms the baseline models on several metrics such as fidelity and exactness.",
    "IsOld": true
  },
  {
    "Paper-ID": "iclr/ChangCGD19",
    "Title": "Explaining Image Classifiers by Counterfactual Generation.",
    "url": "https://openreview.net/forum?id=B1MXz20cYQ",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ICLR"
    },
    "Authors": [
      "Chun-Hao Chang",
      "Elliot Creager",
      "Anna Goldenberg",
      "David Duvenaud"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Heatmap",
      "Prototypes"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "iclr/ChenSWJ19",
    "Title": "L-Shapley and C-Shapley - Efficient Model Interpretation for Structured Data.",
    "url": "https://openreview.net/forum?id=S1E3Ko09F7",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ICLR"
    },
    "Authors": [
      "Jianbo Chen",
      "Le Song",
      "Martin J. Wainwright",
      "Michael I. Jordan"
    ],
    "Type of Data": [
      "Images",
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Localization"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "iclr/FortuinHLSR19",
    "Title": "SOM-VAE - Interpretable Discrete Representation Learning on Time Series.",
    "url": "https://openreview.net/forum?id=rygjcsR9Y7",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ICLR"
    },
    "Authors": [
      "Vincent Fortuin",
      "Matthias H\u00fcser",
      "Francesco Locatello",
      "Heiko Strathmann",
      "Gunnar R\u00e4tsch"
    ],
    "Type of Data": [
      "Images",
      "Time series"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Representation learning",
      "Clustering"
    ],
    "Type of Explanation": [
      "Representation Visualization",
      "Representation Synthesis"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "iclr/SinghMY19",
    "Title": "Hierarchical interpretations for neural network predictions.",
    "url": "https://openreview.net/forum?id=SkEqro0ctQ",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ICLR"
    },
    "Authors": [
      "Chandan Singh",
      "W. James Murdoch",
      "Bin Yu"
    ],
    "Type of Data": [
      "Images",
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Graph"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "iclr/ZhengPBH19",
    "Title": "Revealing interpretable object representations from human behavior.",
    "url": "https://openreview.net/forum?id=ryxSrhC9KX",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ICLR"
    },
    "Authors": [
      "Charles Y. Zheng",
      "Francisco Pereira",
      "Chris I. Baker",
      "Martin N. Hebart"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Representation learning"
    ],
    "Type of Explanation": [
      "Disentanglement",
      "Feature Importance"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "icml/0002LA19",
    "Title": "Exploring interpretable LSTM neural networks over multi-variable data.",
    "url": "http://proceedings.mlr.press/v97/guo19b.html",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ICML"
    },
    "Authors": [
      "Tian Guo",
      "Tao Lin",
      "Nino Antulov-Fantulin"
    ],
    "Type of Data": [
      "Time series"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Regression"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Heatmap"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "icml/AnconaOG19",
    "Title": "Explaining Deep Neural Networks with a Polynomial Time Algorithm for Shapley Value Approximation.",
    "url": "http://proceedings.mlr.press/v97/ancona19a.html",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ICML"
    },
    "Authors": [
      "Marco Ancona",
      "Cengiz \u00d6ztireli",
      "Markus H. Gross"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Images",
      "Any"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification",
      "Regression"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "icml/DunckerBBS19",
    "Title": "Learning interpretable continuous-time models of latent stochastic dynamical systems.",
    "url": "http://proceedings.mlr.press/v97/duncker19a.html",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ICML"
    },
    "Authors": [
      "Lea Duncker",
      "Gergo Bohner",
      "Julien Boussard",
      "Maneesh Sahani"
    ],
    "Type of Data": [
      "Time series"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "Bayesian or Hierarchical Network"
    ],
    "Type of Task": [
      "Regression"
    ],
    "Type of Explanation": [
      "Representation Visualization",
      "Feature plot"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "icml/GoyalWEBPL19",
    "Title": "Counterfactual Visual Explanations.",
    "url": "http://proceedings.mlr.press/v97/goyal19a.html",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ICML"
    },
    "Authors": [
      "Yash Goyal",
      "Ziyan Wu",
      "Jan Ernst",
      "Dhruv Batra",
      "Devi Parikh",
      "Stefan Lee"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Localization",
      "Prototypes",
      "Representation Synthesis"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "icml/SinglaWFF19",
    "Title": "Understanding Impacts of High-Order Loss Approximations and Features in Deep Learning Interpretation.",
    "url": "http://proceedings.mlr.press/v97/singla19a.html",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ICML"
    },
    "Authors": [
      "Sahil Singla",
      "Eric Wallace",
      "Shi Feng",
      "Soheil Feizi"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Heatmap"
    ],
    "Method used to explain": [
      "Post-hoc explanation method",
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "icml/VedantamDLRBP19",
    "Title": "Probabilistic Neural Symbolic Models for Interpretable Visual Question Answering.",
    "url": "http://proceedings.mlr.press/v97/vedantam19a.html",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ICML"
    },
    "Authors": [
      "Ramakrishna Vedantam",
      "Karan Desai",
      "Stefan Lee",
      "Marcus Rohrbach",
      "Dhruv Batra",
      "Devi Parikh"
    ],
    "Type of Data": [
      "Images",
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Question Answering"
    ],
    "Type of Explanation": [
      "Decision Rules"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model",
      "Supervised explanation training"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "icml/WangZB19",
    "Title": "Bias Also Matters - Bias Attribution for Deep Neural Network Explanation.",
    "url": "http://proceedings.mlr.press/v97/wang19p.html",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "ICML"
    },
    "Authors": [
      "Shengjie Wang",
      "Tianyi Zhou",
      "Jeff A. Bilmes"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Heatmap"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "ijcai/ChenW0WBWC19",
    "Title": "Co-Attentive Multi-Task Learning for Explainable Recommendation.",
    "url": "https://doi.org/10.24963/ijcai.2019/296",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "IJCAI"
    },
    "Authors": [
      "Zhongxia Chen",
      "Xiting Wang",
      "Xing Xie",
      "Tong Wu",
      "Guoqing Bu",
      "Yining Wang",
      "Enhong Chen"
    ],
    "Type of Data": [
      "Text",
      "User-item matrix"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Recommendation"
    ],
    "Type of Explanation": [
      "Text"
    ],
    "Method used to explain": [
      "Supervised explanation training"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Despite widespread adoption, recommender systems remain mostly black boxes. Recently, providing explanations about why items are recommended has attracted increasing attention due to its capability to enhance user trust and satisfaction. In this paper, we propose a co-attentive multi-task learning model for explainable recommendation. Our model improves both prediction accuracy and explainability of recommendation by fully exploiting the correlations between the recommendation task and the explanation task. In particular, we design an encoder-selector-decoder architecture inspired by human's information-processing model in cognitive psychology. We also propose a hierarchical co-attentive selector to effectively model the cross knowledge transferred for both tasks. Our model not only enhances prediction accuracy of the recommendation task, but also generates linguistic explanations that are fluent, useful, and highly personalized. Experiments on three public datasets demonstrate the effectiveness of our model.",
    "IsOld": true
  },
  {
    "Paper-ID": "ijcai/FuscoVVWS19",
    "Title": "RecoNet - An Interpretable Neural Architecture for Recommender Systems.",
    "url": "https://doi.org/10.24963/ijcai.2019/325",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "IJCAI"
    },
    "Authors": [
      "Francesco Fusco",
      "Michalis Vlachos",
      "Vasileios Vasileiadis",
      "Kathrin Wardatzky",
      "Johannes Schneider"
    ],
    "Type of Data": [
      "User-item matrix"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Recommendation"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Post-hoc explanation method",
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Neural systems offer high predictive accuracy but are plagued by long training times and low interpretability. We present a simple neural architecture for recommender systems that lifts several of these shortcomings. Firstly, the approach has a high predictive power that is comparable to state-of-the-art recommender approaches. Secondly, owing to its simplicity, the trained model can be interpreted easily because it provides the individual contribution of each input feature to the decision. Our method is three orders of magnitude faster than general-purpose explanatory approaches, such as LIME. Finally, thanks to its design, our architecture addresses cold-start issues, and therefore the model does not require retraining in the presence of new users.",
    "IsOld": true
  },
  {
    "Paper-ID": "ijcai/HouWCLZL19",
    "Title": "Explainable Fashion Recommendation - A Semantic Attribute Region Guided Approach.",
    "url": "https://doi.org/10.24963/ijcai.2019/650",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "IJCAI"
    },
    "Authors": [
      "Min Hou",
      "Le Wu",
      "Enhong Chen",
      "Zhi Li",
      "Vincent W. Zheng",
      "Qi Liu"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification",
      "Recommendation"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Heatmap",
      "Localization",
      "Prototypes",
      "Text"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "In fashion recommender systems, each product usually consists of multiple semantic attributes (e.g., sleeves, collar, etc). When making cloth decisions, people usually show preferences for different semantic attributes (e.g., the clothes with v-neck collar). Nevertheless, most previous fashion recommendation models comprehend the clothing images with a global content representation and lack detailed understanding of users' semantic preferences, which usually leads to inferior recommendation performance. To bridge this gap, we propose a novel Semantic Attribute Explainable Recommender System (SAERS). Specifically, we first introduce a fine-grained interpretable semantic space. We then develop a Semantic Extraction Network (SEN) and Fine-grained Preferences Attention (FPA) module to project users and items into this space, respectively. With SAERS, we are capable of not only providing cloth recommendations for users, but also explaining the reason why we recommend the cloth through intuitive visual attribute semantic highlights in a personalized manner. Extensive experiments conducted on real-world datasets clearly demonstrate the effectiveness of our approach compared with the state-of-the-art methods.",
    "IsOld": true
  },
  {
    "Paper-ID": "ijcai/KennyK19",
    "Title": "Twin-Systems to Explain Artificial Neural Networks using Case-Based Reasoning - Comparative Tests of Feature-Weighting Methods in ANN-CBR Twins for XAI.",
    "url": "https://doi.org/10.24963/ijcai.2019/376",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "IJCAI"
    },
    "Authors": [
      "Eoin M. Kenny",
      "Mark T. Keane"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Images",
      "Any"
    ],
    "Type of Problem": [
      "Model Explanation",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification",
      "Regression"
    ],
    "Type of Explanation": [
      "Prototypes"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "In this paper, twin-systems are described to address the eXplainable artificial intelligence (XAI) problem, where a black box model is mapped to a white box \u201ctwin\u201d that is more interpretable, with both systems using the same dataset. The framework is instantiated by twinning an artificial neural network (ANN; black box) with a case-based reasoning system (CBR; white box), and mapping the feature weights from the former to the latter to find cases that explain the ANN\u2019s outputs. Using a novel evaluation method, the effectiveness of this twin-system approach is demonstrated by showing that nearest neighbor cases can be found to match the ANN predictions for benchmark datasets. Several feature-weighting methods are competitively tested in two experiments, including our novel, contributions-based method (called COLE) that is found to perform best. The tests consider the \u201dtwinning\u201d of traditional multilayer perceptron (MLP) networks and convolutional neural networks (CNN) with CBR systems. For the CNNs trained on image data, qualitative evidence shows that cases provide plausible explanations for the CNN\u2019s classifications.",
    "IsOld": true
  },
  {
    "Paper-ID": "ijcai/LiYYJ19",
    "Title": "Learning Interpretable Deep State Space Model for Probabilistic Time Series Forecasting.",
    "url": "https://doi.org/10.24963/ijcai.2019/402",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "IJCAI"
    },
    "Authors": [
      "Longyuan Li",
      "Junchi Yan",
      "Xiaokang Yang",
      "Yaohui Jin"
    ],
    "Type of Data": [
      "Time series"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Regression"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Probabilistic time series forecasting involves estimating the distribution of future based on its history, which is essential for risk management in downstream decision-making. We propose a deep state space model for probabilistic time series forecasting whereby the non-linear emission model and transition model are parameterized by networks and the dependency is modeled by recurrent neural nets. We take the automatic relevance determination (ARD) view and devise a network to exploit the exogenous variables in addition to time series. In particular, our ARD network can incorporate the uncertainty of the exogenous variables and eventually helps identify useful exogenous variables and suppress those irrelevant for forecasting. The distribution of multi-step ahead forecasts are approximated by Monte Carlo simulation. We show in experiments that our model produces accurate and sharp probabilistic forecasts. The estimated uncertainty of our forecasting also realistically increases over time, in a spontaneous manner.",
    "IsOld": true
  },
  {
    "Paper-ID": "ijcai/ZhangR19",
    "Title": "Learning Interpretable Relational Structures of Hinge-loss Markov Random Fields.",
    "url": "https://doi.org/10.24963/ijcai.2019/838",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "IJCAI"
    },
    "Authors": [
      "Yue Zhang",
      "Arti Ramesh"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Policy learning",
      "Classification"
    ],
    "Type of Explanation": [
      "Decision Rules"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Statistical relational models such as Markov logic networks (MLNs) and hinge-loss Markov random fields (HL-MRFs) are specified using templated weighted first-order logic clauses, leading to the creation of complex, yet easy to encode models that effectively combine uncertainty and logic. Learning the structure of these models from data reduces the human effort of identifying the right structures. In this work, we present an asynchronous deep reinforcement learning algorithm to automatically learn HL-MRF clause structures. Our algorithm possesses the ability to learn semantically meaningful structures that appeal to human intuition and understanding, while simultaneously being able to learn structures from data, thus learning structures that have both the desirable qualities of interpretability and good prediction performance. The asynchronous nature of our algorithm further provides the ability to learn diverse structures via exploration, while remaining scalable. We demonstrate the ability of the models to learn semantically meaningful structures that also achieve better prediction performance when compared with a greedy search algorithm, a path-based algorithm, and manually defined clauses on two computational social science applications: i) modeling recovery in alcohol use disorder, and ii) detecting bullying.",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/ChengSHZ19",
    "Title": "Incorporating Interpretability into Latent Factor Models via Fast Influence Analysis.",
    "url": "https://doi.org/10.1145/3292500.3330857",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Weiyu Cheng",
      "Yanyan Shen",
      "Linpeng Huang",
      "Yanmin Zhu"
    ],
    "Type of Data": [
      "User-item matrix"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Latent factor models (LFMs) such as matrix factorization have achieved the state-of-the-art performance among various collaborative filtering approaches for recommendation. Despite the high recommendation accuracy of LFMs, a critical issue to be resolved is their lack of interpretability. Extensive efforts have been devoted to interpreting the prediction results of LFMs. However, they either rely on auxiliary information which may not be available in practice, or sacrifice recommendation accuracy for interpretability. Influence functions, stemming from robust statistics, have been developed to understand the effect of training points on the predictions of black-box models. Inspired by this, we propose a novel explanation method named FIA (Fast Influence Analysis) to understand the prediction of trained LFMs by tracing back to the training data with influence functions. We present how to employ influence functions to measure the impact of historical user-item interactions on the prediction results of LFMs and provide intuitive neighbor-style explanations based on the most influential interactions. Our proposed FIA exploits the characteristics of two important LFMs, matrix factorization and neural collaborative filtering, and is capable of accelerating the overall influence analysis process. We provide a detailed complexity analysis for FIA over LFMs and conduct extensive experiments to evaluate its performance using real-world datasets. The results demonstrate the effectiveness and efficiency of FIA, and the usefulness of the generated explanations for the recommendation results.",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/Jia0RLH19",
    "Title": "Improving the Quality of Explanations with Local Embedding Perturbations.",
    "url": "https://doi.org/10.1145/3292500.3330930",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Yunzhe Jia",
      "James Bailey",
      "Kotagiri Ramamohanarao",
      "Christopher Leckie",
      "Michael E. Houle"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Tree Ensemble",
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Localization"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Classifier explanations have been identified as a crucial component of knowledge discovery. Local explanations evaluate the behavior of a classifier in the vicinity of a given instance. A key step in this approach is to generate synthetic neighbors of the given instance. This neighbor generation process is challenging and it has considerable impact on the quality of explanations. To assess quality of generated neighborhoods, we propose a local intrinsic dimensionality (LID) based locality constraint. Based on this, we then propose a new neighborhood generation method. Our method first fits a local embedding/subspace around a given instance using the LID of the test instance as the target dimensionality, then generates neighbors in the local embedding and projects them back to the original space. Experimental results show that our method generates more realistic neighborhoods and consequently better explanations. It can be used in combination with existing local explanation algorithms.",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/MingXQR19",
    "Title": "Interpretable and Steerable Sequence Learning via Prototypes.",
    "url": "https://doi.org/10.1145/3292500.3330908",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Yao Ming",
      "Panpan Xu",
      "Huamin Qu",
      "Liu Ren"
    ],
    "Type of Data": [
      "Text",
      "Time series",
      "Other"
    ],
    "Type of Problem": [
      "Outcome Explanation",
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Prototypes"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "One of the major challenges in machine learning nowadays is to provide predictions with not only high accuracy but also user-friendly explanations. Although in recent years we have witnessed increasingly popular use of deep neural networks for sequence modeling, it is still challenging to explain the rationales behind the model outputs, which is essential for building trust and supporting the domain experts to validate, critique and refine the model. We propose ProSeNet, an interpretable and steerable deep sequence model with natural explanations derived from case-based reasoning. The prediction is obtained by comparing the inputs to a few prototypes, which are exemplar cases in the problem domain. For better interpretability, we define several criteria for constructing the prototypes, including simplicity, diversity, and sparsity and propose the learning objective and the optimization procedure. ProSeNet also provides a user-friendly approach to model steering: domain experts without any knowledge on the underlying model or parameters can easily incorporate their intuition and experience by manually refining the prototypes. We conduct experiments on a wide range of real-world applications, including predictive diagnostics for automobiles, ECG, and protein sequence classification and sentiment analysis on texts. The result shows that ProSeNet can achieve accuracy on par with state-of-the-art deep learning models. We also evaluate the interpretability of the results with concrete case studies. Finally, through user study on Amazon Mechanical Turk (MTurk), we demonstrate that the model selects high-quality prototypes which align well with human knowledge and can be interactively refined for better interpretability without loss of performance.",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/ShuCW0L19",
    "Title": "dEFEND - Explainable Fake News Detection.",
    "url": "https://doi.org/10.1145/3292500.3330935",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Kai Shu",
      "Limeng Cui",
      "Suhang Wang",
      "Dongwon Lee",
      "Huan Liu"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Text"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/Tao0WFYZ019",
    "Title": "Log2Intent - Towards Interpretable User Modeling via Recurrent Semantics Memory Unit.",
    "url": "https://doi.org/10.1145/3292500.3330889",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Zhiqiang Tao",
      "Sheng Li",
      "Zhaowen Wang",
      "Chen Fang",
      "Longqi Yang",
      "Handong Zhao",
      "Yun Fu"
    ],
    "Type of Data": [
      "Time series"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Text"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model",
      "Supervised explanation training"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/YanZDSSK19",
    "Title": "GroupINN - Grouping-based Interpretable Neural Network for Classification of Limited, Noisy Brain Data.",
    "url": "https://doi.org/10.1145/3292500.3330921",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Yujun Yan",
      "Jiong Zhu",
      "Marlena Duda",
      "Eric Solarz",
      "Chandra Sekhar Sripada",
      "Danai Koutra"
    ],
    "Type of Data": [
      "Images",
      "Time series"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Graph",
      "Heatmap"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/YoshidaTK19",
    "Title": "Learning Interpretable Metric between Graphs - Convex Formulation and Computation with Graph Mining.",
    "url": "https://doi.org/10.1145/3292500.3330845",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Tomoki Yoshida",
      "Ichiro Takeuchi",
      "Masayuki Karasuyama"
    ],
    "Type of Data": [
      "Graph data"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Representation learning"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Localization",
      "Graph"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Graph is a standard approach to modeling structured data. Although many machine learning methods depend on the metric of the input objects, defining an appropriate distance function on graph is still a controversial issue. We propose a novel supervised metric learning method for a subgraph-based distance, called interpretable graph metric learning (IGML). IGML optimizes the distance function in such a way that a small number of important subgraphs can be adaptively selected. This optimization is computationally intractable with naive application of existing optimization algorithms. We construct a graph mining based efficient algorithm to deal with this computational difficulty. Important advantages of our method are 1) guarantee of the optimality from the convex formulation, and 2) high interpretability of results. To our knowledge, none of the existing studies provide an interpretable subgraph-based metric in a supervised manner. In our experiments, we empirically verify superior or comparable prediction performance of IGML to other existing graph classification methods which do not have clear interpretability. Further, we demonstrate usefulness of IGML through some illustrative examples of extracted subgraphs and an example of data analysis on the learned metric space.",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/ZhangTKLCC19",
    "Title": "Axiomatic Interpretability for Multiclass Additive Models.",
    "url": "https://doi.org/10.1145/3292500.3330898",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Xuezhou Zhang",
      "Sarah Tan",
      "Paul Koch",
      "Yin Lou",
      "Urszula Chajewska",
      "Rich Caruana"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature plot"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Generalized additive models (GAMs) are favored in many regression and binary classification problems because they are able to fit complex, nonlinear functions while still remaining interpretable. In the first part of this paper, we generalize a state-of-the-art GAM learning algorithm based on boosted trees to the multiclass setting, showing that this multiclass algorithm outperforms existing GAM learning algorithms and sometimes matches the performance of full complexity models such as gradient boosted trees. In the second part, we turn our attention to the interpretability of GAMs in the multiclass setting. Surprisingly, the natural interpretability of GAMs breaks down when there are more than two classes. Naive interpretation of multiclass GAMs can lead to false conclusions. Inspired by binary GAMs, we identify two axioms that any additive model must satisfy in order to not be visually misleading. We then develop a technique called Additive Post-Processing for Interpretability (API) that provably transforms a pretrained additive model to satisfy the interpretability axioms without sacrificing accuracy. The technique works not just on models trained with our learning algorithm, but on any multiclass additive model, including multiclass linear and logistic regression. We demonstrate the effectiveness of API on a 12-class infant mortality dataset.",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/ZhaoGS19",
    "Title": "Riker - Mining Rich Keyword Representations for Interpretable Product Question Answering.",
    "url": "https://doi.org/10.1145/3292500.3330985",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Jie Zhao",
      "Ziyu Guan",
      "Huan Sun"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Question Answering"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/ChenLTBRS19",
    "Title": "This Looks Like That - Deep Learning for Interpretable Image Recognition.",
    "url": "https://proceedings.neurips.cc/paper/2019/hash/adf7ee2dcf142b0e11888e72b43fcb75-Abstract.html",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Chaofan Chen",
      "Oscar Li",
      "Daniel Tao",
      "Alina Barnett",
      "Cynthia Rudin",
      "Jonathan Su"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Prototypes"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/GhorbaniWZK19",
    "Title": "Towards Automatic Concept-based Explanations.",
    "url": "https://proceedings.neurips.cc/paper/2019/hash/77d2afcb31f6493e350fca61764efb9a-Abstract.html",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Amirata Ghorbani",
      "James Wexler",
      "James Y. Zou",
      "Been Kim"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Prototypes",
      "Localization",
      "Feature Importance"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/HoyerMKKF19",
    "Title": "Grid Saliency for Context Explanations of Semantic Segmentation.",
    "url": "https://proceedings.neurips.cc/paper/2019/hash/6950aa02ae8613af620668146dd11840-Abstract.html",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Lukas Hoyer",
      "Mauricio Munoz",
      "Prateek Katiyar",
      "Anna Khoreva",
      "Volker Fischer"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Other"
    ],
    "Type of Explanation": [
      "Heatmap",
      "Localization"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/KimL19",
    "Title": "Learning Dynamics of Attention - Human Prior for Interpretable Machine Reasoning.",
    "url": "https://proceedings.neurips.cc/paper/2019/hash/ae3539867aaeec609a4260c6feb725f4-Abstract.html",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Wonjae Kim",
      "Yoonho Lee"
    ],
    "Type of Data": [
      "Images",
      "Text",
      "Graph data"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Question Answering"
    ],
    "Type of Explanation": [
      "Heatmap"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/WangN19",
    "Title": "Deliberative Explanations - visualizing network insecurities.",
    "url": "https://proceedings.neurips.cc/paper/2019/hash/68053af2923e00204c3ca7c6a3150cf7-Abstract.html",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Pei Wang",
      "Nuno Vasconcelos"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Localization",
      "Prototypes"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/YingBYZL19",
    "Title": "GNNExplainer - Generating Explanations for Graph Neural Networks.",
    "url": "https://proceedings.neurips.cc/paper/2019/hash/d80b7040b773199015de6d3b4293c8ff-Abstract.html",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Zhitao Ying",
      "Dylan Bourgeois",
      "Jiaxuan You",
      "Marinka Zitnik",
      "Jure Leskovec"
    ],
    "Type of Data": [
      "Graph data"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Graph"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "sigir/BalogRA19",
    "Title": "Transparent, Scrutable and Explainable User Models for Personalized Recommendation.",
    "url": "https://doi.org/10.1145/3331184.3331211",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "SIGIR"
    },
    "Authors": [
      "Krisztian Balog",
      "Filip Radlinski",
      "Shushan Arakelyan"
    ],
    "Type of Data": [
      "User-item matrix"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Recommendation"
    ],
    "Type of Explanation": [
      "Text"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Most recommender systems base their recommendations on implicit or explicit item-level feedback provided by users. These item ratings are combined into a complex user model, which then predicts the suitability of other items. While effective, such methods have limited scrutability and transparency. For instance, if a user's interests change, then many item ratings would usually need to be modified to significantly shift the user's recommendations. Similarly, explaining how the system characterizes the user is impossible, short of presenting the entire list of known item ratings. In this paper, we present a new set-based recommendation technique that permits the user model to be explicitly presented to users in natural language, empowering users to understand recommendations made and improve the recommendations dynamically. While performing comparably to traditional collaborative filtering techniques in a standard static setting, our approach allows users to efficiently improve recommendations. Further, it makes it easier for the model to be validated and adjusted, building user trust and understanding.",
    "IsOld": true
  },
  {
    "Paper-ID": "sigir/HanSYWN19",
    "Title": "Prototype-guided Attribute-wise Interpretable Scheme for Clothing Matching.",
    "url": "https://doi.org/10.1145/3331184.3331245",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "SIGIR"
    },
    "Authors": [
      "Xianjing Han",
      "Xuemeng Song",
      "Jianhua Yin",
      "Yinglong Wang",
      "Liqiang Nie"
    ],
    "Type of Data": [
      "Images",
      "Text",
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Prototypes",
      "Text"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Recently, as an essential part of people's daily life, clothing matching has gained increasing research attention. Most existing efforts focus on the numerical compatibility modeling between fashion items with advanced neural networks, and hence suffer from the poor interpretation, which makes them less applicable in real world applications. In fact, people prefer to know not only whether the given fashion items are compatible, but also the reasonable interpretations as well as suggestions regarding how to make the incompatible outfit harmonious. Considering that the research line of the comprehensively interpretable clothing matching is largely untapped, in this work, we propose a prototype-guided attribute-wise interpretable compatibility modeling (PAICM) scheme, which seamlessly integrates the latent compatible/incompatible prototype learning and compatibility modeling with the Bayesian personalized ranking (BPR) framework. In particular, the latent attribute interaction prototypes, learned by the non-negative matrix factorization (NMF), are treated as templates to interpret the discordant attribute and suggest the alternative item for each fashion item pair. Extensive experiments on the real-world dataset have demonstrated the effectiveness of our scheme.",
    "IsOld": true
  },
  {
    "Paper-ID": "sigir/LiQPQDW19",
    "Title": "A Capsule Network for Recommendation and Explaining What You Like and Dislike.",
    "url": "https://doi.org/10.1145/3331184.3331216",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "SIGIR"
    },
    "Authors": [
      "Chenliang Li",
      "Cong Quan",
      "Li Peng",
      "Yunwei Qi",
      "Yuming Deng",
      "Libing Wu"
    ],
    "Type of Data": [
      "Text",
      "User-item matrix"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Recommendation"
    ],
    "Type of Explanation": [
      "Heatmap",
      "Localization"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "User reviews contain rich semantics towards the preference of users to features of items. Recently, many deep learning based solutions have been proposed by exploiting reviews for recommendation. The attention mechanism is mainly adopted in these works to identify words or aspects that are important for rating prediction. However, it is still hard to understand whether a user likes or dislikes an aspect of an item according to what viewpoint the user holds and to what extent, without examining the review details. Here, we consider a pair of a viewpoint held by a user and an aspect of an item as a logic unit. Reasoning a rating behavior by discovering the informative logic units from the reviews and resolving their corresponding sentiments could enable a better rating prediction with explanation. To this end, in this paper, we propose a capsule network based model for rating prediction with user reviews, named CARP. For each user-item pair, CARP is devised to extract the informative logic units from the reviews and infer their corresponding sentiments. The model firstly extracts the viewpoints and aspects from the user and item review documents respectively. Then we derive the representation of each logic unit based on its constituent viewpoint and aspect. A sentiment capsule architecture with a novel Routing by Bi-Agreement mechanism is proposed to identify the informative logic unit and the sentiment based representations in user-item level for rating prediction. Extensive experiments are conducted over seven real-world datasets with diverse characteristics. Our results demonstrate that the proposed CARP obtains substantial performance gain over recently proposed state-of-the-art models in terms of prediction accuracy. Further analysis shows that our model can successfully discover the interpretable reasons at a finer level of granularity.",
    "IsOld": true
  },
  {
    "Paper-ID": "sigir/TaoJWW19",
    "Title": "The FacT - Taming Latent Factor Models for Explainability with Factorization Trees.",
    "url": "https://doi.org/10.1145/3331184.3331244",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "SIGIR"
    },
    "Authors": [
      "Yiyi Tao",
      "Yiling Jia",
      "Nan Wang",
      "Hongning Wang"
    ],
    "Type of Data": [
      "User-item matrix"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Recommendation"
    ],
    "Type of Explanation": [
      "Decision Tree",
      "Text"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Latent factor models have achieved great success in personalized recommendations, but they are also notoriously difficult to explain. In this work, we integrate regression trees to guide the learning of latent factor models for recommendation, and use the learnt tree structure to explain the resulting latent factors. Specifically, we build regression trees on users and items respectively with user-generated reviews, and associate a latent profile to each node on the trees to represent users and items. With the growth of regression tree, the latent factors are gradually refined under the regularization imposed by the tree structure. As a result, we are able to track the creation of latent profiles by looking into the path of each factor on regression trees, which thus serves as an explanation for the resulting recommendations. Extensive experiments on two large collections of Amazon and Yelp reviews demonstrate the advantage of our model over several competitive baseline algorithms. Besides, our extensive user study also confirms the practical value of explainable recommendations generated by our model.",
    "IsOld": true
  },
  {
    "Paper-ID": "sigir/XianFMMZ19",
    "Title": "Reinforcement Knowledge Graph Reasoning for Explainable Recommendation.",
    "url": "https://doi.org/10.1145/3331184.3331203",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "SIGIR"
    },
    "Authors": [
      "Yikun Xian",
      "Zuohui Fu",
      "S. Muthukrishnan",
      "Gerard de Melo",
      "Yongfeng Zhang"
    ],
    "Type of Data": [
      "User-item matrix"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Policy learning",
      "Recommendation"
    ],
    "Type of Explanation": [
      "Graph"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Recent advances in personalized recommendation have sparked great interest in the exploitation of rich structured information provided by knowledge graphs. Unlike most existing approaches that only focus on leveraging knowledge graphs for more accurate recommendation, we aim to conduct explicit reasoning with knowledge for decision making so that the recommendations are generated and supported by an interpretable causal inference procedure. To this end, we propose a method called Policy-Guided Path Reasoning (PGPR), which couples recommendation and interpretability by providing actual paths in a knowledge graph. Our contributions include four aspects. We first highlight the significance of incorporating knowledge graphs into recommendation to formally define and interpret the reasoning process. Second, we propose a reinforcement learning (RL) approach featured by an innovative soft reward strategy, user-conditional action pruning and a multi-hop scoring function. Third, we design a policy-guided graph search algorithm to efficiently and effectively sample reasoning paths for recommendation. Finally, we extensively evaluate our method on several large-scale real-world benchmark datasets, obtaining favorable results compared with state-of-the-art methods.",
    "IsOld": true
  },
  {
    "Paper-ID": "sigir/Yang0WMF0C19",
    "Title": "Interpretable Fashion Matching with Rich Attributes.",
    "url": "https://doi.org/10.1145/3331184.3331242",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "SIGIR"
    },
    "Authors": [
      "Xun Yang",
      "Xiangnan He",
      "Xiang Wang",
      "Yunshan Ma",
      "Fuli Feng",
      "Meng Wang",
      "Tat-Seng Chua"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Decision Rules",
      "Decision Tree",
      "Prototypes"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Understanding the mix-and-match relationships of fashion items receives increasing attention in fashion industry. Existing methods have primarily utilized the visual content to learn the visual compatibility and performed matching in a latent space. Despite their effectiveness, these methods work like a black box and cannot reveal the reasons that two items match well. The rich attributes associated with fashion items, e.g.,off-shoulder dress and black skinny jean, which describe the semantics of items in a human-interpretable way, have largely been ignored. This work tackles the interpretable fashion matching task, aiming to inject interpretability into the compatibility modeling of items. Specifically, given a corpus of matched pairs of items, we not only can predict the compatibility score of unseen pairs, but also learn the interpretable patterns that lead to a good match, e.g., white T-shirt matches with black trouser. We propose a new solution named A ttribute-based I nterpretable C ompatibility (AIC) method, which consists of three modules: 1) a tree-based module that extracts decision rules on matching prediction; 2) an embedding module that learns vector representation for a rule by accounting for the attribute semantics; and 3) a joint modeling module that unifies the visual embedding and rule embedding to predict the matching score. To justify our proposal, we contribute a new Lookastic dataset with fashion attributes available. Extensive experiments show that AIC not only outperforms several state-of-the-art methods, but also provides good interpretability on matching decisions.",
    "IsOld": true
  },
  {
    "Paper-ID": "www/MaZCJWLMR19",
    "Title": "Jointly Learning Explainable Rules for Recommendation with Knowledge Graph.",
    "url": "https://doi.org/10.1145/3308558.3313607",
    "Year": "2019",
    "Venue": {
      "isOld": true,
      "value": "WWW"
    },
    "Authors": [
      "Weizhi Ma",
      "Min Zhang",
      "Yue Cao",
      "Woojeong Jin",
      "Chenyang Wang",
      "Yiqun Liu",
      "Shaoping Ma",
      "Xiang Ren"
    ],
    "Type of Data": [
      "Graph data",
      "User-item matrix"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Recommendation"
    ],
    "Type of Explanation": [
      "Decision Rules"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Explainability and effectiveness are two key aspects for building recommender systems. Prior efforts mostly focus on incorporating side information to achieve better recommendation performance. However, these methods have some weaknesses: (1) prediction of neural network-based embedding methods are hard to explain and debug; (2) symbolic, graph-based approaches (e.g., meta path-based models) require manual efforts and domain knowledge to define patterns and rules, and ignore the item association types (e.g. substitutable and complementary). In this paper, we propose a novel joint learning framework to integrate induction of explainable rules from knowledge graph with construction of a rule-guided neural recommendation model. The framework encourages two modules to complement each other in generating effective and explainable recommendation: 1) inductive rules, mined from item-centric knowledge graphs, summarize common multi-hop relational patterns for inferring different item associations and provide human-readable explanation for model prediction; 2) recommendation module can be augmented by induced rules and thus have better generalization ability dealing with the cold-start issue. Extensive experiments1 show that our proposed method has achieved significant improvements in item recommendation over baselines on real-world datasets. Our model demonstrates robust performance over \u201cnoisy\u201d item knowledge graphs, generated by linking item names to related entities.",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/BiskSCM18",
    "Title": "Learning Interpretable Spatial Operations in a Rich 3D Blocks World.",
    "url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17410",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Yonatan Bisk",
      "Kevin J. Shih",
      "Yejin Choi",
      "Daniel Marcu"
    ],
    "Type of Data": [
      "Other"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Policy learning"
    ],
    "Type of Explanation": [
      "Other"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/HsuM0S18",
    "Title": "An Interpretable Generative Adversarial Approach to Classification of Latent Entity Relations in Unstructured Sentences.",
    "url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16629",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Shiou Tian Hsu",
      "Changsung Moon",
      "Paul Jones",
      "Nagiza F. Samatova"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Localization"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/LiLCR18",
    "Title": "Deep Learning for Case-Based Reasoning Through Prototypes - A Neural Network That Explains Its Predictions.",
    "url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17082",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Oscar Li",
      "Hao Liu",
      "Chaofan Chen",
      "Cynthia Rudin"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Prototypes"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/NguyenKLW18",
    "Title": "An Interpretable Joint Graphical Model for Fact-Checking From Crowds.",
    "url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16673",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "An T. Nguyen",
      "Aditya Kharosekar",
      "Matthew Lease",
      "Byron C. Wallace"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Logistic Regression"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Other"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/PalangiSHD18",
    "Title": "Question-Answering with Grammatically-Interpretable Representations.",
    "url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17090",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Hamid Palangi",
      "Paul Smolensky",
      "Xiaodong He",
      "Li Deng"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification",
      "Question Answering"
    ],
    "Type of Explanation": [
      "Disentanglement"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/Ribeiro0G18",
    "Title": "Anchors - High-Precision Model-Agnostic Explanations.",
    "url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16982",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Marco T\u00falio Ribeiro",
      "Sameer Singh",
      "Carlos Guestrin"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Images",
      "Text"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Tree Ensemble",
      "Any (for a specific task); model-agnostic",
      "Logistic Regression"
    ],
    "Type of Task": [
      "Classification",
      "Question Answering"
    ],
    "Type of Explanation": [
      "Decision Rules",
      "Localization"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/RossD18",
    "Title": "Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing Their Input Gradients.",
    "url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17337",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Andrew Slavin Ross",
      "Finale Doshi-Velez"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Representation Synthesis"
    ],
    "Method used to explain": [
      "Post-hoc explanation method",
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/RustamovK18",
    "Title": "Interpretable Graph-Based Semi-Supervised Learning via Flows.",
    "url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16396",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Raif M. Rustamov",
      "James T. Klosowski"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Images",
      "Text"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Classification",
      "Regression"
    ],
    "Type of Explanation": [
      "Graph"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/SubramanianPJBH18",
    "Title": "SPINE - SParse Interpretable Neural Embeddings.",
    "url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17433",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Anant Subramanian",
      "Danish Pruthi",
      "Harsh Jhamtani",
      "Taylor Berg-Kirkpatrick",
      "Eduard H. Hovy"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Tree Ensemble",
      "Support Vector Machine",
      "Logistic Regression",
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Classification",
      "Regression",
      "Representation learning"
    ],
    "Type of Explanation": [
      "Disentanglement"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/WuHPZ0D18",
    "Title": "Beyond Sparsity - Tree Regularization of Deep Models for Interpretability.",
    "url": "https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16285",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Mike Wu",
      "Michael C. Hughes",
      "Sonali Parbhoo",
      "Maurizio Zazzi",
      "Volker Roth",
      "Finale Doshi-Velez"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Time series"
    ],
    "Type of Problem": [
      "Model Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Decision Tree"
    ],
    "Method used to explain": [
      "Post-hoc explanation method",
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "acl/EskenaziLZ18",
    "Title": "Unsupervised Discrete Sentence Representation Learning for Interpretable Neural Dialog Generation.",
    "url": "https://www.aclweb.org/anthology/P18-1101/",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "ACL"
    },
    "Authors": [
      "Tiancheng Zhao",
      "Kyusong Lee",
      "Maxine Esk\u00e9nazi"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Generation",
      "Representation learning"
    ],
    "Type of Explanation": [
      "Disentanglement"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "The encoder-decoder dialog model is one of the most prominent methods used to build dialog systems in complex domains. Yet it is limited because it cannot output interpretable actions as in traditional systems, which hinders humans from understanding its generation process. We present an unsupervised discrete sentence representation learning method that can integrate with any existing encoder-decoder dialog models for interpretable response generation. Building upon variational autoencoders (VAEs), we present two novel models, DI-VAE and DI-VST that improve VAEs and can discover interpretable semantics via either auto encoding or context predicting. Our methods have been validated on real-world dialog datasets to discover semantic representations and enhance encoder-decoder models with interpretable generation.",
    "IsOld": true
  },
  {
    "Paper-ID": "acl/InuiTT18",
    "Title": "Interpretable and Compositional Relation Learning by Joint Training with an Autoencoder.",
    "url": "https://www.aclweb.org/anthology/P18-1200/",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "ACL"
    },
    "Authors": [
      "Ryo Takahashi",
      "Ran Tian",
      "Kentaro Inui"
    ],
    "Type of Data": [
      "Graph data"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Representation learning",
      "Other"
    ],
    "Type of Explanation": [
      "Disentanglement",
      "Representation Visualization"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Embedding models for entities and relations are extremely useful for recovering missing facts in a knowledge base. Intuitively, a relation can be modeled by a matrix mapping entity vectors. However, relations reside on low dimension sub-manifolds in the parameter space of arbitrary matrices \u2013 for one reason, composition of two relations M1, M2 may match a third M3 (e.g. composition of relations currency_of_country and country_of_film usually matches currency_of_film_budget), which imposes compositional constraints to be satisfied by the parameters (i.e. M1*M2=M3). In this paper we investigate a dimension reduction technique by training relations jointly with an autoencoder, which is expected to better capture compositional constraints. We achieve state-of-the-art on Knowledge Base Completion tasks with strongly improved Mean Rank, and show that joint training with an autoencoder leads to interpretable sparse codings of relations, helps discovering compositional constraints and benefits from compositional training. Our source code is released at github.com/tianran/glimvec.",
    "IsOld": true
  },
  {
    "Paper-ID": "acl/MorencyCPLZ18",
    "Title": "Multimodal Language Analysis in the Wild - CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph.",
    "url": "https://www.aclweb.org/anthology/P18-1208/",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "ACL"
    },
    "Authors": [
      "Amir Zadeh",
      "Paul Pu Liang",
      "Soujanya Poria",
      "Erik Cambria",
      "Louis-Philippe Morency"
    ],
    "Type of Data": [
      "Text",
      "Video",
      "Time series"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Feature plot"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.",
    "IsOld": true
  },
  {
    "Paper-ID": "acl/SchutzeRP18",
    "Title": "Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement.",
    "url": "https://www.aclweb.org/anthology/P18-1032/",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "ACL"
    },
    "Authors": [
      "Nina P\u00f6rner",
      "Hinrich Sch\u00fctze",
      "Benjamin Roth"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "The behavior of deep neural networks (DNNs) is hard to understand. This makes it necessary to explore post hoc explanation methods. We conduct the first comprehensive evaluation of explanation methods for NLP. To this end, we design two novel evaluation paradigms that cover two important classes of NLP problems: small context and large context problems. Both paradigms require no manual annotation and are therefore broadly applicable. We also introduce LIMSSE, an explanation method inspired by LIME that is designed for NLP. We show empirically that LIMSSE, LRP and DeepLIFT are the most effective explanation methods and recommend them for explaining DNNs in NLP.",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/ChuangL0F18",
    "Title": "Learning to Act Properly - Predicting and Explaining Affordances From Images.",
    "url": "http://openaccess.thecvf.com/content_cvpr_2018/html/Chuang_Learning_to_Act_CVPR_2018_paper.html",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "Ching-Yao Chuang",
      "Jiaman Li",
      "Antonio Torralba",
      "Sanja Fidler"
    ],
    "Type of Data": [
      "Images",
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Localization",
      "Text"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "We address the problem of affordance reasoning in diverse scenes that appear in the real world. Affordances relate the agent's actions to their effects when taken on the surrounding objects. In our work, we take the egocentric view of the scene, and aim to reason about action-object affordances that respect both the physical world as well as the social norms imposed by the society. We also aim to teach artificial agents why some actions should not be taken in certain situations, and what would likely happen if these actions would be taken. We collect a new dataset that builds upon ADE20k [32], referred to as ADE-Affordance, which contains annotations enabling such rich visual reasoning. We propose a model that exploits Graph Neural Networks to propagate contextual information from the scene in order to perform detailed affordance reasoning about each object. Our model is showcased through various ablation studies, pointing to successes and challenges in this complex task.",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/FongV18",
    "Title": "Net2Vec - Quantifying and Explaining How Concepts Are Encoded by Filters in Deep Neural Networks.",
    "url": "http://openaccess.thecvf.com/content_cvpr_2018/html/Fong_Net2Vec_Quantifying_and_CVPR_2018_paper.html",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "Ruth Fong",
      "Andrea Vedaldi"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Localization",
      "Prototypes"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "In an effort to understand the meaning of the intermediate representations captured by deep networks, recent papers have tried to associate specific semantic concepts to individual neural network filter responses, where interesting correlations are often found, largely by focusing on extremal filter responses. In this paper, we show that this approach can favor easy-to-interpret cases that are not necessarily representative of the average behavior of a representation. A more realistic but harder-to-study hypothesis is that semantic representations are distributed, and thus filters must be studied in conjunction. In order to investigate this idea while enabling systematic visualization and quantification of multiple filter responses, we introduce the Net2Vec framework, in which semantic concepts are mapped to vectorial embeddings based on corresponding filter responses. By studying such embeddings, we are able to show that 1., in most cases, multiple filters are required to code for a concept, that 2., often filters are not concept specific and help encode multiple concepts, and that 3., compared to single filter activations, filter embeddings are able to better characterize the meaning of a representation and its relationship to other concepts.",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/MascharkaTSM18",
    "Title": "Transparency by Design - Closing the Gap Between Performance and Interpretability in Visual Reasoning.",
    "url": "http://openaccess.thecvf.com/content_cvpr_2018/html/Mascharka_Transparency_by_Design_CVPR_2018_paper.html",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "David Mascharka",
      "Philip Tran",
      "Ryan Soklaski",
      "Arjun Majumdar"
    ],
    "Type of Data": [
      "Images",
      "Text"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Question Answering"
    ],
    "Type of Explanation": [
      "Heatmap"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Visual question answering requires high-order reasoning about an image, which is a fundamental capability needed by machine systems to follow complex directives. Recently, modular networks have been shown to be an effective framework for performing visual reasoning tasks. While modular networks were initially designed with a degree of model transparency, their performance on complex visual reasoning benchmarks was lacking. Current state-of-the-art approaches do not provide an effective mechanism for understanding the reasoning process. In this paper, we close the performance gap between interpretable models and state-of-the-art visual reasoning methods. We propose a set of visual-reasoning primitives which, when composed, manifest as a model capable of performing complex reasoning tasks in an explicitly-interpretable manner. The fidelity and interpretability of the primitives' outputs enable an unparalleled ability to diagnose the strengths and weaknesses of the resulting model. Critically, we show that these primitives are highly performant, achieving state-of-the-art accuracy of 99.1% on the CLEVR dataset. We also show that our model is able to effectively learn generalized representations when provided a small amount of data containing novel object attributes. Using the CoGenT generalization task, we show more than a 20 percentage point improvement over the current state of the art.",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/ParkHARSDR18",
    "Title": "Multimodal Explanations - Justifying Decisions and Pointing to the Evidence.",
    "url": "http://openaccess.thecvf.com/content_cvpr_2018/html/Park_Multimodal_Explanations_Justifying_CVPR_2018_paper.html",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "Dong Huk Park",
      "Lisa Anne Hendricks",
      "Zeynep Akata",
      "Anna Rohrbach",
      "Bernt Schiele",
      "Trevor Darrell",
      "Marcus Rohrbach"
    ],
    "Type of Data": [
      "Images",
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Question Answering"
    ],
    "Type of Explanation": [
      "Heatmap",
      "Localization",
      "Text"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Deep models that are both effective and explainable are desirable in many settings; prior explainable models have been unimodal, offering either image-based visualization of attention weights or text-based generation of post-hoc justifications. We propose a multimodal approach to explanation, and argue that the two modalities provide complementary explanatory strengths. We collect two new datasets to define and evaluate this task, and propose a novel model which can provide joint textual rationale generation and attention visualization. Our datasets define visual and textual justifications of a classification decision for activity recognition tasks (ACT-X) and for visual question answering tasks (VQA-X). We quantitatively show that training with the textual explanations not only yields better textual justification models, but also better localizes the evidence that supports the decision. We also qualitatively show cases where visual explanation is more insightful than textual explanation, and vice versa, supporting our thesis that multimodal explanation models offer significant benefits over unimodal approaches.",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/WangS0H18",
    "Title": "Interpret Neural Networks by Identifying Critical Data Routing Paths.",
    "url": "http://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Interpret_Neural_Networks_CVPR_2018_paper.html",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "Yulong Wang",
      "Hang Su",
      "Bo Zhang",
      "Xiaolin Hu"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Other"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Interpretability of a deep neural network aims to explain the rationale behind its decisions and enable the users to understand the intelligent agents, which has become an important issue due to its importance in practical applications. To address this issue, we develop a Distillation Guided Routing method, which is a flexible framework to interpret a deep neural network by identifying critical data routing paths and analyzing the functional processing behavior of the corresponding layers. Specifically, we propose to discover the critical nodes on the data routing paths during network inferring prediction for individual input samples by learning associated control gates for each layer's output channel. The routing paths can, therefore, be represented based on the responses of concatenated control gates from all the layers, which reflect the network's semantic selectivity regarding to the input patterns and more detailed functional process across different layer levels. Based on the discoveries, we propose an adversarial sample detection algorithm by learning a classifier to discriminate whether the critical data routing paths are from real or adversarial samples. Experiments demonstrate that our algorithm can effectively achieve high defense rate with minor training overhead.",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/WuLCJL18",
    "Title": "Interpretable Video Captioning via Trajectory Structured Localization.",
    "url": "http://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Interpretable_Video_Captioning_CVPR_2018_paper.html",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "Xian Wu",
      "Guanbin Li",
      "Qingxing Cao",
      "Qingge Ji",
      "Liang Lin"
    ],
    "Type of Data": [
      "Video"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Generation"
    ],
    "Type of Explanation": [
      "Localization"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Automatically describing open-domain videos with natural language are attracting increasing interest in the field of artificial intelligence. Most existing methods simply borrow ideas from image captioning and obtain a compact video representation from an ensemble of global image feature before feeding to an RNN decoder which outputs a sentence of variable length. However, it is not only arduous for the generator to focus on specific salient objects at different time given the global video representation, it is more formidable to capture the fine-grained motion information and the relation between moving instances for more subtle linguistic descriptions. In this paper, we propose a Trajectory Structured Attentional Encoder-Decoder (TSA-ED) neural network framework for more elaborate video captioning which works by integrating local spatial-temporal representation at trajectory level through structured attention mechanism. Our proposed method is based on a LSTM-based encoder-decoder framework, which incorporates an attention modeling scheme to adaptively learn the correlation between sentence structure and the moving objects in videos, and consequently generates more accurate and meticulous statement description in the decoding stage. Experimental results demonstrate that the feature representation and structured attention mechanism based on the trajectory cluster can efficiently obtain the local motion information in the video to help generate a more fine-grained video description, and achieve the state-of-the-art performance on the well-known Charades and MSVD datasets.",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/ZhangWZ18a",
    "Title": "Interpretable Convolutional Neural Networks.",
    "url": "http://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_Interpretable_Convolutional_Neural_CVPR_2018_paper.html",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "Quanshi Zhang",
      "Ying Nian Wu",
      "Song-Chun Zhu"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Disentanglement",
      "Heatmap",
      "Localization"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "This paper proposes a method to modify a traditional convolutional neural network (CNN) into an interpretable CNN, in order to clarify knowledge representations in high conv-layers of the CNN. In an interpretable CNN, each filter in a high conv-layer represents a specific object part. Our interpretable CNNs use the same training data as ordinary CNNs without a need for any annotations of object parts or textures for supervision. The interpretable CNN automatically assigns each filter in a high conv-layer with an object part during the learning process. We can apply our method to different types of CNNs with various structures. The explicit knowledge representation in an interpretable CNN can help people understand the logic inside a CNN, i.e. what patterns are memorized by the CNN for prediction. Experiments have shown that filters in an interpretable CNN are more semantically meaningful than those in a traditional CNN. The code is available at https://github.com/zqs1022/interpretableCNN.",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/ZhangXWXY18",
    "Title": "DeepVoting - A Robust and Explainable Deep Network for Semantic Part Detection Under Partial Occlusion.",
    "url": "http://openaccess.thecvf.com/content_cvpr_2018/html/Zhang_DeepVoting_A_Robust_CVPR_2018_paper.html",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "Zhishuai Zhang",
      "Cihang Xie",
      "Jianyu Wang",
      "Lingxi Xie",
      "Alan L. Yuille"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Heatmap",
      "Localization",
      "Prototypes"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "In this paper, we study the task of detecting semantic parts of an object, e.g., a wheel of a car, under partial occlusion. We propose that all models should be trained without seeing occlusions while being able to transfer the learned knowledge to deal with occlusions. This setting alleviates the difficulty in collecting an exponentially large dataset to cover occlusion patterns and is more essential. In this scenario, the proposal-based deep networks, like RCNN-series, often produce unsatisfactory results, because both the proposal extraction and classification stages may be confused by the irrelevant occluders. To address this, [25] proposed a voting mechanism that combines multiple local visual cues to detect semantic parts. The semantic parts can still be detected even though some visual cues are missing due to occlusions. However, this method is manually-designed, thus is hard to be optimized in an end-to-end manner. In this paper, we present DeepVoting, which incorporates the robustness shown by [25] into a deep network, so that the whole pipeline can be jointly optimized. Specifically, it adds two layers after the intermediate features of a deep network, e.g., the pool-4 layer of VGGNet. The first layer extracts the evidence of local visual cues, and the second layer performs a voting mechanism by utilizing the spatial relationship between visual cues and semantic parts. We also propose an improved version DeepVoting+ by learning visual cues from context outside objects. In experiments, DeepVoting achieves significantly better performance than several baseline methods, including Faster-RCNN, for semantic part detection under occlusion. In addition, DeepVoting enjoys explainability as the detection results can be diagnosed via looking up the voting cues.",
    "IsOld": true
  },
  {
    "Paper-ID": "icdm/JhaWXZ18",
    "Title": "Interpretable Word Embeddings for Medical Domain.",
    "url": "https://doi.org/10.1109/ICDM.2018.00135",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "ICDM"
    },
    "Authors": [
      "Kishlay Jha",
      "Yaqing Wang",
      "Guangxu Xun",
      "Aidong Zhang"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Representation learning"
    ],
    "Type of Explanation": [
      "Disentanglement"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Word embeddings are finding their increasing application in a variety of biomedical Natural Language Processing (bioNLP) tasks, ranging from drug discovery to automated disease diagnosis. While these word embeddings in their entirety have shown meaningful syntactic and semantic regularities, however, the meaning of individual dimensions remains elusive. This becomes problematic both in general and particularly in sensitive domains such as bio-medicine, wherein, the interpretability of results is crucial to its widespread adoption. To address this issue, in this study, we aim to improve the interpretability of pre-trained word embeddings generated from a text corpora, and in doing so provide a systematic approach to formalize the problem. More specifically, we exploit the rich categorical knowledge present in the biomedical domain, and propose to learn a transformation matrix that transforms the input embeddings to a new space where they are both interpretable and retain their original expressive features. Experiments conducted on the largest available biomedical corpus suggests that the model is capable of performing interpretability that resembles closely to the human-level intuition.",
    "IsOld": true
  },
  {
    "Paper-ID": "icdm/KarlssonRPG18",
    "Title": "Explainable Time Series Tweaking via Irreversible and Reversible Temporal Transformations.",
    "url": "https://doi.org/10.1109/ICDM.2018.00036",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "ICDM"
    },
    "Authors": [
      "Isak Karlsson",
      "Jonathan Rebane",
      "Panagiotis Papapetrou",
      "Aristides Gionis"
    ],
    "Type of Data": [
      "Time series"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Tree Ensemble"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Representation Synthesis"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Time series classification has received great attention over the past decade with a wide range of methods focusing on predictive performance by exploiting various types of temporal features. Nonetheless, little emphasis has been placed on interpretability and explainability. In this paper, we formulate the novel problem of explainable time series tweaking, where, given a time series and an opaque classifier that provides a particular classification decision for the time series, we want to find the minimum number of changes to be performed to the given time series so that the classifier changes its decision to another class. We show that the problem is NP-hard, and focus on two instantiations of the problem, which we refer to as reversible and irreversible time series tweaking. The classifier under investigation is the random shapelet forest classifier. Moreover, we propose two algorithmic solutions for the two problems along with simple optimizations, as well as a baseline solution using the nearest neighbor classifier. An extensive experimental evaluation on a variety of real datasets demonstrates the usefulness and effectiveness of our problem formulation and solutions.",
    "IsOld": true
  },
  {
    "Paper-ID": "icdm/WangCYWW018",
    "Title": "A Reinforcement Learning Framework for Explainable Recommendation.",
    "url": "https://doi.org/10.1109/ICDM.2018.00074",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "ICDM"
    },
    "Authors": [
      "Xiting Wang",
      "Yiru Chen",
      "Jie Yang",
      "Le Wu",
      "Zhengtao Wu",
      "Xing Xie"
    ],
    "Type of Data": [
      "User-item matrix"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Recommendation"
    ],
    "Type of Explanation": [
      "Text"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Explainable recommendation, which provides explanations about why an item is recommended, has attracted increasing attention due to its ability in helping users make better decisions and increasing users' trust in the system. Existing explainable recommendation methods either ignore the working mechanism of the recommendation model or are designed for a specific recommendation model. Moreover, it is difficult for existing methods to ensure the presentation quality of the explanations (e.g., consistency). To solve these problems, we design a reinforcement learning framework for explainable recommendation. Our framework can explain any recommendation model (model-agnostic) and can flexibly control the explanation quality based on the application scenario. To demonstrate the effectiveness of our framework, we show how it can be used for generating sentence-level explanations. Specifically, we instantiate the explanation generator in the framework with a personalized-attention-based neural network. Offline experiments demonstrate that our method can well explain both collaborative filtering methods and deep-learning-based models. Evaluation with human subjects shows that the explanations generated by our method are significantly more useful than the explanations generated by the baselines.",
    "IsOld": true
  },
  {
    "Paper-ID": "iclr/KindermansSAMEK18",
    "Title": "Learning how to explain neural networks - PatternNet and PatternAttribution.",
    "url": "https://openreview.net/forum?id=Hkn7CBaTW",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "ICLR"
    },
    "Authors": [
      "Pieter-Jan Kindermans",
      "Kristof T. Sch\u00fctt",
      "Maximilian Alber",
      "Klaus-Robert M\u00fcller",
      "Dumitru Erhan",
      "Been Kim",
      "Sven D\u00e4hne"
    ],
    "Type of Data": [
      "Images",
      "Any"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Heatmap"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "iclr/ShuXS18",
    "Title": "Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning.",
    "url": "https://openreview.net/forum?id=SJJQVZW0b",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "ICLR"
    },
    "Authors": [
      "Tianmin Shu",
      "Caiming Xiong",
      "Richard Socher"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Policy learning"
    ],
    "Type of Explanation": [
      "Graph",
      "Text"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "iclr/TrottXS18",
    "Title": "Interpretable Counting for Visual Question Answering.",
    "url": "https://openreview.net/forum?id=S1J2ZyZ0Z",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "ICLR"
    },
    "Authors": [
      "Alexander Trott",
      "Caiming Xiong",
      "Richard Socher"
    ],
    "Type of Data": [
      "Images",
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Question Answering"
    ],
    "Type of Explanation": [
      "Localization"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "icml/AdelGW18",
    "Title": "Discovering Interpretable Representations for Both Deep Generative and Discriminative Models.",
    "url": "http://proceedings.mlr.press/v80/adel18a.html",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "ICML"
    },
    "Authors": [
      "Tameem Adel",
      "Zoubin Ghahramani",
      "Adrian Weller"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification",
      "Representation learning"
    ],
    "Type of Explanation": [
      "Disentanglement"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "icml/AinsworthFLF18",
    "Title": "oi-VAE - Output Interpretable VAEs for Nonlinear Group Factor Analysis.",
    "url": "http://proceedings.mlr.press/v80/ainsworth18a.html",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "ICML"
    },
    "Authors": [
      "Samuel K. Ainsworth",
      "Nicholas J. Foti",
      "Adrian K. C. Lee",
      "Emily B. Fox"
    ],
    "Type of Data": [
      "Images",
      "Video",
      "Time series"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Generation"
    ],
    "Type of Explanation": [
      "Disentanglement"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "icml/KimWGCWVS18",
    "Title": "Interpretability Beyond Feature Attribution - Quantitative Testing with Concept Activation Vectors (TCAV).",
    "url": "http://proceedings.mlr.press/v80/kim18d.html",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "ICML"
    },
    "Authors": [
      "Been Kim",
      "Martin Wattenberg",
      "Justin Gilmer",
      "Carrie J. Cai",
      "James Wexler",
      "Fernanda B. Vi\u00e9gas",
      "Rory Sayres"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Heatmap",
      "Prototypes",
      "Representation Synthesis"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "icml/VermaMSKC18",
    "Title": "Programmatically Interpretable Reinforcement Learning.",
    "url": "http://proceedings.mlr.press/v80/verma18a.html",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "ICML"
    },
    "Authors": [
      "Abhinav Verma",
      "Vijayaraghavan Murali",
      "Rishabh Singh",
      "Pushmeet Kohli",
      "Swarat Chaudhuri"
    ],
    "Type of Data": [
      "Time series"
    ],
    "Type of Problem": [
      "Model Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Policy learning"
    ],
    "Type of Explanation": [
      "Decision Rules"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "ijcai/HuJCC18",
    "Title": "Interpretable Recommendation via Attraction Modeling - Learning Multilevel Attractiveness over Multimodal Movie Contents.",
    "url": "https://doi.org/10.24963/ijcai.2018/472",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "IJCAI"
    },
    "Authors": [
      "Liang Hu",
      "Songlei Jian",
      "Longbing Cao",
      "Qingkui Chen"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Text",
      "User-item matrix"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Recommendation"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "New contents like blogs and online videos are produced in every second in the new media age. We argue that attraction is one of the decisive factors for user selection of new contents. However, collaborative filtering cannot work without user feedback; and the existing content-based recommender systems are ineligible to capture and interpret the attractive points on new contents. Accordingly, we propose attraction modeling to learn and interpret user attractiveness. Specially, we build a multilevel attraction model (MLAM) over the content features -- the story (textual data) and cast members (categorical data) of movies. In particular, we design multilevel personal filters to calculate users' attractiveness on words, sentences and cast members at different levels. The experimental results show the superiority of MLAM over the state-of-the-art methods. In addition, a case study is provided to demonstrate the interpretability of MLAM by visualizing user attractiveness on a movie.",
    "IsOld": true
  },
  {
    "Paper-ID": "ijcai/LabreucheF18",
    "Title": "Explaining Multi-Criteria Decision Aiding Models with an Extended Shapley Value.",
    "url": "https://doi.org/10.24963/ijcai.2018/46",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "IJCAI"
    },
    "Authors": [
      "Christophe Labreuche",
      "Simon Fossier"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Bayesian or Hierarchical Network"
    ],
    "Type of Task": [
      "Other"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "The capability to explain the result of aggregation models to decision makers is key to reinforcing user trust. In practice, Multi-Criteria Decision Aiding models are often organized in a hierarchical way, based on a tree of criteria. We present an explanation approach usable with any hierarchical multi-criteria model, based on an influence index of each attribute on the decision. A set of desirable axioms are defined. We show that there is a unique index fulfilling these axioms. This new index is an extension of the Shapley value on trees. An efficient rewriting of this index, drastically reducing the computation time, is obtained. Finally, the use of the new index is illustrated on an example.",
    "IsOld": true
  },
  {
    "Paper-ID": "ijcai/LiuSH18",
    "Title": "Contextual Outlier Interpretation.",
    "url": "https://doi.org/10.24963/ijcai.2018/341",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "IJCAI"
    },
    "Authors": [
      "Ninghao Liu",
      "Donghwa Shin",
      "Xia Hu"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Support Vector Machine",
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Anomaly detection"
    ],
    "Type of Explanation": [
      "Localization",
      "Prototypes",
      "Other"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "While outlier detection has been intensively studied in many applications, interpretation is becoming increasingly important to help people trust and evaluate the developed detection models through providing intrinsic reasons why the given outliers are identified. It is a nontrivial task for interpreting the abnormality of outliers due to the distinct characteristics of different detection models, complicated structures of data in certain applications, and imbalanced distribution of outliers and normal instances. In addition, contexts where outliers locate, as well as the relation between outliers and the contexts, are usually overlooked in existing interpretation frameworks. To tackle the issues, in this paper, we propose a Contextual Outlier INterpretation (COIN) framework to explain the abnormality of outliers spotted by detectors. The interpretability of an outlier is achieved through three aspects, i.e., outlierness score, attributes that contribute to the abnormality, and contextual description of its neighborhoods. Experimental results on various types of datasets demonstrate the flexibility and effectiveness of the proposed framework.",
    "IsOld": true
  },
  {
    "Paper-ID": "ijcai/LuoAPWZYH18",
    "Title": "Beyond Polarity - Interpretable Financial Sentiment Analysis with Hierarchical Query-driven Attention.",
    "url": "https://doi.org/10.24963/ijcai.2018/590",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "IJCAI"
    },
    "Authors": [
      "Ling Luo",
      "Xiang Ao",
      "Feiyang Pan",
      "Jin Wang",
      "Tong Zhao",
      "Ningzi Yu",
      "Qing He"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Sentiment analysis has played a significant role in financial applications in recent years. The informational and emotive aspects of news texts may affect the prices, volatilities, volume of trades, and even potential risks of financial subjects. Previous studies in this field mainly focused on identifying polarity~(e.g. positive or negative). However, as financial decisions broadly require justifications, only plausible polarity cannot provide enough evidence during the decision making processes of humanity. Hence an explainable solution is in urgent demand. In this paper, we present an interpretable neural net framework for financial sentiment analysis. First, we design a hierarchical model to learn the representation of a document from multiple granularities. In addition, we propose a query-driven attention mechanism to satisfy the unique characteristics of financial documents. With the domain specified questions provided by the financial analysts, we can discover different spotlights for queries from different aspects. We conduct extensive experiments on a real-world dataset. The results demonstrate that our framework can learn better representation of the document and unearth meaningful clues on replying different users? preferences. It also outperforms the state-of-the-art methods on sentiment prediction of financial documents.\u00a0",
    "IsOld": true
  },
  {
    "Paper-ID": "ijcai/RagoCT18",
    "Title": "Argumentation-Based Recommendations - Fantastic Explanations and How to Find Them.",
    "url": "https://doi.org/10.24963/ijcai.2018/269",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "IJCAI"
    },
    "Authors": [
      "Antonio Rago",
      "Oana Cocarascu",
      "Francesca Toni"
    ],
    "Type of Data": [
      "User-item matrix"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Recommendation"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Graph"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "A significant problem of recommender systems is their inability to explain recommendations, resulting in turn in ineffective feedback from users and the inability to adapt to users\u2019 preferences. We propose a hybrid method for calculating predicted ratings, built upon an item/aspect-based graph with users\u2019 partially given ratings, that can be naturally used to provide explanations for recommendations, extracted from user-tailored Tripolar Argumentation Frameworks (TFs). We show that our method can be understood as a gradual semantics for TFs, exhibiting a desirable, albeit weak, property of balance. We also show experimentally that our method is competitive in generating correct predictions, compared with state-of-the-art methods, and illustrate how users can interact with the generated explanations to improve quality of recommendations.",
    "IsOld": true
  },
  {
    "Paper-ID": "ijcai/SatoSS018",
    "Title": "Interpretable Adversarial Perturbation in Input Embedding Space for Text.",
    "url": "https://doi.org/10.24963/ijcai.2018/601",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "IJCAI"
    },
    "Authors": [
      "Motoki Sato",
      "Jun Suzuki",
      "Hiroyuki Shindo",
      "Yuji Matsumoto"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Generation"
    ],
    "Type of Explanation": [
      "Prototypes",
      "Text"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Following great success in the image processing field, the idea of adversarial training has been applied to tasks in the natural language processing (NLP) field.\n\nOne promising approach directly applies adversarial training developed in the image processing field to the input word embedding space instead of the discrete input space of texts.\n\nHowever, this approach abandons such interpretability as generating adversarial texts to significantly improve the performance of NLP tasks.\n\nThis paper restores interpretability to such methods by restricting the directions of perturbations toward the existing words in the input embedding space.\n\nAs a result, we can straightforwardly reconstruct each input with perturbations to an actual text by considering the perturbations to be the replacement of words in the sentence while maintaining or even improving the task performance.",
    "IsOld": true
  },
  {
    "Paper-ID": "ijcai/ShihCD18",
    "Title": "A Symbolic Approach to Explaining Bayesian Network Classifiers.",
    "url": "https://doi.org/10.24963/ijcai.2018/708",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "IJCAI"
    },
    "Authors": [
      "Andy Shih",
      "Arthur Choi",
      "Adnan Darwiche"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Model Explanation",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Bayesian or Hierarchical Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Localization",
      "White-box model"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "We propose an approach for explaining Bayesian network classifiers, which is based on compiling such classifiers into decision functions that have a tractable and symbolic form. We introduce two types of explanations for why a classifier may have classified an instance positively or negatively and suggest algorithms for computing these explanations. The first type of explanation identifies a minimal set of the currently active features that is responsible for the current classification, while the second type of explanation identifies a minimal set of features whose current state (active or not) is sufficient for the classification. We consider in particular the compilation of Naive and Latent-Tree Bayesian network classifiers into Ordered Decision Diagrams (ODDs), providing a context for evaluating our proposal using case studies and experiments based on classifiers from the literature.",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/BaiZEV18",
    "Title": "Interpretable Representation Learning for Healthcare via Capturing Disease Progression through Time.",
    "url": "https://doi.org/10.1145/3219819.3219904",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Tian Bai",
      "Shanshan Zhang",
      "Brian L. Egleston",
      "Slobodan Vucetic"
    ],
    "Type of Data": [
      "Text",
      "Time series"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Various deep learning models have recently been applied to predictive modeling of Electronic Health Records (EHR). In medical claims data, which is a particular type of EHR data, each patient is represented as a sequence of temporally ordered irregularly sampled visits to health providers, where each visit is recorded as an unordered set of medical codes specifying patient's diagnosis and treatment provided during the visit. Based on the observation that different patient conditions have different temporal progression patterns, in this paper we propose a novel interpretable deep learning model, called Timeline. The main novelty of Timeline is that it has a mechanism that learns time decay factors for every medical code. This allows the Timeline to learn that chronic conditions have a longer lasting impact on future visits than acute conditions. Timeline also has an attention mechanism that improves vector embeddings of visits. By analyzing the attention weights and disease progression functions of Timeline, it is possible to interpret the predictions and understand how risks of future visits change over time. We evaluated Timeline on two large-scale real world data sets. The specific task was to predict what is the primary diagnosis category for the next hospital visit given previous visits. Our results show that Timeline has higher accuracy than the state of the art deep learning models based on RNN. In addition, we demonstrate that time decay factors and attentions learned by Timeline are in accord with the medical knowledge and that Timeline can provide a useful insight into its predictions.",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/ChuHHWP18",
    "Title": "Exact and Consistent Interpretation for Piecewise Linear Neural Networks - A Closed Form Solution.",
    "url": "https://doi.org/10.1145/3219819.3220063",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Lingyang Chu",
      "Xia Hu",
      "Juhua Hu",
      "Lanjun Wang",
      "Jian Pei"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Heatmap",
      "White-box model"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Strong intelligent machines powered by deep neural networks are increasingly deployed as black boxes to make decisions in risk-sensitive domains, such as finance and medical. To reduce potential risk and build trust with users, it is critical to interpret how such machines make their decisions. Existing works interpret a pre-trained neural network by analyzing hidden neurons, mimicking pre-trained models or approximating local predictions. However, these methods do not provide a guarantee on the exactness and consistency of their interpretations. In this paper, we propose an elegant closed form solution named $OpenBox$ to compute exact and consistent interpretations for the family of Piecewise Linear Neural Networks (PLNN). The major idea is to first transform a PLNN into a mathematically equivalent set of linear classifiers, then interpret each linear classifier by the features that dominate its prediction. We further apply $OpenBox$ to demonstrate the effectiveness of non-negative and sparse constraints on improving the interpretability of PLNNs. The extensive experiments on both synthetic and real world data sets clearly demonstrate the exactness and consistency of our interpretation.",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/DuLSH18",
    "Title": "Towards Explanation of DNN-based Prediction with Guided Feature Inversion.",
    "url": "https://doi.org/10.1145/3219819.3220099",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Mengnan Du",
      "Ninghao Liu",
      "Qingquan Song",
      "Xia Hu"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Heatmap"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "While deep neural networks (DNN) have become an effective computational tool, the prediction results are often criticized by the lack of interpretability, which is essential in many real-world applications such as health informatics. Existing attempts based on local interpretations aim to identify relevant features contributing the most to the prediction of DNN by monitoring the neighborhood of a given input. They usually simply ignore the intermediate layers of the DNN that might contain rich information for interpretation. To bridge the gap, in this paper, we propose to investigate a guided feature inversion framework for taking advantage of the deep architectures towards effective interpretation. The proposed framework not only determines the contribution of each feature in the input but also provides insights into the decision-making process of DNN models. By further interacting with the neuron of the target category at the output layer of the DNN, we enforce the interpretation result to be class-discriminative. We apply the proposed interpretation model to different CNN architectures to provide explanations for image data and conduct extensive experiments on ImageNet and PASCAL VOC07 datasets. The interpretation results demonstrate the effectiveness of our proposed framework in providing class-discriminative interpretation for DNN-based prediction.",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/Janakiraman18",
    "Title": "Explaining Aviation Safety Incidents Using Deep Temporal Multiple Instance Learning.",
    "url": "https://doi.org/10.1145/3219819.3219871",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Vijay Manikandan Janakiraman"
    ],
    "Type of Data": [
      "Time series"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Localization",
      "Text"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Although aviation accidents are rare, safety incidents occur more frequently and require a careful analysis to detect and mitigate risks in a timely manner. Analyzing safety incidents using operational data and producing event-based explanations is invaluable to airline companies as well as to governing organizations such as the Federal Aviation Administration (FAA) in the United States. However, this task is challenging because of the complexity involved in mining multi-dimensional heterogeneous time series data, the lack of time-step-wise annotation of events in a flight, and the lack of scalable tools to perform analysis over a large number of events. In this work, we propose a precursor mining algorithm that identifies events in the multidimensional time series that are correlated with the safety incident. Precursors are valuable to systems health and safety monitoring and in explaining and forecasting safety incidents. Current methods suffer from poor scalability to high dimensional time series data and are inefficient in capturing temporal behavior. We propose an approach by combining multiple-instance learning (MIL) and deep recurrent neural networks (DRNN) to take advantage of MIL's ability to learn using weakly supervised data and DRNN's ability to model temporal behavior. We describe the algorithm, the data, the intuition behind taking a MIL approach, and a comparative analysis of the proposed algorithm with baseline models. We also discuss the application to a real-world aviation safety problem using data from a commercial airline company and discuss the model's abilities and shortcomings, with some final remarks about possible deployment directions.",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/LiuHLH18",
    "Title": "On Interpretation of Network Embedding via Taxonomy Induction.",
    "url": "https://doi.org/10.1145/3219819.3220001",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Ninghao Liu",
      "Xiao Huang",
      "Jundong Li",
      "Xia Hu"
    ],
    "Type of Data": [
      "Graph data",
      "Other"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Graph",
      "Representation Visualization",
      "Feature Importance"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Network embedding has been increasingly used in many network analytics applications to generate low-dimensional vector representations, so that many off-the-shelf models can be applied to solve a wide variety of data mining tasks. However, similar to many other machine learning methods, network embedding results remain hard to be understood by users. Each dimension in the embedding space usually does not have any specific meaning, thus it is difficult to comprehend how the embedding instances are distributed in the reconstructed space. In addition, heterogeneous content information may be incorporated into network embedding, so it is challenging to specify which source of information is effective in generating the embedding results. In this paper, we investigate the interpretation of network embedding, aiming to understand how instances are distributed in embedding space, as well as explore the factors that lead to the embedding results. We resort to the post-hoc interpretation scheme, so that our approach can be applied to different types of embedding methods. Specifically, the interpretation of network embedding is presented in the form of a taxonomy. Effective objectives and corresponding algorithms are developed towards building the taxonomy. We also design several metrics to evaluate interpretation results. Experiments on real-world datasets from different domains demonstrate that, by comparing with the state-of-the-art alternatives, our approach produces effective and meaningful interpretation to embedding results.",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/LiuYH18",
    "Title": "Adversarial Detection with Model Interpretation.",
    "url": "https://doi.org/10.1145/3219819.3220027",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Ninghao Liu",
      "Hongxia Yang",
      "Xia Hu"
    ],
    "Type of Data": [
      "Text",
      "Any"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Tree Ensemble",
      "Support Vector Machine",
      "Any (for a specific task); model-agnostic",
      "Logistic Regression"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "White-box model"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Machine learning (ML) systems have been increasingly applied in web security applications such as spammer detection, malware detection and fraud detection. These applications have an intrinsic adversarial nature where intelligent attackers can adaptively change their behaviors to avoid being detected by the deployed detectors. Existing efforts against adversaries are usually limited by the type of applied ML models or the specific applications such as image classification. Additionally, the working mechanisms of ML models usually cannot be well understood by users, which in turn impede them from understanding the vulnerabilities of models nor improving their robustness. To bridge the gap, in this paper, we propose to investigate whether model interpretation could potentially help adversarial detection. Specifically, we develop a novel adversary-resistant detection framework by utilizing the interpretation of ML models. The interpretation process explains the mechanism of how the target ML model makes prediction for a given instance, thus providing more insights for crafting adversarial samples. The robustness of detectors is then improved through adversarial training with the adversarial samples. A data-driven method is also developed to empirically estimate costs of adversaries in feature manipulation. Our approach is model-agnostic and can be applied to various types of classification models. Our experimental results on two real-world datasets demonstrate the effectiveness of interpretation-based attacks and how estimated feature manipulation cost would affect the behavior of adversaries.",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/WangWLW18",
    "Title": "Multilevel Wavelet Decomposition Network for Interpretable Time Series Analysis.",
    "url": "https://doi.org/10.1145/3219819.3220060",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Jingyuan Wang",
      "Ze Wang",
      "Jianfeng Li",
      "Junjie Wu"
    ],
    "Type of Data": [
      "Time series"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification",
      "Regression"
    ],
    "Type of Explanation": [
      "Heatmap"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Recent years have witnessed the unprecedented rising of time series from almost all kindes of academic and industrial fields. Various types of deep neural network models have been introduced to time series analysis, but the important frequency information is yet lack of effective modeling. In light of this, in this paper we propose a wavelet-based neural network structure called multilevel Wavelet Decomposition Network (mWDN) for building frequency-aware deep learning models for time series analysis. mWDN preserves the advantage of multilevel discrete wavelet decomposition in frequency learning while enables the fine-tuning of all parameters under a deep neural network framework. Based on mWDN, we further propose two deep learning models called Residual Classification Flow (RCF) and multi-frequecy Long Short-Term Memory (mLSTM) for time series classification and forecasting, respectively. The two models take all or partial mWDN decomposed sub-series in different frequencies as input, and resort to the back propagation algorithm to learn all the parameters globally, which enables seamless embedding of wavelet-based frequency analysis into deep learning frameworks. Extensive experiments on 40 UCR datasets and a real-world user volume dataset demonstrate the excellent performance of our time series models based on mWDN. In particular, we propose an importance analysis method to mWDN based models, which successfully identifies those time-series elements and mWDN layers that are crucially important to time series analysis. This indeed indicates the interpretability advantage of mWDN, and can be viewed as an indepth exploration to interpretable deep learning.",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/YangSJ018",
    "Title": "I Know You'll Be Back - Interpretable New User Clustering and Churn Prediction on a Mobile Social Application.",
    "url": "https://doi.org/10.1145/3219819.3219821",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Carl Yang",
      "Xiaolin Shi",
      "Luo Jie",
      "Jiawei Han"
    ],
    "Type of Data": [
      "Time series",
      "Graph data"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Clustering",
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Representation Visualization"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "As online platforms are striving to get more users, a critical challenge is user churn, which is especially concerning for new users. In this paper, by taking the anonymous large-scale real-world data from Snapchat as an example, we develop ClusChurn , a systematic two-step framework for interpretable new user clustering and churn prediction, based on the intuition that proper user clustering can help understand and predict user churn. Therefore, ClusChurn firstly groups new users into interpretable typical clusters, based on their activities on the platform and ego-network structures. Then we design a novel deep learning pipeline based on LSTM and attention to accurately predict user churn with very limited initial behavior data, by leveraging the correlations among users' multi- dimensional activities and the underlying user types. ClusChurn is also able to predict user types, which enables rapid reactions to different types of user churn. Extensive data analysis and experiments show that ClusChurn provides valuable insight into user behaviors, and achieves state-of-the-art churn prediction performance. The whole framework is deployed as a data analysis pipeline, delivering real-time data analysis and prediction results to multiple relevant teams for business intelligence uses. It is also general enough to be readily adopted by any online systems with user behavior data.",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/ZangC018",
    "Title": "Learning and Interpreting Complex Distributions in Empirical Data.",
    "url": "https://doi.org/10.1145/3219819.3220073",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Chengxi Zang",
      "Peng Cui",
      "Wenwu Zhu"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Time series"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Regression"
    ],
    "Type of Explanation": [
      "White-box model"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "To fit empirical data distributions and then interpret them in a generative way is a common research paradigm to understand the structure and dynamics underlying the data in various disciplines. However, previous works mainly attempt to fit or interpret empirical data distributions in a case-by-case way. Faced with complex data distributions in the real world, can we fit and interpret them by a unified but parsimonious parametric model? In this paper, we view the complex empirical data as being generated by a dynamic system which takes uniform randomness as input. By modeling the generative dynamics of data, we showcase a four-parameter dynamic model together with inference and simulation algorithms, which is able to fit and generate a family of distributions, ranging from Gaussian, Exponential, Power Law, Stretched Exponential (Weibull), to their complex variants with multi-scale complexities. Rather than a black box, our model can be interpreted by a unified differential equation, which captures the underlying generative dynamics. More powerful models can be constructed by our framework in a principled way. We validate our model by various synthetic datasets. We then apply our model to $16$ real-world datasets from different disciplines. We show the systematic biases of fitting these datasets by the most widely used methods and show the superiority of our model. In short, our model potentially provides a framework to fit complex distributions in empirical data, and more importantly, to understand their generative mechanisms.",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/CamburuRLB18",
    "Title": "e-SNLI - Natural Language Inference with Natural Language Explanations.",
    "url": "https://proceedings.neurips.cc/paper/2018/hash/4c7a167bb329bd92580a99ce422d6fa6-Abstract.html",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Oana-Maria Camburu",
      "Tim Rockt\u00e4schel",
      "Thomas Lukasiewicz",
      "Phil Blunsom"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Text"
    ],
    "Method used to explain": [
      "Supervised explanation training"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/GuoHTXL18",
    "Title": "Explaining Deep Learning Models - A Bayesian Non-parametric Approach.",
    "url": "https://proceedings.neurips.cc/paper/2018/hash/4b4edc2630fe75800ddc29a7b4070add-Abstract.html",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Wenbo Guo",
      "Sui Huang",
      "Yunzhe Tao",
      "Xinyu Xing",
      "Lin Lin"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Heatmap",
      "Localization"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/GuptaBCC18",
    "Title": "Diminishing Returns Shape Constraints for Interpretability and Regularization.",
    "url": "https://proceedings.neurips.cc/paper/2018/hash/caa202034f268232c26fac9435f54e15-Abstract.html",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Maya R. Gupta",
      "Dara Bahri",
      "Andrew Cotter",
      "Kevin Robert Canini"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Other"
    ],
    "Type of Task": [
      "Regression"
    ],
    "Type of Explanation": [
      "Feature plot"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/HeoLKLKYH18",
    "Title": "Uncertainty-Aware Attention for Reliable Interpretation and Prediction.",
    "url": "https://proceedings.neurips.cc/paper/2018/hash/285e19f20beded7d215102b49d5c09a0-Abstract.html",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Jay Heo",
      "Haebeom Lee",
      "Saehoon Kim",
      "Juho Lee",
      "Kwang Joon Kim",
      "Eunho Yang",
      "Sung Ju Hwang"
    ],
    "Type of Data": [
      "Time series"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/Norcliffe-Brown18",
    "Title": "Learning Conditioned Graph Structures for Interpretable Visual Question Answering.",
    "url": "https://proceedings.neurips.cc/paper/2018/hash/4aeae10ea1c6433c926cdfa558d31134-Abstract.html",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Will Norcliffe-Brown",
      "Stathis Vafeias",
      "Sarah Parisot"
    ],
    "Type of Data": [
      "Images",
      "Text"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification",
      "Question Answering"
    ],
    "Type of Explanation": [
      "Graph",
      "Localization"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/PlumbMT18",
    "Title": "Model Agnostic Supervised Local Explanations.",
    "url": "https://proceedings.neurips.cc/paper/2018/hash/b495ce63ede0f4efc9eec62cb947c162-Abstract.html",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Gregory Plumb",
      "Denali Molitor",
      "Ameet S. Talwalkar"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Any"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation",
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "Tree Ensemble",
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "White-box model"
    ],
    "Method used to explain": [
      "Post-hoc explanation method",
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/TsangLPML18",
    "Title": "Neural Interaction Transparency (NIT) - Disentangling Learned Interactions for Improved Interpretability.",
    "url": "https://proceedings.neurips.cc/paper/2018/hash/74378afe5e8b20910cf1f939e57f0480-Abstract.html",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Michael Tsang",
      "Hanpeng Liu",
      "Sanjay Purushotham",
      "Pavankumar Murali",
      "Yan Liu"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Images",
      "Any"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Tree Ensemble",
      "Logistic Regression",
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Classification",
      "Regression"
    ],
    "Type of Explanation": [
      "Disentanglement"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/Wang18",
    "Title": "Multi-value Rule Sets for Interpretable Classification with Feature-Efficient Representations.",
    "url": "https://proceedings.neurips.cc/paper/2018/hash/32bbf7b2bc4ed14eb1e9c2580056a989-Abstract.html",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Tong Wang"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "Bayesian or Hierarchical Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Decision Rules"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/YehKYR18",
    "Title": "Representer Point Selection for Explaining Deep Neural Networks.",
    "url": "https://proceedings.neurips.cc/paper/2018/hash/8a7129b8f3edd95b7d969dfc2c8e9d9d-Abstract.html",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Chih-Kuan Yeh",
      "Joon Sik Kim",
      "Ian En-Hsu Yen",
      "Pradeep Ravikumar"
    ],
    "Type of Data": [
      "Any",
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Prototypes"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/ZhangSS18",
    "Title": "Interpreting Neural Network Judgments via Minimal, Stable, and Symbolic Corrections.",
    "url": "https://proceedings.neurips.cc/paper/2018/hash/300891a62162b960cf02ce3827bb363c-Abstract.html",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Xin Zhang",
      "Armando Solar-Lezama",
      "Rishabh Singh"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Prototypes"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "sigir/WangWJY18",
    "Title": "Explainable Recommendation via Multi-Task Learning in Opinionated Text Data.",
    "url": "https://doi.org/10.1145/3209978.3210010",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "SIGIR"
    },
    "Authors": [
      "Nan Wang",
      "Hongning Wang",
      "Yiling Jia",
      "Yue Yin"
    ],
    "Type of Data": [
      "User-item matrix",
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Recommendation"
    ],
    "Type of Explanation": [
      "Localization",
      "Representation Visualization"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Explaining automatically generated recommendations allows users to make more informed and accurate decisions about which results to utilize, and therefore improves their satisfaction. In this work, we develop a multi-task learning solution for explainable recommendation. Two companion learning tasks of user preference modeling for recommendation and opinionated content modeling for explanation are integrated via a joint tensor factorization. As a result, the algorithm predicts not only a user's preference over a list of items, i.e., recommendation, but also how the user would appreciate a particular item at the feature level, i.e., opinionated textual explanation. Extensive experiments on two large collections of Amazon and Yelp reviews confirmed the effectiveness of our solution in both recommendation and explanation tasks, compared with several existing recommendation algorithms. And our extensive user study clearly demonstrates the practical value of the explainable recommendations generated by our algorithm.",
    "IsOld": true
  },
  {
    "Paper-ID": "www/ChenZLM18",
    "Title": "Neural Attentional Rating Regression with Review-level Explanations.",
    "url": "https://doi.org/10.1145/3178876.3186070",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "WWW"
    },
    "Authors": [
      "Chong Chen",
      "Min Zhang",
      "Yiqun Liu",
      "Shaoping Ma"
    ],
    "Type of Data": [
      "Text",
      "User-item matrix"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification",
      "Regression"
    ],
    "Type of Explanation": [
      "Localization"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Reviews information is dominant for users to make online purchasing decisions in e-commerces. However, the usefulness of reviews is varied. We argue that less-useful reviews hurt model's performance, and are also less meaningful for user's reference. While some existing models utilize reviews for improving the performance of recommender systems, few of them consider the usefulness of reviews for recommendation quality. In this paper, we introduce a novel attention mechanism to explore the usefulness of reviews, and propose a Neural Attentional Regression model with Review-level Explanations (NARRE) for recommendation. Specifically, NARRE can not only predict precise ratings, but also learn the usefulness of each review simultaneously. Therefore, the highly-useful reviews are obtained which provide review-level explanations to help users make better and faster decisions. Extensive experiments on benchmark datasets of Amazon and Yelp on different domains show that the proposed NARRE model consistently outperforms the state-of-the-art recommendation approaches, including PMF, NMF, SVD++, HFT, and DeepCoNN in terms of rating prediction, by the proposed attention model that takes review usefulness into consideration. Furthermore, the selected reviews are shown to be effective when taking existing review-usefulness ratings in the system as ground truth. Besides, crowd-sourcing based evaluations reveal that in most cases, NARRE achieves equal or even better performances than system's usefulness rating method in selecting reviews. And it is flexible to offer great help on the dominant cases in real e-commerce scenarios when the ratings on review-usefulness are not available in the system.",
    "IsOld": true
  },
  {
    "Paper-ID": "www/Wang0FNC18",
    "Title": "TEM - Tree-enhanced Embedding Model for Explainable Recommendation.",
    "url": "https://doi.org/10.1145/3178876.3186066",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "WWW"
    },
    "Authors": [
      "Xiang Wang",
      "Xiangnan He",
      "Fuli Feng",
      "Liqiang Nie",
      "Tat-Seng Chua"
    ],
    "Type of Data": [
      "User-item matrix"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Recommendation"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Post-hoc explanation method",
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "While collaborative filtering is the dominant technique in personalized recommendation, it models user-item interactions only and cannot provide concrete reasons for a recommendation. Meanwhile, the rich side information affiliated with user-item interactions (e.g., user demographics and item attributes), which provide valuable evidence that why a recommendation is suitable for a user, has not been fully explored in providing explanations. On the technical side, embedding-based methods, such as Wide&Deep and neural factorization machines, provide state-of-the-art recommendation performance. However, they work like a black-box, for which the reasons underlying a prediction cannot be explicitly presented. On the other hand, tree-based methods like decision trees predict by inferring decision rules from data. While being explainable, they cannot generalize to unseen feature interactions thus fail in collaborative filtering applications. In this work, we propose a novel solution named Tree-enhanced Embedding Method that combines the strengths of embedding-based and tree-based models. We first employ a tree-based model to learn explicit decision rules (aka. cross features) from the rich side information. We next design an embedding model that can incorporate explicit cross features and generalize to unseen cross features on user ID and item ID. At the core of our embedding method is an easy-to-interpret attention network, making the recommendation process fully transparent and explainable. We conduct experiments on two datasets of tourist attraction and restaurant recommendation, demonstrating the superior performance and explainability of our solution.",
    "IsOld": true
  },
  {
    "Paper-ID": "www/WuWYLZ18",
    "Title": "Sharing Deep Neural Network Models with Interpretation.",
    "url": "https://doi.org/10.1145/3178876.3185995",
    "Year": "2018",
    "Venue": {
      "isOld": true,
      "value": "WWW"
    },
    "Authors": [
      "Huijun Wu",
      "Chen Wang",
      "Jie Yin",
      "Kai Lu",
      "Liming Zhu"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Graph",
      "Prototypes"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Despite outperforming humans in many tasks, deep neural network models are also criticized for the lack of transparency and interpretability in decision making. The opaqueness results in uncertainty and low confidence when deploying such a model in model sharing scenarios, where the model is developed by a third party. For a supervised machine learning model, sharing training process including training data is a way to gain trust and to better understand model predictions. However, it is not always possible to share all training data due to privacy and policy constraints. In this paper, we propose a method to disclose a small set of training data that is just sufficient for users to get the insight into a complicated model. The method constructs a boundary tree using selected training data and the tree is able to approximate the complicated deep neural network models with high fidelity. We show that data point pairs in the tree give users significantly better understanding of the model decision boundaries and paves the way for trustworthy model sharing.",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/ZhangCWZ17",
    "Title": "Growing Interpretable Part Graphs on ConvNets via Multi-Shot Learning.",
    "url": "http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14909",
    "Year": "2017",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Quanshi Zhang",
      "Ruiming Cao",
      "Ying Nian Wu",
      "Song-Chun Zhu"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Support Vector Machine"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Graph",
      "Localization"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "acl/XieMDH17",
    "Title": "An Interpretable Knowledge Transfer Model for Knowledge Base Completion.",
    "url": "https://doi.org/10.18653/v1/P17-1088",
    "Year": "2017",
    "Venue": {
      "isOld": true,
      "value": "ACL"
    },
    "Authors": [
      "Qizhe Xie",
      "Xuezhe Ma",
      "Zihang Dai",
      "Eduard H. Hovy"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Knowledge bases are important resources for a variety of natural language processing tasks but suffer from incompleteness. We propose a novel embedding model, ITransF, to perform knowledge base completion. Equipped with a sparse attention mechanism, ITransF discovers hidden concepts of relations and transfer statistical strength through the sharing of concepts. Moreover, the learned associations between relations and concepts, which are represented by sparse attention vectors, can be interpreted easily. We evaluate ITransF on two benchmark datasets\u2014WN18 and FB15k for knowledge base completion and obtains improvements on both the mean rank and Hits@10 metrics, over all baselines that do not use additional information.",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/BauZKO017",
    "Title": "Network Dissection - Quantifying Interpretability of Deep Visual Representations.",
    "url": "https://doi.org/10.1109/CVPR.2017.354",
    "Year": "2017",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "David Bau",
      "Bolei Zhou",
      "Aditya Khosla",
      "Aude Oliva",
      "Antonio Torralba"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Disentanglement",
      "Localization",
      "Text"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a data set of concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are labeled across a broad range of visual concepts including objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability is an axis-independent property of the representation space, then we apply the method to compare the latent representations of various networks when trained to solve different classification problems. We further analyze the effect of training iterations, compare networks trained with different initializations, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/DongSZZ17",
    "Title": "Improving Interpretability of Deep Neural Networks with Semantic Information.",
    "url": "https://doi.org/10.1109/CVPR.2017.110",
    "Year": "2017",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "Yinpeng Dong",
      "Hang Su",
      "Jun Zhu",
      "Bo Zhang"
    ],
    "Type of Data": [
      "Video"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Text"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Interpretability of deep neural networks (DNNs) is essential since it enables users to understand the overall strengths and weaknesses of the models, conveys an understanding of how the models will behave in the future, and how to diagnose and correct potential problems. However, it is challenging to reason about what a DNN actually does due to its opaque or black-box nature. To address this issue, we propose a novel technique to improve the interpretability of DNNs by leveraging the rich semantic information embedded in human descriptions. By concentrating on the video captioning task, we first extract a set of semantically meaningful topics from the human descriptions that cover a wide range of visual concepts, and integrate them into the model with an interpretive loss. We then propose a prediction difference maximization algorithm to interpret the learned features of each neuron. Experimental results demonstrate its effectiveness in video captioning using the interpretable features, which can also be transferred to video action recognition. By clearly understanding the learned features, users can easily revise false predictions via a human-in-the-loop procedure.",
    "IsOld": true
  },
  {
    "Paper-ID": "cvpr/LiangLSFYX17",
    "Title": "Interpretable Structure-Evolving LSTM.",
    "url": "https://doi.org/10.1109/CVPR.2017.234",
    "Year": "2017",
    "Venue": {
      "isOld": true,
      "value": "CVPR"
    },
    "Authors": [
      "Xiaodan Liang",
      "Liang Lin",
      "Xiaohui Shen",
      "Jiashi Feng",
      "Shuicheng Yan",
      "Eric P. Xing"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Localization"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "This paper develops a general framework for learning interpretable data representation via Long Short-Term Memory (LSTM) recurrent neural networks over hierarchal graph structures. Instead of learning LSTM models over the pre-fixed structures, we propose to further learn the intermediate interpretable multi-level graph structures in a progressive and stochastic way from data during the LSTM network optimization. We thus call this model the structure-evolving LSTM. In particular, starting with an initial element-level graph representation where each node is a small data element, the structure-evolving LSTM gradually evolves the multi-level graph representations by stochastically merging the graph nodes with high compatibilities along the stacked LSTM layers. In each LSTM layer, we estimate the compatibility of two connected nodes from their corresponding LSTM gate outputs, which is used to generate a merging probability. The candidate graph structures are accordingly generated where the nodes are grouped into cliques with their merging probabilities. We then produce the new graph structure with a Metropolis-Hasting algorithm, which alleviates the risk of getting stuck in local optimums by stochastic sampling with an acceptance probability. Once a graph structure is accepted, a higher-level graph is then constructed by taking the partitioned cliques as its nodes. During the evolving process, representation becomes more abstracted in higher-levels where redundant information is filtered out, allowing more efficient propagation of long-range data dependencies. We evaluate the effectiveness of structure-evolving LSTM in the application of semantic object parsing and demonstrate its advantage over state-of-the-art LSTM models on standard benchmarks.",
    "IsOld": true
  },
  {
    "Paper-ID": "iccv/KimC17",
    "Title": "Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention.",
    "url": "https://doi.org/10.1109/ICCV.2017.320",
    "Year": "2017",
    "Venue": {
      "isOld": true,
      "value": "ICCV"
    },
    "Authors": [
      "Jinkyu Kim",
      "John F. Canny"
    ],
    "Type of Data": [
      "Images",
      "Video"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Regression"
    ],
    "Type of Explanation": [
      "Heatmap"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Deep neural perception and control networks are likely to be a key component of self-driving vehicles. These models need to be explainable - they should provide easy-tointerpret rationales for their behavior - so that passengers, insurance companies, law enforcement, developers etc., can understand what triggered a particular behavior. Here we explore the use of visual explanations. These explanations take the form of real-time highlighted regions of an image that causally influence the network\u2019s output (steering control). Our approach is two-stage. In the first stage, we use a visual attention model to train a convolution network endto- end from images to steering angle. The attention model highlights image regions that potentially influence the network\u2019s output. Some of these are true influences, but some are spurious. We then apply a causal filtering step to determine which input regions actually influence the output. This produces more succinct visual explanations and more accurately exposes the network\u2019s behavior. We demonstrate the effectiveness of our model on three datasets totaling 16 hours of driving. We first show that training with attention does not degrade the performance of the end-to-end network. Then we show that the network causally cues on a variety of features that are used by humans while driving.",
    "IsOld": true
  },
  {
    "Paper-ID": "iccv/SelvarajuCDVPB17",
    "Title": "Grad-CAM - Visual Explanations from Deep Networks via Gradient-Based Localization.",
    "url": "https://doi.org/10.1109/ICCV.2017.74",
    "Year": "2017",
    "Venue": {
      "isOld": true,
      "value": "ICCV"
    },
    "Authors": [
      "Ramprasaath R. Selvaraju",
      "Michael Cogswell",
      "Abhishek Das",
      "Ramakrishna Vedantam",
      "Devi Parikh",
      "Dhruv Batra"
    ],
    "Type of Data": [
      "Images",
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Heatmap",
      "Localization"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "We propose a technique for producing \u2018visual explanations\u2019 for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach \u2013 Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for \u2018dog\u2019 or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad- CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. visual question answering) or reinforcement learning, without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a \u2018stronger\u2019 deep network from a \u2018weaker\u2019 one even when both make identical predictions. Our code is available at https: //github.com/ramprs/grad-cam/ along with a demo on CloudCV [2] and video at youtu.be/COjUB9Izk6E.",
    "IsOld": true
  },
  {
    "Paper-ID": "iccv/WorrallGTB17",
    "Title": "Interpretable Transformations with Encoder-Decoder Networks.",
    "url": "https://doi.org/10.1109/ICCV.2017.611",
    "Year": "2017",
    "Venue": {
      "isOld": true,
      "value": "ICCV"
    },
    "Authors": [
      "Daniel E. Worrall",
      "Stephan J. Garbin",
      "Daniyar Turmukhambetov",
      "Gabriel J. Brostow"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification",
      "Generation",
      "Representation learning"
    ],
    "Type of Explanation": [
      "Disentanglement"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Deep feature spaces have the capacity to encode complex transformations of their input data. However, understanding the relative feature-space relationship between two transformed encoded images is difficult. For instance, what is the relative feature space relationship between two rotated images? What is decoded when we interpolate in feature space? Ideally, we want to disentangle confounding factors, such as pose, appearance, and illumination, from object identity. Disentangling these is difficult because they interact in very nonlinear ways. We propose a simple method to construct a deep feature space, with explicitly disentangled representations of several known transformations. A person or algorithm can then manipulate the disentangled representation, for example, to re-render an image with explicit control over parameterized degrees of freedom. The feature space is constructed using a transforming encoder-decoder network with a custom feature transform layer, acting on the hidden representations. We demonstrate the advantages of explicit disentangling on a variety of datasets and transformations, and as an aid for traditional tasks, such as classification.",
    "IsOld": true
  },
  {
    "Paper-ID": "icdm/OyamadaN17",
    "Title": "Relational Mixture of Experts - Explainable Demographics Prediction with Behavioral Data.",
    "url": "https://doi.org/10.1109/ICDM.2017.45",
    "Year": "2017",
    "Venue": {
      "isOld": true,
      "value": "ICDM"
    },
    "Authors": [
      "Masafumi Oyamada",
      "Shinji Nakadai"
    ],
    "Type of Data": [
      "User-item matrix"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "Support Vector Machine",
      "Bayesian or Hierarchical Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Decision Rules",
      "Heatmap"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Given a collection of basic customer demographics (e.g., age and gender) andtheir behavioral data (e.g., item purchase histories), how can we predictsensitive demographics (e.g., income and occupation) that not every customermakes available?This demographics prediction problem is modeled as a classification task inwhich a customer's sensitive demographic y is predicted from his featurevector x. So far, two lines of work have tried to produce a\"good\" feature vector x from the customer's behavioraldata: (1) application-specific feature engineering using behavioral data and (2) representation learning (such as singular value decomposition or neuralembedding) on behavioral data. Although these approaches successfullyimprove the predictive performance, (1) designing a good feature requiresdomain experts to make a great effort and (2) features obtained fromrepresentation learning are hard to interpret. To overcome these problems, we present a Relational Infinite SupportVector Machine (R-iSVM), a mixture-of-experts model that can leveragebehavioral data. Instead of augmenting the feature vectors of customers, R-iSVM uses behavioral data to find out behaviorally similar customerclusters and constructs a local prediction model at each customer cluster. In doing so, R-iSVM successfully improves the predictive performance withoutrequiring application-specific feature designing and hard-to-interpretrepresentations. Experimental results on three real-world datasets demonstrate the predictiveperformance and interpretability of R-iSVM. Furthermore, R-iSVM can co-existwith previous demographics prediction methods to further improve theirpredictive performance.",
    "IsOld": true
  },
  {
    "Paper-ID": "iclr/YuV17",
    "Title": "Towards Deep Interpretability (MUS-ROVER II) - Learning Hierarchical Representations of Tonal Music.",
    "url": "https://openreview.net/forum?id=ryhqQFKgl",
    "Year": "2017",
    "Venue": {
      "isOld": true,
      "value": "ICLR"
    },
    "Authors": [
      "Haizi Yu",
      "Lav R. Varshney"
    ],
    "Type of Data": [
      "Other"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "Bayesian or Hierarchical Network"
    ],
    "Type of Task": [
      "Representation learning"
    ],
    "Type of Explanation": [
      "Decision Rules",
      "Graph"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "icml/DempseyMSDGMR17",
    "Title": "iSurvive - An Interpretable, Event-time Prediction Model for mHealth.",
    "url": "http://proceedings.mlr.press/v70/dempsey17a.html",
    "Year": "2017",
    "Venue": {
      "isOld": true,
      "value": "ICML"
    },
    "Authors": [
      "Walter H. Dempsey",
      "Alexander Moreno",
      "Christy K. Scott",
      "Michael L. Dennis",
      "David H. Gustafson",
      "Susan A. Murphy",
      "James M. Rehg"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Time series"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Regression"
    ],
    "Type of Explanation": [
      "White-box model"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "icml/FoersterGSCS17",
    "Title": "Input Switched Affine Networks - An RNN Architecture Designed for Interpretability.",
    "url": "http://proceedings.mlr.press/v70/foerster17a.html",
    "Year": "2017",
    "Venue": {
      "isOld": true,
      "value": "ICML"
    },
    "Authors": [
      "Jakob N. Foerster",
      "Justin Gilmer",
      "Jascha Sohl-Dickstein",
      "Jan Chorowski",
      "David Sussillo"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation",
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Generation"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Feature plot"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "ijcai/RossHD17",
    "Title": "Right for the Right Reasons - Training Differentiable Models by Constraining their Explanations.",
    "url": "https://doi.org/10.24963/ijcai.2017/371",
    "Year": "2017",
    "Venue": {
      "isOld": true,
      "value": "IJCAI"
    },
    "Authors": [
      "Andrew Slavin Ross",
      "Michael C. Hughes",
      "Finale Doshi-Velez"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Images",
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Heatmap"
    ],
    "Method used to explain": [
      "Supervised explanation training"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Neural networks are among the most accurate supervised learning methods in use today, but their opacity makes them difficult to trust in critical applications, especially when conditions in training differ from those in test. Recent work on explanations for black-box models has produced tools (e.g. LIME) to show the implicit rules behind predictions, which can help us identify when models are right for the wrong reasons. However, these methods do not scale to explaining entire datasets and cannot correct the problems they reveal. We introduce a method for efficiently explaining and regularizing differentiable models by examining and selectively penalizing their input gradients, which provide a normal to the decision boundary. We apply these penalties both based on expert annotation and in an unsupervised fashion that encourages diverse models with qualitatively different decision boundaries for the same classification problem. On multiple datasets, we show our approach generates faithful explanations and models that generalize much better when conditions differ between training and test.",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/TolomeiSHL17",
    "Title": "Interpretable Predictions of Tree-based Ensembles via Actionable Feature Tweaking.",
    "url": "https://doi.org/10.1145/3097983.3098039",
    "Year": "2017",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Gabriele Tolomei",
      "Fabrizio Silvestri",
      "Andrew Haines",
      "Mounia Lalmas"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "Tree Ensemble"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Other"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Machine-learned models are often described as \"black boxes\". In many real-world applications however, models may have to sacrifice predictive power in favour of human-interpretability. When this is the case, feature engineering becomes a crucial task, which requires significant and time-consuming human effort. Whilst some features are inherently static, representing properties that cannot be influenced (e.g., the age of an individual), others capture characteristics that could be adjusted (e.g., the daily amount of carbohydrates taken). Nonetheless, once a model is learned from the data, each prediction it makes on new instances is irreversible - assuming every instance to be a static point located in the chosen feature space. There are many circumstances however where it is important to understand (i) why a model outputs a certain prediction on a given instance, (ii) which adjustable features of that instance should be modified, and finally (iii) how to alter such a prediction when the mutated instance is input back to the model. In this paper, we present a technique that exploits the internals of a tree-based ensemble classifier to offer recommendations for transforming true negative instances into positively predicted ones. We demonstrate the validity of our approach using an online advertising application. First, we design a Random Forest classifier that effectively separates between two types of ads: low (negative) and high (positive) quality ads (instances). Then, we introduce an algorithm that provides recommendations that aim to transform a low quality ad (negative instance) into a high quality one (positive instance). Finally, we evaluate our approach on a subset of the active inventory of a large ad network, Yahoo Gemini.",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/ElenbergDFK17",
    "Title": "Streaming Weak Submodularity - Interpreting Neural Networks on the Fly.",
    "url": "https://proceedings.neurips.cc/paper/2017/hash/c182f930a06317057d31c73bb2fedd4f-Abstract.html",
    "Year": "2017",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Ethan R. Elenberg",
      "Alexandros G. Dimakis",
      "Moran Feldman",
      "Amin Karbasi"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Localization"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/HsuZG17",
    "Title": "Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data.",
    "url": "https://proceedings.neurips.cc/paper/2017/hash/0a0a0c8aaa00ade50f74a3f0ca981ed7-Abstract.html",
    "Year": "2017",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Wei-Ning Hsu",
      "Yu Zhang",
      "James R. Glass"
    ],
    "Type of Data": [
      "Time series"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Representation learning"
    ],
    "Type of Explanation": [
      "Disentanglement",
      "Representation Synthesis"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/LiSE17",
    "Title": "InfoGAIL - Interpretable Imitation Learning from Visual Demonstrations.",
    "url": "https://proceedings.neurips.cc/paper/2017/hash/2cd4e8a2ce081c3d7c32c3cde4312ef7-Abstract.html",
    "Year": "2017",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Yunzhu Li",
      "Jiaming Song",
      "Stefano Ermon"
    ],
    "Type of Data": [
      "Other"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Policy learning"
    ],
    "Type of Explanation": [
      "Disentanglement"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/LundbergL17",
    "Title": "A Unified Approach to Interpreting Model Predictions.",
    "url": "https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html",
    "Year": "2017",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Scott M. Lundberg",
      "Su-In Lee"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Images",
      "Any"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Classification",
      "Regression"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Heatmap"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/RaghuGYS17",
    "Title": "SVCCA - Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability.",
    "url": "https://proceedings.neurips.cc/paper/2017/hash/dc6a7e655d7e5840e66733e9ee67cc69-Abstract.html",
    "Year": "2017",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Maithra Raghu",
      "Justin Gilmer",
      "Jason Yosinski",
      "Jascha Sohl-Dickstein"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification",
      "Regression"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "icdm/ChenCHCCSD16",
    "Title": "Interpretable Clustering via Discriminative Rectangle Mixture Model.",
    "url": "https://doi.org/10.1109/ICDM.2016.0097",
    "Year": "2016",
    "Venue": {
      "isOld": true,
      "value": "ICDM"
    },
    "Authors": [
      "Junxiang Chen",
      "Yale Chang",
      "Brian Hobbs",
      "Peter J. Castaldi",
      "Michael H. Cho",
      "Edwin K. Silverman",
      "Jennifer G. Dy"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Clustering"
    ],
    "Type of Explanation": [
      "Decision Rules",
      "Representation Visualization"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Clustering is a technique that is usually applied as a tool for exploratory data analysis. Because of the exploratory nature of this task, it would be beneficial if a clustering method generates interpretable results, and allows incorporating domain knowledge. This motivates us to develop a probabilistic discriminative model that learns a rectangular decision rule for each cluster, we call Discriminative Rectangle Mixture (DReaM) model. DReaM gives interpretable clustering results, because the rectangular decision rules discovered explicitly illustrate how one cluster is defined and differs from other clusters. It also facilitates us to take advantage of existing rules because we can choose informative prior distributions for the rectangular rules. Moreover, DReaM allows that the features for generating rules do not have to be the same as the features for discovering cluster structure. We approximate the distribution for the rules discovered via variational inference. Experimental results demonstrate that DReaM gives more interpretable clustering results, and yet its performance is comparable to existing clustering methods when solving traditional clustering. Furthermore, in real applications, DReaM is able to effectively take advantage of domain knowledge, and to generate reasonable clustering results.",
    "IsOld": true
  },
  {
    "Paper-ID": "icdm/DangSSSB16",
    "Title": "Outlier Detection from Network Data with Subnetwork Interpretation.",
    "url": "https://doi.org/10.1109/ICDM.2016.0101",
    "Year": "2016",
    "Venue": {
      "isOld": true,
      "value": "ICDM"
    },
    "Authors": [
      "Xuan-Hong Dang",
      "Arlei Silva",
      "Ambuj K. Singh",
      "Ananthram Swami",
      "Prithwish Basu"
    ],
    "Type of Data": [
      "Graph data"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Anomaly detection"
    ],
    "Type of Explanation": [
      "Localization"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Detecting a small number of outliers from a set of data observations is always challenging. This problem is more difficult in the setting of multiple network samples, where computing the anomalous degree of a network sample is generally not sufficient. In fact, explaining why a given network is exceptional, expressed in the form of subnetwork, is also equally important. We develop a novel algorithm to address these two key problems. We treat each network sample as a potential outlier and identify subnetworks that help discriminate it from nearby samples. The algorithm is developed in the framework of network regression combined with the constraints on both network topology and L1-norm shrinkage to perform subnetwork discovery. Our method thus goes beyond subspace/subgraph discovery. We also show that the developed method converges to a global optimum. Empirical evaluation on various real-world network datasets demonstrates the advantages of our algorithm over various baseline methods.",
    "IsOld": true
  },
  {
    "Paper-ID": "icdm/WangRDLKM16",
    "Title": "Bayesian Rule Sets for Interpretable Classification.",
    "url": "https://doi.org/10.1109/ICDM.2016.0171",
    "Year": "2016",
    "Venue": {
      "isOld": true,
      "value": "ICDM"
    },
    "Authors": [
      "Tong Wang",
      "Cynthia Rudin",
      "Finale Doshi-Velez",
      "Yimin Liu",
      "Erica Klampfl",
      "Perry MacNeille"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "Bayesian or Hierarchical Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Decision Rules"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "A Rule Set model consists of a small number of short rules for interpretable classification, where an instance is classified as positive if it satisfies at least one of the rules. The rule set provides reasons for predictions, and also descriptions of a particular class. We present a Bayesian framework for learning Rule Set models, with prior parameters that the user can set to encourage the model to have a desired size and shape in order to conform with a domain-specific definition of interpretability. We use an efficient inference approach for searching for the MAP solution and provide theoretical bounds to reduce computation. We apply Rule Set models to ten UCI data sets and compare the performance with other interpretable and non-interpretable models.",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/LakkarajuBL16",
    "Title": "Interpretable Decision Sets - A Joint Framework for Description and Prediction.",
    "url": "https://doi.org/10.1145/2939672.2939874",
    "Year": "2016",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Himabindu Lakkaraju",
      "Stephen H. Bach",
      "Jure Leskovec"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Decision Rules"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "One of the most important obstacles to deploying predictive models is the fact that humans do not understand and trust them. Knowing which variables are important in a model's prediction and how they are combined can be very powerful in helping people understand and trust automatic decision making systems. Here we propose interpretable decision sets, a framework for building predictive models that are highly accurate, yet also highly interpretable. Decision sets are sets of independent if-then rules. Because each rule can be applied independently, decision sets are simple, concise, and easily interpretable. We formalize decision set learning through an objective function that simultaneously optimizes accuracy and interpretability of the rules. In particular, our approach learns short, accurate, and non-overlapping rules that cover the whole feature space and pay attention to small but important classes. Moreover, we prove that our objective is a non-monotone submodular function, which we efficiently optimize to find a near-optimal set of rules. Experiments show that interpretable decision sets are as accurate at classification as state-of-the-art machine learning techniques. They are also three times smaller on average than rule-based models learned by other methods. Finally, results of a user study show that people are able to answer multiple-choice questions about the decision boundaries of interpretable decision sets and write descriptions of classes based on them faster and more accurately than with other rule-based models that were designed for interpretability. Overall, our framework provides a new approach to interpretable machine learning that balances accuracy, interpretability, and computational efficiency.",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/Ribeiro0G16",
    "Title": "\"Why Should I Trust You?\" - Explaining the Predictions of Any Classifier.",
    "url": "https://doi.org/10.1145/2939672.2939778",
    "Year": "2016",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Marco T\u00falio Ribeiro",
      "Sameer Singh",
      "Carlos Guestrin"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Images",
      "Text"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Tree Ensemble",
      "Support Vector Machine",
      "Any (for a specific task); model-agnostic",
      "Logistic Regression",
      "Other"
    ],
    "Type of Task": [
      "Classification",
      "Regression"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Localization"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/ChenCDHSSA16",
    "Title": "InfoGAN - Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.",
    "url": "https://proceedings.neurips.cc/paper/2016/hash/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Abstract.html",
    "Year": "2016",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Xi Chen",
      "Yan Duan",
      "Rein Houthooft",
      "John Schulman",
      "Ilya Sutskever",
      "Pieter Abbeel"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Representation learning"
    ],
    "Type of Explanation": [
      "Disentanglement"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/ChoiBSKSS16",
    "Title": "RETAIN - An Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism.",
    "url": "https://proceedings.neurips.cc/paper/2016/hash/231141b34c82aa95e48810a9d1b33a79-Abstract.html",
    "Year": "2016",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Edward Choi",
      "Mohammad Taha Bahadori",
      "Jimeng Sun",
      "Joshua Kulas",
      "Andy Schuetz",
      "Walter F. Stewart"
    ],
    "Type of Data": [
      "Time series"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/Jitkrittum0CG16",
    "Title": "Interpretable Distribution Features with Maximum Testing Power.",
    "url": "https://proceedings.neurips.cc/paper/2016/hash/0a09c8844ba8f0936c20bd791130d6b6-Abstract.html",
    "Year": "2016",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Wittawat Jitkrittum",
      "Zolt\u00e1n Szab\u00f3",
      "Kacper P. Chwialkowski",
      "Arthur Gretton"
    ],
    "Type of Data": [
      "Tabular / structured",
      "Images",
      "Text"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "Any (for a specific task); model-agnostic",
      "Other"
    ],
    "Type of Task": [
      "Other"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Heatmap"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/KimKK16",
    "Title": "Examples are not enough, learn to criticize! Criticism for Interpretability.",
    "url": "https://proceedings.neurips.cc/paper/2016/hash/5680522b8e2bb01943234bce7bf84534-Abstract.html",
    "Year": "2016",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Been Kim",
      "Oluwasanmi Koyejo",
      "Rajiv Khanna"
    ],
    "Type of Data": [
      "Images"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Any (for a specific task); model-agnostic"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Prototypes"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/LakkarajuL16",
    "Title": "Confusions over Time - An Interpretable Bayesian Model to Characterize Trends in Decision Making.",
    "url": "https://proceedings.neurips.cc/paper/2016/hash/97d0145823aeb8ed80617be62e08bdcc-Abstract.html",
    "Year": "2016",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Himabindu Lakkaraju",
      "Jure Leskovec"
    ],
    "Type of Data": [
      "Time series",
      "User-item matrix"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "Bayesian or Hierarchical Network"
    ],
    "Type of Task": [
      "Classification",
      "Clustering"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Prototypes"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/ZhaoP16",
    "Title": "Interpretable Nonlinear Dynamic Modeling of Neural Trajectories.",
    "url": "https://proceedings.neurips.cc/paper/2016/hash/b2531e7bb29bf22e1daae486fae3417a-Abstract.html",
    "Year": "2016",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Yuan Zhao",
      "Il Memming Park"
    ],
    "Type of Data": [
      "Time series"
    ],
    "Type of Problem": [
      "Model Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Regression"
    ],
    "Type of Explanation": [
      "White-box model"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "sigir/ZhaoLRMYR16",
    "Title": "Explainable User Clustering in Short Text Streams.",
    "url": "https://doi.org/10.1145/2911451.2911522",
    "Year": "2016",
    "Venue": {
      "isOld": true,
      "value": "SIGIR"
    },
    "Authors": [
      "Yukun Zhao",
      "Shangsong Liang",
      "Zhaochun Ren",
      "Jun Ma",
      "Emine Yilmaz",
      "Maarten de Rijke"
    ],
    "Type of Data": [
      "Text",
      "Time series",
      "Graph data"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Clustering"
    ],
    "Type of Explanation": [
      "Prototypes"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "User clustering has been studied from different angles: behavior-based, to identify similar browsing or search patterns, and content-based, to identify shared interests. Once user clusters have been found, they can be used for recommendation and personalization. So far, content-based user clustering has mostly focused on static sets of relatively long documents. Given the dynamic nature of social media, there is a need to dynamically cluster users in the context of short text streams. User clustering in this setting is more challenging than in the case of long documents as it is difficult to capture the users' dynamic topic distributions in sparse data settings. To address this problem, we propose a dynamic user clustering topic model (or UCT for short). UCT adaptively tracks changes of each user's time-varying topic distribution based both on the short texts the user posts during a given time period and on the previously estimated distribution. To infer changes, we propose a Gibbs sampling algorithm where a set of word-pairs from each user is constructed for sampling. The clustering results are explainable and human-understandable, in contrast to many other clustering algorithms. For evaluation purposes, we work with a dataset consisting of users and tweets from each user. Experimental results demonstrate the effectiveness of our proposed clustering model compared to state-of-the-art baselines.",
    "IsOld": true
  },
  {
    "Paper-ID": "aaai/KimPRS15",
    "Title": "Scalable and Interpretable Data Representation for High-Dimensional, Complex Data.",
    "url": "http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9738",
    "Year": "2015",
    "Venue": {
      "isOld": true,
      "value": "AAAI"
    },
    "Authors": [
      "Been Kim",
      "Kayur Patel",
      "Afshin Rostamizadeh",
      "Julie A. Shah"
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/WangFM15",
    "Title": "Trading Interpretability for Accuracy - Oblique Treed Sparse Additive Models.",
    "url": "https://doi.org/10.1145/2783258.2783407",
    "Year": "2015",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Jialei Wang",
      "Ryohei Fujimaki",
      "Yosuke Motohashi"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Transparent Box Design"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Classification",
      "Regression"
    ],
    "Type of Explanation": [
      "Decision Tree",
      "Feature plot"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Model interpretability has been recognized to play a key role in practical data mining. Interpretable models provide significant insights on data and model behaviors and may convince end-users to employ certain models. In return for these advantages, however, there is generally a sacrifice in accuracy, i.e., flexibility of model representation (e.g., linear, rule-based, etc.) and model complexity needs to be restricted in order for users to be able to understand the results. This paper proposes oblique treed sparse additive models (OT-SpAMs). Our main focus is on developing a model which sacrifices a certain degree of interpretability for accuracy but achieves entirely sufficient accuracy with such fully non-linear models as kernel support vector machines (SVMs). OT-SpAMs are instances of region-specific predictive models. They divide feature spaces into regions with sparse oblique tree splitting and assign local sparse additive experts to individual regions. In order to maintain OT-SpAM interpretability, we have to keep the overall model structure simple, and this produces simultaneous model selection issues for sparse oblique region structures and sparse local experts. We address this problem by extending factorized asymptotic Bayesian inference. We demonstrate, on simulation, benchmark, and real world datasets that, in terms of accuracy, OT-SpAMs outperform state-of-the-art interpretable models and perform competitively with kernel SVMs, while still providing results that are highly understandable.",
    "IsOld": true
  },
  {
    "Paper-ID": "nips/KimSD15",
    "Title": "Mind the Gap - A Generative Approach to Interpretable Feature Selection and Extraction.",
    "url": "https://proceedings.neurips.cc/paper/2015/hash/82965d4ed8150294d4330ace00821d77-Abstract.html",
    "Year": "2015",
    "Venue": {
      "isOld": true,
      "value": "NeurIPS"
    },
    "Authors": [
      "Been Kim",
      "Julie A. Shah",
      "Finale Doshi-Velez"
    ],
    "Type of Data": [
      "Tabular / structured"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "Bayesian or Hierarchical Network"
    ],
    "Type of Task": [
      "Clustering"
    ],
    "Type of Explanation": [
      "Feature Importance"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "",
    "IsOld": true
  },
  {
    "Paper-ID": "acl/FysheTMM14",
    "Title": "Interpretable Semantic Vectors from a Joint Model of Brain- and Text- Based Meaning.",
    "url": "https://doi.org/10.3115/v1/p14-1046",
    "Year": "2014",
    "Venue": {
      "isOld": true,
      "value": "ACL"
    },
    "Authors": [
      "Alona Fyshe",
      "Partha Pratim Talukdar",
      "Brian Murphy",
      "Tom M. Mitchell"
    ],
    "Type of Data": [
      "Time series",
      "Text"
    ],
    "Type of Problem": [
      "Model Inspection",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Representation learning"
    ],
    "Type of Explanation": [
      "Heatmap",
      "Feature Importance"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Vector space models (VSMs) represent word meanings as points in a high dimensional space. VSMs are typically created using a large text corpora, and so represent word semantics as observed in text. We present a new algorithm (JNNSE) that can incorporate a measure of semantics not previously used to create VSMs: brain activation data recorded while people read words. The resulting model takes advantage of the complementary strengths and weaknesses of corpus and brain activation data to give a more complete representation of semantics. Evaluations show that the model 1) matches a behavioral measure of semantics more closely, 2) can be used to predict corpus data for unseen words and 3) has predictive power that generalizes across brain imaging technologies and across subjects. We believe that the model is thus a more faithful representation of mental vocabularies.",
    "IsOld": true
  },
  {
    "Paper-ID": "icdm/ChanLBR14",
    "Title": "TRIBAC - Discovering Interpretable Clusters and Latent Structures in Graphs.",
    "url": "https://doi.org/10.1109/ICDM.2014.118",
    "Year": "2014",
    "Venue": {
      "isOld": true,
      "value": "ICDM"
    },
    "Authors": [
      "Jeffrey Chan",
      "Christopher Leckie",
      "James Bailey",
      "Kotagiri Ramamohanarao"
    ],
    "Type of Data": [
      "Graph data"
    ],
    "Type of Problem": [
      "Model Inspection"
    ],
    "Type of Model to be Explained": [
      "Other"
    ],
    "Type of Task": [
      "Clustering"
    ],
    "Type of Explanation": [
      "Feature plot"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Graphs are a powerful representation of relational data, such as social and biological networks. Often, these entities form groups and are organised according to a latent structure. However, these groupings and structures are generally unknown and it can be difficult to identify them. Graph clustering is an important type of approach used to discover these vertex groups and the latent structure within graphs. One type of approach for graph clustering is non-negative matrix factorisation However, the formulations of existing factorisation approaches can be overly relaxed and their groupings and results consequently difficult to interpret, may fail to discover the true latent structure and groupings, and converge to extreme solutions. In this paper, we propose a new formulation of the graph clustering problem that results in clusterings that are easy to interpret. Combined with a novel algorithm, the clusterings are also more accurate than state-of-the-art algorithms for both synthetic and real datasets.",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/BarbieriBM14",
    "Title": "Who to follow and why - link prediction with explanations.",
    "url": "https://doi.org/10.1145/2623330.2623733",
    "Year": "2014",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Nicola Barbieri",
      "Francesco Bonchi",
      "Giuseppe Manco"
    ],
    "Type of Data": [
      "Graph data"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Bayesian or Hierarchical Network"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Localization",
      "Feature Importance"
    ],
    "Method used to explain": [
      "Post-hoc explanation method"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "User recommender systems are a key component in any on-line social networking platform: they help the users growing their network faster, thus driving engagement and loyalty. In this paper we study link prediction with explanations for user recommendation in social networks. For this problem we propose WTFW (\"Who to Follow and Why\"), a stochastic topic model for link prediction over directed and nodes-attributed graphs. Our model not only predicts links, but for each predicted link it decides whether it is a \"topical\" or a \"social\" link, and depending on this decision it produces a different type of explanation. A topical link is recommended between a user interested in a topic and a user authoritative in that topic: the explanation in this case is a set of binary features describing the topic responsible of the link creation. A social link is recommended between users which share a large social neighborhood: in this case the explanation is the set of neighbors which are more likely to be responsible for the link creation. Our experimental assessment on real-world data confirms the accuracy of WTFW in the link prediction and the quality of the associated explanations.",
    "IsOld": true
  },
  {
    "Paper-ID": "kdd/GhalwashRO14",
    "Title": "Utilizing temporal patterns for estimating uncertainty in interpretable early decision making.",
    "url": "https://doi.org/10.1145/2623330.2623694",
    "Year": "2014",
    "Venue": {
      "isOld": true,
      "value": "KDD"
    },
    "Authors": [
      "Mohamed F. Ghalwash",
      "Vladan Radosavljevic",
      "Zoran Obradovic"
    ],
    "Type of Data": [
      "Time series"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "Any (for a specific task); model-agnostic",
      "Other"
    ],
    "Type of Task": [
      "Classification"
    ],
    "Type of Explanation": [
      "Prototypes"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Early classification of time series is prevalent in many time-sensitive applications such as, but not limited to, early warning of disease outcome and early warning of crisis in stock market. \\textcolor{black}{ For example,} early diagnosis allows physicians to design appropriate therapeutic strategies at early stages of diseases. However, practical adaptation of early classification of time series requires an easy to understand explanation (interpretability) and a measure of confidence of the prediction results (uncertainty estimates). These two aspects were not jointly addressed in previous time series early classification studies, such that a difficult choice of selecting one of these aspects is required. In this study, we propose a simple and yet effective method to provide uncertainty estimates for an interpretable early classification method. The question we address here is \"how to provide estimates of uncertainty in regard to interpretable early prediction.\" In our extensive evaluation on twenty time series datasets we showed that the proposed method has several advantages over the state-of-the-art method that provides reliability estimates in early classification. Namely, the proposed method is more effective than the state-of-the-art method, is simple to implement, and provides interpretable results.",
    "IsOld": true
  },
  {
    "Paper-ID": "sigir/ZhangL0ZLM14",
    "Title": "Explicit factor models for explainable recommendation based on phrase-level sentiment analysis.",
    "url": "https://doi.org/10.1145/2600428.2609579",
    "Year": "2014",
    "Venue": {
      "isOld": true,
      "value": "SIGIR"
    },
    "Authors": [
      "Yongfeng Zhang",
      "Guokun Lai",
      "Min Zhang",
      "Yi Zhang",
      "Yiqun Liu",
      "Shaoping Ma"
    ],
    "Type of Data": [
      "Text",
      "User-item matrix"
    ],
    "Type of Problem": [
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network"
    ],
    "Type of Task": [
      "Recommendation"
    ],
    "Type of Explanation": [
      "Feature Importance",
      "Text"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Collaborative Filtering(CF)-based recommendation algorithms, such as Latent Factor Models (LFM), work well in terms of prediction accuracy. However, the latent features make it difficulty to explain the recommendation results to the users. Fortunately, with the continuous growth of online user reviews, the information available for training a recommender system is no longer limited to just numerical star ratings or user/item features. By extracting explicit user opinions about various aspects of a product from the reviews, it is possible to learn more details about what aspects a user cares, which further sheds light on the possibility to make explainable recommendations. In this work, we propose the Explicit Factor Model (EFM) to generate explainable recommendations, meanwhile keep a high prediction accuracy. We first extract explicit product features (i.e. aspects) and user opinions by phrase-level sentiment analysis on user reviews, then generate both recommendations and disrecommendations according to the specific product features to the user's interests and the hidden features learned. Besides, intuitional feature-level explanations about why an item is or is not recommended are generated from the model. Offline experimental results on several real-world datasets demonstrate the advantages of our framework over competitive baseline algorithms on both rating prediction and top-K recommendation tasks. Online experiments show that the detailed explanations make the recommendations and disrecommendations more influential on user's purchasing behavior.",
    "IsOld": true
  },
  {
    "Paper-ID": "acl/SchuffAV20",
    "Title": "F1 is Not Enough! Models and Evaluation Towards User-Centered Explainable Question Answering.",
    "url": "https://doi.org/10.18653/v1/2020.emnlp-main.575",
    "Year": "2020",
    "Venue": {
      "isOld": true,
      "value": "ACL"
    },
    "Authors": [
      "Hendrik Schuff",
      "Heike Adel",
      "Ngoc Thang Vu",
    ],
    "Type of Data": [
      "Text"
    ],
    "Type of Problem": [
      "Transparent Box Design",
      "Outcome Explanation"
    ],
    "Type of Model to be Explained": [
      "(Deep) Neural Network",
      "Bayesian or Hierarchical Network"
    ],
    "Type of Task": [
      "Question Answering"
    ],
    "Type of Explanation": [
      "Text"
    ],
    "Method used to explain": [
      "Interpretability built into the predictive model",
      "Supervised explanation training"
    ],
    "Should the paper be included?": "Yes",
    "Should the paper be included with filter?": "Yes",
    "Abstract": "Explainable question answering systems predict an answer together with an explanation showing why the answer has been selected. The goal is to enable users to assess the correctness of the system and understand its reasoning process. However, we show that current models and evaluation settings have shortcomings regarding the coupling of answer and explanation which might cause serious issues in user experience. As a remedy, we propose a hierarchical model and a new regularization term to strengthen the answer-explanation coupling as well as two evaluation scores to quantify the coupling. We conduct experiments on the HOTPOTQA benchmark data set and perform a user study. The user study shows that our models increase the ability of the users to judge the correctness of the system and that scores like F1 are not enough to estimate the usefulness of a model in a practical setting with human users. Our scores are better aligned with user experience, making them promising candidates for model selection.",
    "IsOld": false
  }
]
